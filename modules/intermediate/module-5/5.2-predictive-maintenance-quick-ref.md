# Module 5.2 â€“ Predictive Maintenance Quick Reference

## CLI Usage

### Training Models
```bash
# Basic classification model (event in next 24 hours)
python 5.2-predictive-maintenance-pipeline.py train --model logistic --save model.joblib

# Advanced XGBoost with custom horizon
python 5.2-predictive-maintenance-pipeline.py train \
    --model xgboost \
    --target event_in_72h \
    --horizon 72 \
    --n-estimators 200 \
    --max-depth 8 \
    --save xgb_72h.joblib

# Regression for time-to-event prediction
python 5.2-predictive-maintenance-pipeline.py train \
    --task regression \
    --target time_to_event \
    --model rf \
    --n-estimators 150 \
    --save rul_model.joblib

# SMOTE for imbalanced data
python 5.2-predictive-maintenance-pipeline.py train \
    --use-smote \
    --smote-k-neighbors 3 \
    --model lightgbm

# Cost-sensitive threshold optimization
python 5.2-predictive-maintenance-pipeline.py train \
    --threshold-method cost_based \
    --cost-fp 1.0 \
    --cost-fn 15.0 \
    --model catboost
```

### Model Evaluation
```bash
# Evaluate saved model
python 5.2-predictive-maintenance-pipeline.py evaluate --model-path model.joblib

# Evaluate on different dataset
python 5.2-predictive-maintenance-pipeline.py evaluate \
    --model-path model.joblib \
    --dataset synthetic_maintenance
```

### Making Predictions
```bash
# Single prediction with JSON input
python 5.2-predictive-maintenance-pipeline.py predict \
    --model-path model.joblib \
    --input-json '{"sensor_1":0.5, "sensor_2":1.2, "tool_id":"T001"}'

# Prediction from file
echo '{"sensor_1":0.5, "sensor_2":1.2, "tool_id":"T001"}' > input.json
python 5.2-predictive-maintenance-pipeline.py predict \
    --model-path model.joblib \
    --input-file input.json
```

## Feature Engineering Recipes

### Time Window Features
```python
# Rolling statistics (multiple windows)
for window in [6, 12, 24, 48]:
    df[f'sensor_mean_{window}h'] = df['sensor'].rolling(window).mean()
    df[f'sensor_std_{window}h'] = df['sensor'].rolling(window).std()
    df[f'sensor_range_{window}h'] = (
        df['sensor'].rolling(window).max() -
        df['sensor'].rolling(window).min()
    )

# EWMA for trend detection
df['sensor_ewma_12h'] = df['sensor'].ewm(halflife=12).mean()
df['sensor_ewma_24h'] = df['sensor'].ewm(halflife=24).mean()

# Lag features for temporal dependencies
for lag in [1, 6, 12, 24]:
    df[f'sensor_lag_{lag}h'] = df['sensor'].shift(lag)

# Trend features (slope over window)
def trend_slope(series):
    x = np.arange(len(series))
    return np.polyfit(x, series.values, 1)[0] if len(series) > 1 else 0

df['sensor_trend_12h'] = df['sensor'].rolling(12).apply(trend_slope)
```

### Degradation Indicators
```python
# Deviation from baseline
df['sensor_baseline'] = df.groupby('tool_id')['sensor'].transform('mean')
df['sensor_deviation'] = df['sensor'] - df['sensor_baseline']

# Cumulative statistics since last maintenance
df['hours_since_maint'] = df.groupby('tool_id')['timestamp'].diff().dt.total_seconds() / 3600
df['hours_since_maint'] = df.groupby('tool_id')['hours_since_maint'].cumsum()

# Cross-sensor interactions
df['temp_pressure_ratio'] = df['temperature'] / (df['pressure'] + 1e-6)
df['flow_stability'] = df['flow'].rolling(6).std()
```

## Model Selection Guide

| Model | Use Case | Pros | Cons | Hyperparameters |
|-------|----------|------|------|-----------------|
| **Logistic Regression** | Baseline, fast inference | Interpretable, fast | Linear assumptions | `C=1.0` |
| **Random Forest** | Robust baseline | Handles mixed data | Can overfit | `n_estimators=100`, `max_depth=6` |
| **XGBoost** | High performance | Excellent results | Hyperparameter sensitive | `n_estimators=100`, `max_depth=6`, `learning_rate=0.1` |
| **LightGBM** | Large datasets | Fast training | Less stable | `n_estimators=100`, `max_depth=6` |
| **CatBoost** | Categorical features | Robust defaults | Slower training | `iterations=100`, `depth=6` |

## Threshold Optimization

### Youden's J Statistic
```python
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_true, y_proba)
youden_scores = tpr - fpr
optimal_threshold = thresholds[np.argmax(youden_scores)]
```

### Cost-Based Optimization
```python
def find_optimal_threshold(y_true, y_proba, cost_fp=1.0, cost_fn=10.0):
    thresholds = np.linspace(0.01, 0.99, 100)
    costs = []

    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        tn = np.sum((y_true == 0) & (y_pred == 0))
        fp = np.sum((y_true == 0) & (y_pred == 1))
        fn = np.sum((y_true == 1) & (y_pred == 0))
        tp = np.sum((y_true == 1) & (y_pred == 1))

        cost = fp * cost_fp + fn * cost_fn
        costs.append(cost)

    return thresholds[np.argmin(costs)]
```

## Manufacturing Metrics

### Prediction Within Specification (PWS)
```python
# Classification PWS (accuracy)
pws_classification = np.mean(y_pred == y_true)

# Regression PWS (tolerance-based)
tolerance_hours = 24
pws_regression = np.mean(np.abs(y_pred - y_true) <= tolerance_hours)
```

### Estimated Loss
```python
# Classification loss
def classification_loss(y_true, y_pred, cost_fp=1.0, cost_fn=10.0):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return fp * cost_fp + fn * cost_fn

# Regression loss (asymmetric)
def regression_loss(y_true, y_pred, underestimate_penalty=2.0):
    errors = y_pred - y_true
    costs = np.where(errors < 0,
                    np.abs(errors) * underestimate_penalty,  # Underestimate
                    np.abs(errors))  # Overestimate
    return np.mean(costs)
```

## Cross-Validation Patterns

### Time-Based Split with Embargo
```python
def time_based_split(df, train_ratio=0.7, embargo_hours=24):
    df_sorted = df.sort_values('timestamp')
    split_idx = int(len(df_sorted) * train_ratio)

    train_end_time = df_sorted.iloc[split_idx]['timestamp']
    embargo_end = train_end_time + pd.Timedelta(hours=embargo_hours)

    train_data = df_sorted[df_sorted['timestamp'] <= train_end_time]
    test_data = df_sorted[df_sorted['timestamp'] >= embargo_end]

    return train_data, test_data
```

### Group-Based (Tool-Aware) Split
```python
from sklearn.model_selection import GroupKFold

gkf = GroupKFold(n_splits=5)
for train_idx, val_idx in gkf.split(X, y, groups=df['tool_id']):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    # Train and validate
```

## Imbalance Handling

### Class Weights
```python
# Automatic balancing
class_weight = 'balanced'

# Manual weights (emphasize minority class)
class_weight = {0: 1.0, 1: 10.0}
```

### SMOTE Oversampling
```python
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Create pipeline with SMOTE
pipeline = ImbPipeline([
    ('imputer', SimpleImputer()),
    ('scaler', StandardScaler()),
    ('smote', SMOTE(k_neighbors=5)),
    ('classifier', XGBClassifier())
])
```

## JSON Output Formats

### Training Output
```json
{
  "status": "trained",
  "model": "xgboost",
  "task": "classification",
  "target": "event_in_24h",
  "n_samples": 43800,
  "n_features": 176,
  "metrics": {
    "roc_auc": 0.995,
    "pr_auc": 0.764,
    "precision": 0.367,
    "recall": 0.993,
    "f1": 0.536,
    "pws": 0.971,
    "estimated_loss": 1326.0,
    "cost_per_sample": 0.030
  },
  "threshold": 0.541
}
```

### Evaluation Output
```json
{
  "status": "evaluated",
  "model": "xgboost",
  "task": "classification",
  "target": "event_in_24h",
  "metrics": {
    "roc_auc": 0.995,
    "pr_auc": 0.764,
    "pws": 0.971,
    "estimated_loss": 1326.0
  }
}
```

### Prediction Output
```json
{
  "status": "predicted",
  "model": "xgboost",
  "task": "classification",
  "target": "event_in_24h",
  "prediction": 0,
  "probability": 0.234,
  "threshold": 0.541,
  "input": {"sensor_1": 0.5, "sensor_2": 1.2}
}
```

## Troubleshooting Guide

### Common Issues

**1. Poor Model Performance**
- Check class balance: `df['target'].value_counts()`
- Verify feature engineering: plot sensor trends around events
- Try different models: start with XGBoost
- Adjust threshold: use cost-based optimization

**2. Data Leakage**
```python
# Check for future information
def check_leakage(df):
    # Ensure timestamps are chronological
    assert df['timestamp'].is_monotonic_increasing

    # Check for perfect correlation with target
    for col in df.columns:
        if col != 'target':
            corr = df[col].corr(df['target'])
            if abs(corr) > 0.95:
                print(f"Potential leakage in {col}: correlation = {corr:.3f}")
```

**3. Memory Issues with Large Datasets**
- Use chunked processing for feature engineering
- Reduce window sizes or number of sensors
- Sample data for development, full data for production

**4. Prediction Errors**
- Ensure input features match training schema
- Check for missing engineered features
- Verify tool_id exists in training data

### Performance Optimization

**Feature Engineering Speedup**
```python
# Use vectorized operations
df['sensor_mean_24h'] = df.groupby('tool_id')['sensor'].rolling(24).mean().values

# Parallel processing for multiple tools
from multiprocessing import Pool
def process_tool(tool_data):
    return create_features(tool_data)

with Pool() as pool:
    results = pool.map(process_tool, tool_groups)
```

**Model Training Speedup**
```python
# Use GPU acceleration (if available)
xgb_params = {
    'tree_method': 'gpu_hist',  # GPU acceleration
    'gpu_id': 0
}

# Early stopping for large datasets
xgb_params.update({
    'early_stopping_rounds': 10,
    'eval_metric': 'auc'
})
```

## Integration Examples

### Batch Scoring Script
```python
def batch_score(model_path, data_path, output_path):
    # Load model and data
    pipeline = PredictiveMaintenancePipeline.load(model_path)
    df = pd.read_parquet(data_path)

    # Generate features
    feature_cols = [col for col in df.columns if col.startswith('sensor_')]
    df = create_time_window_features(df, feature_cols)

    # Make predictions
    X = df[pipeline.feature_names]
    predictions = pipeline.predict_proba(X)[:, 1]

    # Save results
    results = df[['tool_id', 'timestamp']].copy()
    results['maintenance_probability'] = predictions
    results['alert'] = predictions >= pipeline.fitted_threshold
    results.to_parquet(output_path)
```

### Real-Time Monitoring
```python
def real_time_monitor(model_path, kafka_topic):
    from kafka import KafkaConsumer

    pipeline = PredictiveMaintenancePipeline.load(model_path)
    consumer = KafkaConsumer(kafka_topic)

    for message in consumer:
        sensor_data = json.loads(message.value)

        # Extract features (pre-computed in streaming pipeline)
        features = extract_features(sensor_data)

        # Predict
        prob = pipeline.predict_proba([features])[0, 1]

        if prob >= pipeline.fitted_threshold:
            send_alert(sensor_data['tool_id'], prob)
```

## Best Practices Checklist

- [ ] **Data Quality**: Verify sensor calibration and maintenance records
- [ ] **Temporal Validation**: Use time-based splits with embargo periods
- [ ] **Feature Engineering**: Include rolling stats, EWMA, and lag features
- [ ] **Class Balance**: Handle imbalance with weights or resampling
- [ ] **Threshold Tuning**: Optimize for business cost function
- [ ] **Manufacturing Metrics**: Report PWS and estimated loss
- [ ] **Model Monitoring**: Track feature drift and performance decay
- [ ] **Documentation**: Maintain model cards and decision logs
- [ ] **Stakeholder Alignment**: Ensure metrics match business objectives
- [ ] **Gradual Rollout**: Start with human-in-the-loop validation

---

For detailed theory and advanced topics, see `5.2-predictive-maintenance-fundamentals.md`  
For hands-on examples and EDA, see `5.2-predictive-maintenance-analysis.ipynb`
