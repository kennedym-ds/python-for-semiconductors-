# Module 5.2 â€“ Predictive Maintenance Fundamentals for Semiconductor Manufacturing

## 1. Introduction: Predictive Maintenance in Semiconductor Fabs

Predictive maintenance (PdM) represents a paradigm shift from reactive and scheduled maintenance to data-driven, condition-based maintenance strategies. In semiconductor manufacturing, where tool downtime can cost hundreds of thousands of dollars per hour, predictive maintenance enables:

- **Early Warning Systems**: Detect impending failures days or weeks in advance
- **Optimized Maintenance Scheduling**: Balance tool availability with maintenance needs
- **Root Cause Analysis**: Identify degradation patterns and failure modes
- **Yield Protection**: Prevent tool-related yield excursions through proactive intervention

## 2. Use Cases in Semiconductor Manufacturing

### 2.1 Remaining Useful Life (RUL) Estimation
- **Objective**: Predict time until next maintenance event or failure
- **Models**: Regression (time-to-event), survival analysis, degradation models
- **Business Value**: Optimize maintenance schedules, reduce emergency downtime
- **Challenges**: Censored data (maintenance before failure), varying operating conditions

### 2.2 Early Warning Alerts
- **Objective**: Binary classification of impending maintenance needs
- **Models**: Classification with optimized thresholds for alert generation
- **Business Value**: Proactive maintenance planning, prevent catastrophic failures
- **Challenges**: Class imbalance, false positive costs (unnecessary alerts)

### 2.3 Tool Health Scoring
- **Objective**: Continuous monitoring of tool condition and performance
- **Models**: Anomaly detection, multivariate monitoring, drift detection
- **Business Value**: Real-time visibility into fleet health, trend analysis
- **Challenges**: Normal variation vs. degradation, multi-tool normalization

## 3. Labeling Strategies for Predictive Maintenance

### 3.1 Event-in-Next-K-Hours Classification
```python
# Binary classification approach
event_in_24h = (time_to_next_maintenance <= 24).astype(int)
event_in_72h = (time_to_next_maintenance <= 72).astype(int)
```

**Advantages:**
- Simple binary classification problem
- Clear business interpretation (alert horizon)
- Well-established ML algorithms and metrics

**Considerations:**
- Horizon selection impacts precision/recall tradeoff
- Class imbalance (events are rare)
- Temporal dependencies in labels

### 3.2 Time-to-Event Regression
```python
# Regression to predict exact time until event
time_to_event = hours_until_next_maintenance
```

**Advantages:**
- More granular predictions
- Natural handling of varying time horizons
- Rich information for maintenance scheduling

**Considerations:**
- Censoring (scheduled maintenance before failure)
- Non-linear degradation patterns
- Uncertainty quantification becomes critical

### 3.3 Censoring and Survival Analysis
- **Right Censoring**: Observation ends before event occurs
- **Left Censoring**: Event occurred before observation started (rare)
- **Interval Censoring**: Event occurred within known time interval

**Handling Strategies:**
- Survival models (Cox proportional hazards, parametric)
- Censoring-aware loss functions
- Two-stage models (classification + regression)

## 4. Feature Engineering for Time-Series Data

### 4.1 Rolling Window Statistics
Essential for capturing temporal patterns and degradation trends:

```python
# Basic rolling statistics
sensor_mean_24h = df['sensor'].rolling(24).mean()
sensor_std_24h = df['sensor'].rolling(24).std()
sensor_min_24h = df['sensor'].rolling(24).min()
sensor_max_24h = df['sensor'].rolling(24).max()
sensor_range_24h = sensor_max_24h - sensor_min_24h
```

**Window Size Selection:**
- Short windows (1-6 hours): Capture immediate changes
- Medium windows (12-24 hours): Daily patterns and shifts
- Long windows (48-168 hours): Weekly cycles and long-term trends

### 4.2 Exponentially Weighted Moving Averages (EWMA)
Emphasize recent observations while maintaining historical context:

```python
# EWMA with different half-lives
sensor_ewma_12h = df['sensor'].ewm(halflife=12).mean()
sensor_ewma_24h = df['sensor'].ewm(halflife=24).mean()
```

**Applications:**
- Noise reduction while preserving responsiveness
- Drift detection (current vs. EWMA baseline)
- Adaptive thresholds for anomaly detection

### 4.3 Lag Features
Capture temporal dependencies and lead-lag relationships:

```python
# Lag features
sensor_lag_1h = df['sensor'].shift(1)
sensor_lag_6h = df['sensor'].shift(6)
sensor_lag_12h = df['sensor'].shift(12)
```

**Design Considerations:**
- Balance between prediction horizon and available history
- Cross-sensor lag relationships (temperature affects pressure)
- Recipe-dependent lag patterns

### 4.4 Trend and Seasonality Features
```python
# Linear trend over window
def calc_trend_slope(series):
    x = np.arange(len(series))
    return np.polyfit(x, series.values, 1)[0]

sensor_trend_12h = df['sensor'].rolling(12).apply(calc_trend_slope)

# Seasonality indicators
hour_of_day = df['timestamp'].dt.hour
day_of_week = df['timestamp'].dt.dayofweek
```

## 5. Model Selection and Architecture

### 5.1 Tree-Based Ensembles (Recommended)
**XGBoost, LightGBM, CatBoost**

**Advantages:**
- Handle mixed data types and missing values
- Capture non-linear patterns and interactions
- Built-in feature importance
- Robust to outliers

**Hyperparameter Considerations:**
- `n_estimators`: 100-1000 (balance performance vs. speed)
- `max_depth`: 3-8 (prevent overfitting on time series)
- `learning_rate`: 0.01-0.3 (lower for larger ensembles)

### 5.2 Linear Baselines
**Logistic Regression, Linear Regression**

**Advantages:**
- Fast training and inference
- Interpretable coefficients
- Reliable uncertainty estimates
- Good regularization properties

**Use Cases:**
- Baseline models and benchmarking
- High-frequency inference requirements
- Regulatory/compliance environments requiring interpretability

### 5.3 Survival Models (Advanced)
**Cox Proportional Hazards, Parametric Survival Models**

**Advantages:**
- Natural handling of censored data
- Hazard ratios provide interpretable risk factors
- Time-varying effects

**Implementation Considerations:**
- Requires specialized libraries (lifelines, scikit-survival)
- More complex hyperparameter tuning
- Limited support in production MLOps platforms

## 6. Cross-Validation and Temporal Data Splitting

### 6.1 Time-Based Splits with Embargo
Critical to prevent data leakage in temporal prediction:

```python
# Time-based split with embargo period
train_end = '2023-06-01'
embargo_days = 7  # No predictions within 7 days of training
test_start = pd.to_datetime(train_end) + pd.Timedelta(days=embargo_days)

train_data = df[df['timestamp'] <= train_end]
test_data = df[df['timestamp'] >= test_start]
```

### 6.2 Group-Based Validation
When multiple tools or lots are present:

```python
from sklearn.model_selection import GroupKFold

# Ensure tools don't appear in both train and validation
gkf = GroupKFold(n_splits=5)
for train_idx, val_idx in gkf.split(X, y, groups=df['tool_id']):
    # Train and validate
    pass
```

### 6.3 Walk-Forward Validation
Simulate production deployment scenario:

```python
# Expanding window walk-forward validation
for i in range(min_train_weeks, total_weeks):
    train_data = df[df['week'] <= i]
    test_data = df[df['week'] == i + 1]
    # Train, predict, evaluate
```

## 7. Class Imbalance and Threshold Optimization

### 7.1 Imbalance Handling Strategies

**Class Weights (Recommended)**
```python
# Balanced class weights
class_weight = 'balanced'  # Automatically balances
# Custom weights
class_weight = {0: 1.0, 1: 10.0}  # Higher weight for positive class
```

**Resampling Techniques**
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# SMOTE for oversampling minority class
smote = SMOTE(k_neighbors=5, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
```

### 7.2 Threshold Optimization

**Youden's J Statistic**
Maximizes sensitivity + specificity - 1:
```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_true, y_proba)
youden_scores = tpr - fpr
optimal_threshold = thresholds[np.argmax(youden_scores)]
```

**Cost-Based Optimization**
Minimize expected cost:
```python
def optimize_threshold_cost(y_true, y_proba, cost_fp=1.0, cost_fn=10.0):
    best_threshold = 0.5
    min_cost = float('inf')

    for threshold in np.linspace(0.01, 0.99, 100):
        y_pred = (y_proba >= threshold).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        cost = fp * cost_fp + fn * cost_fn

        if cost < min_cost:
            min_cost = cost
            best_threshold = threshold

    return best_threshold
```

## 8. Manufacturing-Specific Metrics

### 8.1 Prediction Within Specification (PWS)
Percentage of predictions within acceptable engineering tolerance:

```python
def calculate_pws(y_true, y_pred, tolerance=24):
    """PWS for regression (time predictions within tolerance hours)"""
    within_spec = np.abs(y_pred - y_true) <= tolerance
    return np.mean(within_spec)

def calculate_pws_classification(y_true, y_pred):
    """PWS for classification (accuracy)"""
    return np.mean(y_pred == y_true)
```

### 8.2 Estimated Loss
Cost-based metric reflecting business impact:

```python
def estimated_loss_classification(y_true, y_pred, cost_fp=1.0, cost_fn=10.0):
    """Cost-based loss for classification"""
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return fp * cost_fp + fn * cost_fn

def estimated_loss_regression(y_true, y_pred, underestimate_penalty=2.0):
    """Cost-based loss for regression with asymmetric penalties"""
    errors = y_pred - y_true
    costs = np.where(errors < 0,
                    np.abs(errors) * underestimate_penalty,  # Underestimate
                    np.abs(errors))  # Overestimate
    return np.mean(costs)
```

## 9. Uncertainty Quantification and Calibration

### 9.1 Calibration for Classification
Ensure predicted probabilities reflect true likelihood:

```python
from sklearn.calibration import CalibratedClassifierCV

# Platt scaling or Isotonic regression
calibrated_clf = CalibratedClassifierCV(base_classifier, cv=3, method='isotonic')
calibrated_clf.fit(X_train, y_train)
calibrated_probs = calibrated_clf.predict_proba(X_test)
```

### 9.2 Prediction Intervals for Regression
Quantify uncertainty in time-to-event predictions:

```python
# Bootstrap-based confidence intervals
def bootstrap_prediction_intervals(model, X, n_bootstrap=100, alpha=0.05):
    predictions = []
    for i in range(n_bootstrap):
        # Bootstrap sample
        bootstrap_idx = np.random.choice(len(X), len(X), replace=True)
        X_bootstrap = X.iloc[bootstrap_idx]
        pred_bootstrap = model.predict(X_bootstrap)
        predictions.append(pred_bootstrap)

    predictions = np.array(predictions)
    lower = np.percentile(predictions, 100 * alpha/2, axis=0)
    upper = np.percentile(predictions, 100 * (1 - alpha/2), axis=0)

    return lower, upper
```

## 10. Operational Considerations

### 10.1 Real-Time Feature Computation
- **Streaming Windows**: Use tools like Apache Kafka + ksqlDB for real-time aggregations
- **Feature Stores**: Cache expensive computations (trend analysis, EWMA)
- **Latency Requirements**: Balance feature complexity with inference speed

### 10.2 Model Deployment Patterns
- **Batch Scoring**: Daily/hourly predictions for maintenance planning
- **Streaming Inference**: Real-time health monitoring and alerts
- **Edge Deployment**: On-tool models for immediate response

### 10.3 Monitoring and Maintenance
- **Feature Drift**: Monitor distribution shifts in sensor data
- **Performance Decay**: Track prediction accuracy over time
- **Concept Drift**: Detect changes in failure patterns or operating regimes

## 11. Integration with Manufacturing Systems

### 11.1 MES/ERP Integration
- **Work Order Generation**: Automatic maintenance scheduling based on predictions
- **Parts Planning**: Proactive ordering based on failure probability
- **Resource Allocation**: Technician scheduling and tool allocation

### 11.2 SPC Integration
- **Statistical Process Control**: Complement traditional control charts
- **Alarm Correlation**: Link predictive alerts with SPC violations
- **Root Cause Analysis**: Use model features for investigation

## 12. Regulatory and Compliance Considerations

### 12.1 Model Validation
- **Performance Testing**: Demonstrate superiority over current methods
- **Robustness Testing**: Validate across different operating conditions
- **Documentation**: Maintain model cards and validation reports

### 12.2 Explainability Requirements
- **Feature Importance**: SHAP values for individual predictions
- **Global Explanations**: Partial dependence plots for key sensors
- **Audit Trails**: Decision logs for maintenance recommendations

## 13. Common Pitfalls and Best Practices

### 13.1 Data Leakage Prevention
- **Future Information**: Ensure no features contain future information
- **Target Leakage**: Avoid features that are consequences of the target
- **Temporal Ordering**: Maintain strict chronological data splits

### 13.2 Overfitting in Time Series
- **Validation Strategy**: Use time-based rather than random splits
- **Feature Selection**: Avoid over-engineering on historical patterns
- **Model Complexity**: Balance complexity with generalization

### 13.3 Business Integration
- **Stakeholder Alignment**: Ensure metrics align with business objectives
- **Change Management**: Gradual rollout with human oversight
- **Feedback Loops**: Incorporate maintenance outcomes back into training

## 14. Advanced Topics and Future Directions

### 14.1 Multi-Task Learning
- **Shared Representations**: Joint models for multiple tools or failure modes
- **Transfer Learning**: Leverage models across similar equipment
- **Hierarchical Models**: Tool-specific models with shared components

### 14.2 Deep Learning Approaches
- **LSTM/GRU**: For complex temporal dependencies
- **Transformer Models**: Attention-based sequence modeling
- **Autoencoders**: Unsupervised feature learning and anomaly detection

### 14.3 Physics-Informed Models
- **Domain Knowledge**: Incorporate physical degradation models
- **Hybrid Approaches**: Combine physics simulations with data-driven models
- **Constraint Satisfaction**: Ensure predictions respect physical laws

## 15. Summary

Predictive maintenance in semiconductor manufacturing requires careful consideration of:

1. **Temporal Nature**: Time-based validation, embargo periods, no future leakage
2. **Business Context**: Cost-aware metrics, threshold optimization, manufacturing integration
3. **Data Challenges**: Class imbalance, censoring, multi-tool normalization
4. **Model Selection**: Tree ensembles for performance, linear models for speed/interpretability
5. **Feature Engineering**: Rolling statistics, EWMA, lag features, trend analysis
6. **Uncertainty**: Calibration, prediction intervals, confidence estimation
7. **Operations**: Real-time deployment, monitoring, integration with manufacturing systems

The key to successful predictive maintenance implementation is balancing model sophistication with operational practicality, ensuring that predictions drive actionable maintenance decisions that improve equipment availability and reduce costs.

## References

- IEEE Standards for Condition-Based Maintenance
- SEMI E133 (Specification for Equipment Health Factor)
- NASA Prognostics and Health Management literature
- Semiconductor Equipment and Materials International (SEMI) standards
- Industrial Internet Consortium predictive maintenance frameworks
