# Module 4.2 – Unsupervised Learning for Semiconductor Manufacturing

## 1. Motivation: Structure Discovery Without Labels

High-volume semiconductor fabs generate multi-stage, multi-sensor, high-dimensional data streams where labeled outcomes (yield loss root cause, excursion type, defect class) may be sparse, delayed, or noisy. Unsupervised learning enables:

- Clustering wafers / lots / tools into behavioral cohorts
- Detecting emergent excursions earlier than rule-based SPC
- Reducing dimensionality for interpretability (PCA for latent process factors)
- Learning embeddings for downstream supervised tasks (pretraining)
- Exposing anomalous patterns in parametric drift or spatial signatures

## 2. Core Methods Covered

| Task | Algorithms | Semiconductor Use Cases |
|------|------------|-------------------------|
| Clustering | KMeans, Gaussian Mixture (GMM), DBSCAN | Wafer parametric grouping, excursion family discovery |
| Dimensionality Reduction | PCA (variance retention), t-SNE (local manifold), UMAP (global + local) | Latent process factors, wafer map embedding |
| Anomaly / Outlier Scoring | IsolationForest, Local Density (via DBSCAN noise), Reconstruction error (future) | Early drift detection, subtle contamination |
| Cluster Validation | Silhouette, Calinski-Harabasz, Davies-Bouldin, Cluster Size Entropy | Cohort stability, operational viability |

## 3. Synthetic Data Design (Teaching Dataset)

We generate a semi-realistic feature matrix representing inline metrology & process summary metrics:

- Latent factors (equipment health, recipe window, environment) → observed variables via linear + non-linear mixing
- Add controlled shift in a subset of lots to simulate drift
- Inject 2 anomaly mechanisms: sparse high-leverage points + small off-manifold cluster

## 4. Clustering Strategy Pattern

1. Scale numeric features (StandardScaler or RobustScaler if heavy tails)
2. Dimensionality reduction (optional: PCA retain 95% variance)
3. Fit multiple clustering models
4. Compute internal validation metrics
5. Select candidate model balancing quality + operational constraints (e.g., min cluster size)
6. Persist: embeddings, cluster assignments, centroid stats, validation metrics (JSON)

## 5. Semiconductor-Specific Considerations

| Concern | Adjustment |
|---------|------------|
| Tool / Recipe leakage | Cluster within homogeneous recipe strata first |
| Temporal drift | Evaluate silhouette on rolling windows |
| Operational deploy | Enforce minimum cluster population threshold |
| Cluster semantics | Summarize median process parameters per cluster |
| False anomalies | Use multi-model agreement (IsolationForest + distance) |

## 6. Metrics Explained

- Silhouette: Cohesion vs separation (-1 to 1). Good general discriminator.
- Calinski-Harabasz: Ratio of between-cluster to within-cluster dispersion (higher better).
- Davies-Bouldin: Average similarity; penalizes overlapping clusters (lower better).
- Cluster Size Entropy: \( H = -\sum p_i \log p_i \). Too low ⇒ dominance; too high ⇒ fragmentation.
- Anomaly Ratio: Fraction labeled noise (DBSCAN) or marked by isolation forest; track baseline.

## 7. Pipeline Artifacts (What We Save)

| Artifact | Purpose |
|----------|---------|
| model.joblib | Serialized clustering pipeline (scaler, pca, model) |
| clusters.parquet | Input rows + cluster_id + anomaly_flag |
| metrics.json | Validation metrics + model params + timestamp |
| centroids.parquet | Cluster centers in original & PCA space |
| embedding.parquet | 2D embedding for visualization (PCA/t-SNE/UMAP) |

## 8. Operational Guardrails

- Reject model if max(cluster_fraction) > 0.85 (degenerate single-cluster)
- Reject if silhouette < 0.05 (no structure)
- Warn if anomaly_ratio > 0.15 (possible global drift)
- Store run metadata: random_seed, code_version (placeholder), feature_schema hash

## 9. CLI Design (Consistent Pattern)

```bash
python 4.2-unsupervised-pipeline.py train --dataset synthetic_process --model kmeans --k 6 --save model.joblib
python 4.2-unsupervised-pipeline.py evaluate --model-path model.joblib --dataset synthetic_process
python 4.2-unsupervised-pipeline.py predict --model-path model.joblib --input-json '{"f1":0.12,...}'
```

All commands emit JSON to stdout with keys: status, metrics, metadata, (optionals) cluster_distribution, warnings.

## 10. Future Extensions

- Add spatial wafer map embedding (CNN → UMAP) for defect signatures
- Semi-supervised fine-tuning using partial excursion labels
- Streaming incremental clustering (MiniBatchKMeans + drift detector)
- Reconstruction-based anomaly detection (Autoencoder) in Advanced Series

## 11. Checklist Before Promotion

- [ ] Metrics pass guardrails
- [ ] Cluster distribution documented
- [ ] Anomaly definition validated with SMEs
- [ ] Embedding visually inspected (no collapsed manifold)
- [ ] Reproducibility artifacts present (seed, params)

## 12. Quick Reference Snippet

```python
from pathlib import Path
from modules.intermediate.module-4._synthetic import generate_synthetic_unsupervised  # (future helper)
from modules.intermediate.module-4.unsupervised_pipeline import UnsupervisedPipeline

X = generate_synthetic_unsupervised(n_samples=1200)
pipe = UnsupervisedPipeline(model='kmeans', n_clusters=6, pca_variance=0.95).fit(X)
metrics = pipe.evaluate(X)
pipe.save(Path('model.joblib'))
```

---
AI Note: Keep implementation minimal but extensible; emphasize clarity for learners while exposing production-like structure.
