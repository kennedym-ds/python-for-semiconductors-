---
post_title: "Module 4.1 – Ensemble Methods Quick Reference"
author1: "Your Name"
post_slug: "module-4-1-ensemble-quick-ref"
microsoft_alias: "alias"
featured_image: "/images/ensemble-quick-ref.png"
categories: [data-science, machine-learning, manufacturing]
tags: [ensemble, quick-reference, tuning, random-forest, xgboost, lightgbm]
ai_note: "Initial scaffold assisted by AI."
summary: "Cheat sheet for Random Forest, HistGradientBoosting, XGBoost, and LightGBM usage, tuning, and diagnostics in semiconductor yield modeling."
post_date: 2025-09-03
---

# Module 4.1 – Ensemble Methods Quick Reference

Fast, opinionated cheatsheet for RandomForest, HistGradientBoosting, XGBoost, LightGBM in semiconductor yield, excursion, and virtual metrology tasks.

## Core Python Snippets

Train + evaluate (regression example):

```python
from pathlib import Path
from modules.intermediate.module-4._helpers import load_semiconductor_frame  # hypothetical
from modules.intermediate.module-4.ensemble_pipeline import EnsemblePipeline

df = load_semiconductor_frame()
y = df['target'].to_numpy()
X = df.drop(columns=['target'])
pipe = EnsemblePipeline(model='hist_gb', max_depth=8, learning_rate=0.05).fit(X, y)
metrics = pipe.evaluate(X, y)
pipe.save(Path('models/hist_gb.joblib'))
```

CLI (planned) concept (illustrative):

```bash
python 4.1-ensemble-pipeline.py --model xgb --train data/train.parquet --valid data/valid.parquet \
	--target final_yield --params params.yaml --out models/xgb.joblib --report reports/xgb.json
```

## Parameter Cheat Sheet

| Category | RF | HistGradientBoosting | XGBoost | LightGBM | Notes / Semiconductor Angle |
|----------|----|----------------------|---------|----------|-----------------------------|
| Core Complexity | n_estimators, max_depth | max_depth, max_leaf_nodes | n_estimators, max_depth | num_leaves, max_depth | Complexity couples with variance / wafer scarcity |
| Learning Rate | – | learning_rate | eta (learning_rate) | learning_rate | Lower to reduce overfit, add trees |
| Row Subsample | bootstrap (implicit) | – | subsample | bagging_fraction + bagging_freq | Adds stochastic regularization |
| Column Subsample | max_features | – | colsample_bytree/level/node | feature_fraction | Decorrelates features; excision of noise sensors |
| Min Leaf / Child | min_samples_leaf | min_samples_leaf | min_child_weight | min_data_in_leaf | Prevents modeling rare noisy wafer states |
| Regularization | – | l2_regularization | lambda, alpha, gamma | lambda_l1, lambda_l2, min_gain_to_split | Controls complexity / gain inflation |
| Early Stopping | – (OOB only) | early_stopping | early_stopping_rounds | early_stopping_round | Use temporal validation window |
| Monotonic Constraints | No | monotonic_cst | monotone_constraints | monotone_constraints | Encode known process monotonicities |
| Cat Handling | Encode externally | Encode externally | Encode externally / target | Native categorical_feature | Avoid leakage from high-card tool IDs |
| Missing Handling | Surrogate splits implicit | Native | Native | Native | Benefit for sparse inline sensors |

## Tuning Ladder (Minimal Sequence)

1. RandomForest baseline (n_estimators=400, max_depth=None, min_samples_leaf=2, max_features='sqrt').
1. HistGradientBoosting (learning_rate=0.05, max_depth=8, early_stopping=True, monotonic constraints where valid).
1. XGBoost escalate (eta=0.1→0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, min_child_weight=2, n_estimators large with early stopping).
1. LightGBM when feature space wide or large dataset (num_leaves=63, min_data_in_leaf=20, feature_fraction=0.8, bagging_fraction=0.8, learning_rate=0.05).
1. Randomized search (≤30 trials) on key levers: learning_rate, depth/leaves, subsample, colsample, min_child_weight/min_data_in_leaf, regularization.
1. Probability calibration (classification) + monotonic check (partial dependence monotonic shape enforced where expected).

## Quick Diagnostic Table

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| Train >> Validation gap | Over-complex trees | Increase min_child_weight/min_data_in_leaf, add regularization, lower depth |
| Slow convergence XGBoost | Learning rate too low | Increase eta slightly or enable more parallelism (tree_method='hist') |
| Feature importance dominated by IDs | Leakage / high-cardinality | Aggregate or hash bucket, re-evaluate with permutation importance |
| Unstable SHAP across runs | High variance / insufficient trees | Increase n_estimators with lower learning_rate |
| Validation metric oscillates | Noisy small validation set | Use temporal grouping or enlarge validation horizon |
| Over-splitting LightGBM | num_leaves too high | Reduce num_leaves or increase min_data_in_leaf |

## Feature Importance & SHAP

- Start with permutation importance for robust signal vs. noise assessment.
- Use SHAP summary to confirm directional plausibility (e.g., increasing over-etch time reduces yield beyond a knee point).
- For speed on large ensembles: sample 5–10% of wafers for SHAP, stratify by product family.
- Interaction effects: tree SHAP interaction values only if strictly needed (computational cost).

SHAP Caution Checklist:

- [ ] Remove or anonymize wafer/tool IDs before interpretation.
- [ ] Validate sign of top 5 features with process SME.
- [ ] Examine dependence plots for monotonic expectation violations.

## Minimal Reproducibility Steps

1. Fix seeds (python, numpy, library-specific).
1. Record feature list + order.
1. Persist raw parameters & environment versions (pip freeze snapshot).
1. Store validation split definition (temporal boundaries) in metadata.
1. Serialize model + SHA256 hash.

## Suggested Early Stopping Configuration

| Library | Param | Typical Value | Notes |
|---------|-------|---------------|-------|
| XGBoost | early_stopping_rounds | 50 | Provide eval_set with temporal validation |
| LightGBM | early_stopping_round | 50 | Monitor primary metric (e.g., rmse or auc) |
| HistGB | early_stopping | True | Uses internal validation fraction |

## Safe Default Starter Grid (Randomized Search 15 Trials)

| Hyperparameter | Dist / Choices |
|----------------|----------------|
| learning_rate | [0.03, 0.05, 0.07, 0.1] |
| max_depth / num_leaves | depth: [4,6,8,10]; leaves: [31,63,127] |
| subsample / bagging_fraction | Uniform[0.6, 0.9] |
| colsample_bytree / feature_fraction | Uniform[0.6, 0.9] |
| min_child_weight / min_data_in_leaf | [1,2,4,8,16,32] |
| reg_lambda / lambda_l2 | [0, 0.5, 1, 2, 5] |
| gamma / min_gain_to_split | [0, 0.1, 0.2, 0.5, 1] |

## Monotonic Constraints Quick Pattern

1. Identify feature with expected monotonic effect (e.g., over-etch-time negatively impacts yield).
1. Assign constraint sign (+1 increasing, -1 decreasing, 0 none) in order of feature columns.
1. Validate via partial dependence → if violated, re-check feature scaling and leakage.

## Promotion Checklist (Abbreviated)

- [ ] Reproducible training script committed
- [ ] Validation respects temporal ordering
- [ ] Early stopping used (boosting)
- [ ] Calibration evaluated (classification) Brier / ECE
- [ ] SHAP sanity checked by SME
- [ ] Drift baseline metrics stored
- [ ] Model card drafted

---

End of Quick Reference.
