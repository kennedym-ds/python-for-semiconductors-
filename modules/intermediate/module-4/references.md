# Module 4: References and Further Reading

## Ensembles & Unsupervised Learning

### Ensemble Methods

### Essential Books
- **"Ensemble Methods in Machine Learning" by Thomas Dietterich** (2000)
  - Foundational paper on ensemble methods
  - Theoretical analysis of ensemble performance
  - [Machine Learning, Vol. 40, No. 2](https://link.springer.com/article/10.1023/A:1007607513941)

- **"Random Forests" by Leo Breiman** (2001)
  - Original paper introducing Random Forest algorithm
  - Theoretical foundations and practical applications
  - [Machine Learning, Vol. 45, No. 1](https://link.springer.com/article/10.1023/A:1010933404324)

- **"The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman** (Chapters 8, 10, 15)
  - Comprehensive coverage of ensemble methods
  - Bagging, boosting, and random forests
  - [Springer](https://web.stanford.edu/~hastie/ElemStatLearn/)

- **"Pattern Recognition and Machine Learning" by Christopher Bishop** (Chapter 14)
  - Combining models and ensemble learning
  - Bayesian perspective on model combination
  - [Springer](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

### Gradient Boosting and Advanced Ensembles

### Essential Books
- **"Additive Logistic Regression: A Statistical View of Boosting" by Jerome Friedman, Trevor Hastie, and Robert Tibshirani** (2000)
  - Statistical interpretation of boosting algorithms
  - AdaBoost and gradient boosting foundations
  - [Annals of Statistics](https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/Additive-logistic-regression-a-statistical-view-of-boosting/10.1214/aos/1016218223.full)

- **"Greedy Function Approximation: A Gradient Boosting Machine" by Jerome Friedman** (2001)
  - Original gradient boosting machine paper
  - Mathematical foundations and algorithms
  - [Annals of Statistics](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full)

### Practical Ensemble Implementation
- **"XGBoost: A Scalable Tree Boosting System" by Tianqi Chen and Carlos Guestrin** (2016)
  - Modern gradient boosting implementation
  - Scalability and performance optimizations
  - [ACM SIGKDD Conference](https://dl.acm.org/doi/10.1145/2939672.2939785)

- **"LightGBM: A Highly Efficient Gradient Boosting Decision Tree" by Guolin Ke et al.** (2017)
  - Fast gradient boosting implementation
  - Memory efficiency and speed improvements
  - [NIPS Conference](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)

### Unsupervised Learning

### Essential Books
- **"Pattern Recognition and Machine Learning" by Christopher Bishop** (Chapters 9, 12)
  - Mixture models and the EM algorithm
  - Principal Component Analysis and Factor Analysis
  - [Springer](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

- **"The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman** (Chapters 13, 14)
  - Unsupervised learning methods
  - Clustering and self-organizing maps
  - [Springer](https://web.stanford.edu/~hastie/ElemStatLearn/)

- **"Introduction to Statistical Learning" by James, Witten, Hastie, and Tibshirani** (Chapter 12)
  - Practical approach to unsupervised learning
  - PCA, clustering, and applications
  - [Springer](https://www.statlearning.com/)

### Clustering Algorithms

### Essential References
- **"k-means++: The Advantages of Careful Seeding" by David Arthur and Sergei Vassilvitskii** (2007)
  - Improved initialization for k-means clustering
  - Theoretical guarantees and practical benefits
  - [ACM-SIAM Symposium on Discrete Algorithms](https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf)

- **"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise" by Martin Ester et al.** (1996)
  - DBSCAN clustering algorithm
  - Density-based clustering with noise handling
  - [KDD Conference](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)

- **"Gaussian Mixture Models" by Douglas Reynolds** (2009)
  - Comprehensive review of Gaussian mixture models
  - EM algorithm and parameter estimation
  - [Encyclopedia of Biometrics](https://link.springer.com/referenceworkentry/10.1007/978-0-387-73003-5_196)

### Anomaly Detection

### Essential Books
- **"Outlier Analysis" by Charu Aggarwal** (2nd Edition, 2017)
  - Comprehensive treatment of anomaly detection
  - Statistical, proximity-based, and linear methods
  - [Springer](https://link.springer.com/book/10.1007/978-3-319-47578-3)

- **"Anomaly Detection: A Survey" by Varun Chandola, Arindam Banerjee, and Vipin Kumar** (2009)
  - Comprehensive survey of anomaly detection techniques
  - Classification of methods and applications
  - [ACM Computing Surveys](https://dl.acm.org/doi/10.1145/1541880.1541882)

### Isolation Forest and Modern Methods
- **"Isolation Forest" by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou** (2008)
  - Tree-based anomaly detection method
  - Efficient and effective anomaly detection
  - [IEEE ICDM Conference](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)

- **"One-Class Support Vector Machines" by Bernhard Sch√∂lkopf et al.** (2001)
  - Support vector approach to anomaly detection
  - Kernel methods for outlier detection
  - [Neural Computation](https://www.mitpressjournals.org/doi/abs/10.1162/089976601750264965)

### Semiconductor-Specific Applications

### Industry Research Papers
- **"Ensemble Methods for Semiconductor Yield Prediction"** (IEEE Transactions on Semiconductor Manufacturing, 2020)
  - Application of ensemble methods to yield modeling
  - Feature importance and model interpretation
  - Comparison of different ensemble approaches

- **"Unsupervised Learning for Equipment Health Monitoring in Semiconductor Manufacturing"** (Journal of Manufacturing Systems, 2021)
  - Clustering and anomaly detection for predictive maintenance
  - Real-time monitoring applications
  - Integration with manufacturing execution systems

- **"Random Forest Applications in Wafer Defect Classification"** (IEEE Transactions on Components, Packaging and Manufacturing Technology, 2019)
  - Tree-based methods for defect pattern recognition
  - Feature engineering for wafer map data
  - Performance comparison with other methods

### Manufacturing Analytics Applications
- **"Gradient Boosting for Process Parameter Optimization"** (Applied Sciences, 2020)
  - XGBoost and LightGBM for process optimization
  - Feature importance analysis for process understanding
  - Multi-objective optimization using ensemble methods

- **"Clustering Analysis of Semiconductor Process Conditions"** (Computers & Industrial Engineering, 2021)
  - K-means and Gaussian mixture models for process clustering
  - Process regime identification and characterization
  - Anomaly detection for process drift

### Software Libraries and Tools

#### Ensemble Method Libraries
- **XGBoost**: https://xgboost.readthedocs.io/
  - Scalable gradient boosting framework
  - GPU acceleration and distributed computing
  - Feature importance and SHAP integration

- **LightGBM**: https://lightgbm.readthedocs.io/
  - Fast gradient boosting decision trees
  - Memory efficient implementation
  - Categorical feature support

- **CatBoost**: https://catboost.ai/
  - Gradient boosting with categorical features
  - Built-in cross-validation and hyperparameter tuning
  - GPU training support

#### Scikit-learn Ensemble Methods
- **RandomForestClassifier/Regressor**: Random forest implementation
- **GradientBoostingClassifier/Regressor**: Gradient boosting implementation
- **VotingClassifier/Regressor**: Ensemble voting methods
- **BaggingClassifier/Regressor**: Bootstrap aggregating

#### Unsupervised Learning Libraries
- **Scikit-learn Clustering**: https://scikit-learn.org/stable/modules/clustering.html
  - K-means, DBSCAN, Gaussian mixtures
  - Hierarchical clustering methods
  - Cluster validation metrics

- **PyOD (Python Outlier Detection)**: https://pyod.readthedocs.io/
  - Comprehensive outlier detection toolkit
  - Multiple algorithms including Isolation Forest
  - Standardized API and evaluation metrics

#### Specialized Tools
- **HDBSCAN**: https://hdbscan.readthedocs.io/
  - Hierarchical density-based clustering
  - Better cluster discovery than DBSCAN
  - Robust to varying densities

- **UMAP**: https://umap-learn.readthedocs.io/
  - Uniform Manifold Approximation and Projection
  - Dimensionality reduction for visualization
  - Preserves local and global structure

### Model Interpretation and Explainability

#### SHAP (SHapley Additive exPlanations)
- **"A Unified Approach to Interpreting Model Predictions" by Scott Lundberg and Su-In Lee** (2017)
  - Game-theoretic approach to model interpretation
  - Feature importance for individual predictions
  - [NIPS Conference](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)

- **SHAP Library**: https://shap.readthedocs.io/
  - Python library for model explanation
  - Support for tree-based models and deep learning
  - Visualization tools for interpretability

#### LIME (Local Interpretable Model-agnostic Explanations)
- **"Why Should I Trust You? Explaining the Predictions of Any Classifier" by Marco Tulio Ribeiro et al.** (2016)
  - Local explanations for individual predictions
  - Model-agnostic interpretation method
  - [ACM SIGKDD Conference](https://dl.acm.org/doi/10.1145/2939672.2939778)

### Performance Evaluation

#### Ensemble Method Evaluation
- **Out-of-Bag (OOB) Error**: Bootstrap-based error estimation
- **Feature Importance**: Permutation and tree-based importance
- **Cross-Validation**: Proper validation for ensemble methods
- **Diversity Measures**: Ensemble member diversity metrics

#### Clustering Evaluation
- **Internal Validation**:
  - Silhouette coefficient
  - Calinski-Harabasz index
  - Davies-Bouldin index
  - Within-cluster sum of squares (WCSS)

- **External Validation**:
  - Adjusted Rand Index (ARI)
  - Normalized Mutual Information (NMI)
  - Homogeneity and completeness
  - V-measure

#### Anomaly Detection Evaluation
- **ROC-AUC**: Receiver Operating Characteristic curve
- **PR-AUC**: Precision-Recall curve (better for imbalanced data)
- **F1-Score**: Harmonic mean of precision and recall
- **False Positive Rate**: Cost-sensitive evaluation

### Hyperparameter Tuning

#### Ensemble Method Tuning
- **Random Forest Parameters**:
  - n_estimators: Number of trees
  - max_depth: Tree depth limitation
  - min_samples_split: Minimum samples for split
  - max_features: Feature sampling strategy

- **Gradient Boosting Parameters**:
  - learning_rate: Shrinkage parameter
  - n_estimators: Number of boosting rounds
  - max_depth: Tree complexity
  - subsample: Row sampling ratio

#### Unsupervised Learning Tuning
- **K-means Parameters**:
  - n_clusters: Number of clusters
  - init: Initialization method
  - n_init: Number of random initializations
  - max_iter: Maximum iterations

- **DBSCAN Parameters**:
  - eps: Neighborhood distance
  - min_samples: Minimum cluster size
  - metric: Distance metric

### Advanced Topics

#### Ensemble Learning Theory
- **Bias-Variance Decomposition**: Understanding ensemble benefits
- **Diversity-Accuracy Tradeoff**: Balancing ensemble members
- **Ensemble Pruning**: Selecting optimal subset of models
- **Dynamic Ensemble Selection**: Adaptive model selection

#### Advanced Clustering
- **Consensus Clustering**: Combining multiple clustering results
- **Multi-view Clustering**: Clustering with multiple data views
- **Spectral Clustering**: Graph-based clustering methods
- **Deep Clustering**: Neural network-based clustering

#### Online Learning
- **Streaming Ensembles**: Online ensemble learning
- **Incremental Clustering**: Updating clusters with new data
- **Concept Drift Detection**: Adapting to changing data distributions
- **Online Anomaly Detection**: Real-time anomaly detection

### Industry Case Studies

#### Semiconductor Manufacturing Applications
- **Intel Fab Process Optimization**:
  - Random forest for yield prediction
  - Feature importance for process understanding
  - Integration with process control systems

- **TSMC Equipment Health Monitoring**:
  - Isolation forest for anomaly detection
  - Clustering for equipment state analysis
  - Predictive maintenance applications

- **Samsung Quality Control**:
  - Gradient boosting for defect classification
  - Ensemble methods for wafer map analysis
  - Real-time quality monitoring

### Training and Certification

#### Professional Development
- **Coursera Machine Learning Specialization**: https://www.coursera.org/specializations/machine-learning-introduction
  - Advanced machine learning techniques
  - Ensemble methods and unsupervised learning

- **edX MITx Statistics and Data Science**: https://www.edx.org/micromasters/mitx-statistics-and-data-science
  - Statistical foundations of machine learning
  - Clustering and dimensionality reduction

#### Specialized Training
- **Kaggle Learn**: https://www.kaggle.com/learn
  - Practical ensemble methods
  - Feature engineering and model interpretation

- **Fast.ai**: https://course.fast.ai/
  - Practical machine learning for coders
  - Ensemble methods and interpretation

### Conferences and Journals

#### Major Conferences
- **International Conference on Machine Learning (ICML)**: https://icml.cc/
  - Latest research in ensemble methods and unsupervised learning
  - Theoretical advances and practical applications

- **Conference on Neural Information Processing Systems (NeurIPS)**: https://nips.cc/
  - Cutting-edge research in machine learning
  - Ensemble learning and clustering advances

#### Industry Conferences
- **KDD (Knowledge Discovery and Data Mining)**: https://www.kdd.org/
  - Industrial applications of machine learning
  - Real-world case studies and implementations

- **ASMC (Advanced Semiconductor Manufacturing Conference)**: https://www.semi.org/en/asmc
  - Semiconductor manufacturing applications
  - AI and ML in semiconductor industry

---

## Quick Access Links

### Essential Bookmarks
- [XGBoost Documentation](https://xgboost.readthedocs.io/) - Gradient boosting framework
- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html) - Clustering algorithms
- [PyOD Documentation](https://pyod.readthedocs.io/) - Outlier detection toolkit
- [SHAP Documentation](https://shap.readthedocs.io/) - Model interpretation
- [Kaggle Ensembling Guide](https://www.kaggle.com/code/arthurtok/introduction-to-ensembling-stacking-in-python) - Practical ensemble methods

### Emergency References
- [Ensemble Quick Reference](4.1-ensemble-quick-ref.md) - Module quick reference
- [Unsupervised Learning Quick Reference](4.2-unsupervised-quick-ref.md) - Clustering and anomaly detection summary
- [Scikit-learn Ensemble Guide](https://scikit-learn.org/stable/modules/ensemble.html) - Ensemble methods documentation

### Troubleshooting Guides
- [Ensemble Method Pitfalls](https://scikit-learn.org/stable/modules/ensemble.html#tips-on-practical-use) - Common issues
- [Clustering Validation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) - Evaluation metrics
- [Hyperparameter Tuning](https://scikit-learn.org/stable/modules/grid_search.html) - Optimization strategies

---

*Last Updated: January 2024*
*Module 4 Reference Guide - Python for Semiconductors Learning Series*