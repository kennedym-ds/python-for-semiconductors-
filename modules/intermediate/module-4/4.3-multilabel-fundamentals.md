# Module 4.3: Multi-Label Classification for Manufacturing Defects

## Overview

Multi-label classification extends traditional classification by allowing multiple classes to be assigned to a single instance simultaneously. In semiconductor manufacturing, this is critical because:

- A single wafer or product can exhibit **multiple defect types** concurrently
- Defects often co-occur (e.g., contamination + scratches)
- Root cause analysis requires understanding defect combinations
- Quality control systems need to flag all present defects, not just the primary one

This module introduces multi-label classification using the **Steel Plates Faults dataset** as a real-world manufacturing example.

## Learning Objectives

By the end of this module, you will:

1. **Understand multi-label vs. multi-class classification**
2. **Implement problem transformation methods** (Binary Relevance, Classifier Chains)
3. **Apply algorithm adaptation methods** (MLkNN, Label Powerset)
4. **Evaluate models with multi-label metrics** (Hamming Loss, Subset Accuracy, F1)
5. **Handle label correlations and imbalanced labels**
6. **Integrate Steel Plates dataset** for real manufacturing use cases

## Multi-Label Classification Fundamentals

### Problem Definition

**Multi-Class Classification** (traditional):
- Each instance belongs to exactly **one class** from C classes
- Example: Defect type ∈ {Scratch, Stain, Bump}
- Output: Single label

**Multi-Label Classification**:
- Each instance can belong to **zero or more classes** from C classes
- Example: Defect types = {Scratch, Stain} (both present)
- Output: Binary vector of length C (e.g., [1, 1, 0] = Scratch + Stain, no Bump)

### Mathematical Formulation

Given:
- **X**: Feature space (e.g., ℝᵈ for d features)
- **L**: Set of C possible labels {l₁, l₂, ..., lc}
- **Training set**: {(x₁, Y₁), (x₂, Y₂), ..., (xₙ, Yₙ)}
  - xᵢ ∈ X (feature vector)
  - Yᵢ ⊆ L (subset of labels assigned to instance i)

**Goal**: Learn function h: X → 2^L that predicts label subsets for new instances

### Representation

**Binary Indicator Matrix**:
```
Instance | Scratch | Stain | Bump
---------|---------|-------|------
   1     |    1    |   0   |  1     → {Scratch, Bump}
   2     |    0    |   1   |  0     → {Stain}
   3     |    1    |   1   |  1     → {Scratch, Stain, Bump}
```

Each column is a **binary classification problem**.

## Approach Categories

### 1. Problem Transformation Methods

Transform multi-label problem into one or more single-label problems.

#### Binary Relevance (BR)

**Concept**: Train **one binary classifier per label**.

**Process**:
1. For each label lⱼ, create binary dataset:
   - Positive: Instances where lⱼ ∈ Yᵢ
   - Negative: Instances where lⱼ ∉ Yᵢ
2. Train C independent binary classifiers
3. Predict: Apply all C classifiers, aggregate predictions

**Advantages**:
- ✅ Simple, scalable
- ✅ Can use any binary classifier
- ✅ Parallelizable

**Disadvantages**:
- ❌ Ignores label correlations
- ❌ May produce inconsistent predictions

**When to Use**:
- Large number of labels (C > 100)
- Labels are largely independent
- Computational efficiency is critical

#### Classifier Chains (CC)

**Concept**: Chain classifiers to **capture label dependencies**.

**Process**:
1. Define label order: l₁, l₂, ..., lc
2. For label l₁: Train binary classifier on X
3. For label lⱼ (j > 1): Train binary classifier on X ∪ {l₁, ..., lⱼ₋₁}
   - Include predictions of previous labels as features
4. Predict: Apply chain sequentially

**Example** (3 labels: Scratch, Stain, Bump):
```
Classifier 1: P(Scratch | features)
Classifier 2: P(Stain | features, Scratch)
Classifier 3: P(Bump | features, Scratch, Stain)
```

**Advantages**:
- ✅ Captures label correlations
- ✅ Often higher accuracy than BR

**Disadvantages**:
- ❌ Sensitive to label order
- ❌ Error propagation down the chain
- ❌ Not parallelizable

**Mitigation**: Use **Ensemble of Classifier Chains** (ECC) with random label orders.

#### Label Powerset (LP)

**Concept**: Treat each **unique label combination as a single class**.

**Process**:
1. Identify all unique label subsets in training data
2. Transform to multi-class problem with C' classes (C' = number of unique subsets)
3. Train single multi-class classifier
4. Predict: Output the predicted label subset

**Example**:
```
Original Labels     → Multi-Class
{Scratch}          → Class 1
{Stain}            → Class 2
{Scratch, Stain}   → Class 3
{Scratch, Bump}    → Class 4
```

**Advantages**:
- ✅ Preserves label correlations perfectly
- ✅ Can handle label dependencies naturally

**Disadvantages**:
- ❌ Exponential growth: 2^C possible combinations
- ❌ Label combinations rare in training data may not appear
- ❌ Not scalable to many labels

**When to Use**:
- Small number of labels (C < 10)
- Strong label correlations
- Training data covers most combinations

### 2. Algorithm Adaptation Methods

Modify algorithms to handle multi-label data directly.

#### MLkNN (Multi-Label k-Nearest Neighbors)

**Concept**: Extend kNN using **Bayesian inference** on neighbor labels.

**Process**:
1. For new instance x, find k nearest neighbors
2. For each label lⱼ:
   - Count neighbors with lⱼ: nⱼ
   - Compute P(lⱼ | nⱼ neighbors have lⱼ) using Bayesian rule
   - Assign lⱼ if P(lⱼ | nⱼ) > threshold

**Advantages**:
- ✅ Naturally handles label correlations
- ✅ No training time (lazy learning)
- ✅ Works well with small datasets

**Disadvantages**:
- ❌ Slow prediction (compute neighbors every time)
- ❌ Requires careful distance metric selection

#### Multi-Label Decision Trees

**Concept**: Modify tree splitting criteria for multi-label targets.

**Leaf Prediction**: Instead of single class, predict label frequency distribution.

**Example** (leaf contains 10 samples):
- Scratch: 7/10 → P(Scratch) = 0.7
- Stain: 3/10 → P(Stain) = 0.3
- Bump: 1/10 → P(Bump) = 0.1

Assign labels with P(lⱼ) > threshold (e.g., 0.5).

## Evaluation Metrics

Multi-label evaluation requires specialized metrics.

### Example-Based Metrics

Averaged over instances.

#### Subset Accuracy (Exact Match Ratio)

**Definition**: Fraction of instances where predicted labels **exactly match** true labels.

```
Subset Accuracy = (1/n) Σᵢ I(Yᵢ = Ŷᵢ)
```

**Range**: [0, 1], higher is better

**Characteristics**:
- ✅ Strictest metric (no partial credit)
- ❌ Very harsh for problems with many labels

#### Hamming Loss

**Definition**: Fraction of **incorrectly predicted labels** (false positives + false negatives).

```
Hamming Loss = (1/n) Σᵢ (|Yᵢ Δ Ŷᵢ| / C)
```
where Δ is symmetric difference.

**Range**: [0, 1], lower is better

**Interpretation**: Average per-label error rate.

#### F1 Score (Micro/Macro/Samples)

**Micro-averaged**: Aggregate true positives/false positives across all labels, then compute F1.
```
Precision_micro = TP_total / (TP_total + FP_total)
Recall_micro = TP_total / (TP_total + FN_total)
F1_micro = 2 × (Precision_micro × Recall_micro) / (Precision_micro + Recall_micro)
```

**Macro-averaged**: Compute F1 per label, then average.
```
F1_macro = (1/C) Σⱼ F1(lⱼ)
```

**Samples-averaged**: Compute F1 per instance, then average.

**Choice**:
- **Micro**: Dominated by frequent labels (use for imbalanced datasets)
- **Macro**: Equal weight to all labels
- **Samples**: Equal weight to all instances

### Label-Based Metrics

Averaged over labels (treat each label as binary classification).

#### Per-Label Precision, Recall, F1

For each label lⱼ:
```
Precision(lⱼ) = TP(lⱼ) / (TP(lⱼ) + FP(lⱼ))
Recall(lⱼ) = TP(lⱼ) / (TP(lⱼ) + FN(lⱼ))
F1(lⱼ) = 2 × Precision(lⱼ) × Recall(lⱼ) / (Precision(lⱼ) + Recall(lⱼ))
```

**Use Case**: Identify which specific labels are hard to predict.

### Ranking-Based Metrics

For probabilistic predictions (label scores).

#### Coverage

**Definition**: Average number of labels in top-k to cover all true labels.

**Interpretation**: How far down the ranked list you need to go to find all relevant labels.

#### Ranking Loss

**Definition**: Fraction of label pairs that are misordered.

**Lower is better**.

## Steel Plates Faults Dataset

### Domain Context

**Manufacturing Process**: Steel plate production involves:
1. **Hot rolling**: Steel heated to ~1200°C and rolled into plates
2. **Surface inspection**: Automated optical inspection (AOI)
3. **Defect classification**: Identify fault types for quality control

**Business Impact**:
- Defects reduce structural integrity
- Multiple defects compound quality issues
- Accurate multi-label detection enables:
  - Targeted process adjustments
  - Predictive maintenance
  - Scrap reduction

### Dataset Characteristics

**Source**: UCI Machine Learning Repository (ID: 198)

**Instances**: 1,941 steel plates

**Features**: 27 (all numeric)
- **Geometric**: X_Min, X_Max, Y_Min, Y_Max, Pixels_Areas, X_Perimeter, Y_Perimeter
- **Luminosity**: Sum_of_Luminosity, Min_of_Luminosity, Max_of_Luminosity, Luminosity_Index
- **Steel Properties**: TypeOfSteel_A300, TypeOfSteel_A400, Steel_Plate_Thickness
- **Shape Indices**: Edges_Index, Empty_Index, Square_Index, Outside_X_Index, Orientation_Index
- **Transforms**: Sigmoid_of_Areas, Log_X_Index, Log_Y_Index

**Labels**: 7 binary fault indicators
1. **Pastry** (158 samples, 8.1%)
2. **Z_Scratch** (190 samples, 9.8%)
3. **K_Scratch** (391 samples, 20.1%)
4. **Stains** (72 samples, 3.7%)
5. **Dirtiness** (55 samples, 2.8%)
6. **Bumps** (402 samples, 20.7%)
7. **Other_Faults** (673 samples, 34.7%)

**Label Frequency**: Highly imbalanced (2.8% to 34.7%)

**Label Cardinality**: Average number of labels per instance = **1.0** (most samples have 1 label)

**Label Density**: Label cardinality / C = **1.0 / 7 = 0.14** (14%)

**Note**: This dataset exhibits **sparse multi-label structure** (most instances have 1 label, but some have 2+).

### Data Files

Located in `datasets/steel-plates/`:
- **steel_plates_features.csv**: 1941 rows × 27 feature columns
- **steel_plates_targets.csv**: 1941 rows × 7 binary label columns
- **metadata.txt**: Dataset description and feature names
- **README.md**: Dataset documentation

### Loading Example

```python
import pandas as pd
from pathlib import Path

DATA_ROOT = Path(__file__).parent.parent.parent / "datasets" / "steel-plates"

X = pd.read_csv(DATA_ROOT / "steel_plates_features.csv")
y = pd.read_csv(DATA_ROOT / "steel_plates_targets.csv")

print(f"Features: {X.shape}")  # (1941, 27)
print(f"Labels: {y.shape}")    # (1941, 7)
print(f"Label columns: {y.columns.tolist()}")
```

### Label Co-Occurrence Analysis

**Why Important**: Understanding label correlations guides model selection.

**Analysis Method**:
```python
# Compute label co-occurrence matrix
co_occurrence = y.T.dot(y)
```

**Common Co-Occurrences** (hypothetical example):
- K_Scratch + Bumps: 45 samples (scratches often occur with surface bumps)
- Dirtiness + Stains: 12 samples (contamination leads to staining)

**Insight**: If strong correlations exist → Use Classifier Chains or Label Powerset

## Handling Challenges

### Challenge 1: Label Imbalance

**Problem**: Rare labels (e.g., Dirtiness: 2.8%) are hard to learn.

**Solutions**:

1. **Class Weights**: Weight loss function by inverse label frequency
   ```python
   from sklearn.utils.class_weight import compute_class_weight
   weights = compute_class_weight('balanced', classes=[0, 1], y=y_train[:, j])
   ```

2. **Resampling**: Oversample rare labels or undersample frequent labels
   ```python
   from imblearn.over_sampling import RandomOverSampler
   ros = RandomOverSampler(random_state=42)
   X_res, y_res = ros.fit_resample(X_train, y_train[:, j])
   ```

3. **Threshold Tuning**: Use lower decision threshold for rare labels
   ```python
   # Instead of default 0.5
   thresholds = [0.3 if freq < 0.05 else 0.5 for freq in label_frequencies]
   ```

### Challenge 2: Label Correlations

**Problem**: Labels are not independent (e.g., certain defects co-occur).

**Solutions**:

1. **Classifier Chains**: Explicitly model sequential dependencies
2. **Label Powerset**: Preserve all correlations by treating combinations as classes
3. **Feature Engineering**: Add interaction features that capture correlations

### Challenge 3: Scalability

**Problem**: C = 7 is manageable, but some domains have C > 1000 labels.

**Solutions**:

1. **Binary Relevance**: O(C) complexity, parallelizable
2. **Hierarchical Methods**: Exploit label taxonomy (if available)
3. **Embedding Methods**: Project labels to low-dimensional space

### Challenge 4: Evaluation Metric Selection

**Problem**: Different metrics emphasize different aspects.

**Guidelines**:

- **Use Subset Accuracy** if exact matches are critical (e.g., regulatory compliance)
- **Use Hamming Loss** for general error rate
- **Use Micro-F1** if frequent labels are more important
- **Use Macro-F1** if all labels should be weighted equally
- **Report multiple metrics** for comprehensive evaluation

## Semiconductor Manufacturing Applications

### Use Case 1: Wafer Defect Detection

**Scenario**: Wafers can exhibit multiple defect patterns (scratches, particles, edge chips).

**Multi-Label Approach**:
- Labels: {Center_Defect, Edge_Defect, Scratch, Particle, Discoloration}
- Features: Wafer map statistics, process parameters
- Model: Classifier Chains (defects often co-occur)

**Business Value**:
- Comprehensive defect reporting
- Root cause analysis (defect combinations → process issues)
- Yield prediction improvement

### Use Case 2: Equipment Health Monitoring

**Scenario**: Equipment can have multiple failing subsystems simultaneously.

**Multi-Label Approach**:
- Labels: {Pump_Failure, Heater_Failure, Valve_Leak, Chamber_Contamination}
- Features: Sensor readings (temperature, pressure, flow rates)
- Model: Binary Relevance (subsystems largely independent)

**Business Value**:
- Proactive maintenance scheduling
- Prevent cascading failures
- Reduce unplanned downtime

### Use Case 3: Process Fault Diagnosis

**Scenario**: Manufacturing processes can have multiple simultaneous faults.

**Multi-Label Approach**:
- Labels: {Temperature_Deviation, Pressure_Instability, Chemical_Imbalance, Timing_Error}
- Features: Process control data
- Model: MLkNN (capture complex fault interactions)

**Business Value**:
- Faster root cause identification
- Improved process control
- Quality improvement

## Implementation Strategies

### Strategy 1: Scikit-Multilearn

**Library**: `scikit-multilearn`

**Installation**:
```bash
pip install scikit-multilearn
```

**Example**:
```python
from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain
from sklearn.ensemble import RandomForestClassifier

# Binary Relevance
br = BinaryRelevance(classifier=RandomForestClassifier())
br.fit(X_train, y_train)
y_pred = br.predict(X_test)

# Classifier Chains
cc = ClassifierChain(classifier=RandomForestClassifier())
cc.fit(X_train, y_train)
y_pred = cc.predict(X_test)
```

### Strategy 2: Custom Implementation

**Binary Relevance**:
```python
class BinaryRelevanceClassifier:
    def __init__(self, base_estimator):
        self.base_estimator = base_estimator
        self.classifiers = []

    def fit(self, X, y):
        self.classifiers = []
        for j in range(y.shape[1]):
            clf = clone(self.base_estimator)
            clf.fit(X, y[:, j])
            self.classifiers.append(clf)
        return self

    def predict(self, X):
        predictions = []
        for clf in self.classifiers:
            predictions.append(clf.predict(X))
        return np.column_stack(predictions)
```

### Strategy 3: Native Multi-Output Estimators

**Scikit-Learn Support**:
- `RandomForestClassifier` (supports multi-output natively)
- `ExtraTreesClassifier`
- `DecisionTreeClassifier`
- `MultiOutputClassifier` wrapper for others

**Example**:
```python
from sklearn.ensemble import RandomForestClassifier

# Random Forest handles multi-output natively
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # y_train shape: (n_samples, n_labels)
y_pred = rf.predict(X_test)
```

## Best Practices

### Data Preparation

1. **Check Label Distribution**:
   ```python
   label_frequencies = y.sum(axis=0) / len(y)
   print("Label frequencies:", label_frequencies)
   ```

2. **Analyze Label Correlations**:
   ```python
   co_occurrence = y.T.dot(y)
   sns.heatmap(co_occurrence, annot=True)
   ```

3. **Split Preserving Label Distribution**:
   ```python
   from skmultilearn.model_selection import iterative_train_test_split
   X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size=0.2)
   ```

### Model Selection

1. **Start Simple**: Try Binary Relevance with Random Forest (fast baseline)
2. **Add Complexity**: If label correlations exist, try Classifier Chains
3. **Compare**: Evaluate multiple models with comprehensive metrics

### Evaluation

1. **Use Multiple Metrics**:
   ```python
   from sklearn.metrics import hamming_loss, accuracy_score, f1_score

   print(f"Subset Accuracy: {accuracy_score(y_test, y_pred)}")
   print(f"Hamming Loss: {hamming_loss(y_test, y_pred)}")
   print(f"Micro-F1: {f1_score(y_test, y_pred, average='micro')}")
   print(f"Macro-F1: {f1_score(y_test, y_pred, average='macro')}")
   ```

2. **Per-Label Analysis**:
   ```python
   for j, label_name in enumerate(label_names):
       f1 = f1_score(y_test[:, j], y_pred[:, j])
       print(f"{label_name}: F1 = {f1:.3f}")
   ```

### Threshold Tuning

**Problem**: Default threshold (0.5) may not be optimal for imbalanced labels.

**Solution**:
```python
from sklearn.metrics import precision_recall_curve

# For each label, find optimal threshold
optimal_thresholds = []
for j in range(n_labels):
    precision, recall, thresholds = precision_recall_curve(y_test[:, j], y_prob[:, j])
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    optimal_idx = np.argmax(f1_scores)
    optimal_thresholds.append(thresholds[optimal_idx])

# Apply custom thresholds
y_pred_tuned = (y_prob >= np.array(optimal_thresholds)).astype(int)
```

## Summary

Multi-label classification is essential for realistic manufacturing scenarios where:
- Products can have **multiple defects simultaneously**
- Equipment can have **multiple failure modes**
- Processes can have **multiple concurrent faults**

**Key Takeaways**:
1. **Binary Relevance**: Simple, scalable, ignores correlations
2. **Classifier Chains**: Captures correlations, sensitive to order
3. **Label Powerset**: Perfect correlations, not scalable
4. **MLkNN**: Lazy learning, works well with small data
5. **Evaluation**: Use Subset Accuracy, Hamming Loss, Micro/Macro-F1
6. **Steel Plates Dataset**: Real manufacturing data with 7 fault types

**Next Steps**:
- Implement pipelines in `4.3-multilabel-pipeline.py`
- Experiment with interactive notebook `4.3-multilabel-analysis.ipynb`
- Reference quick guide `4.3-multilabel-quick-ref.md`

## References

1. Tsoumakas, G., & Katakis, I. (2007). Multi-label classification: An overview. *International Journal of Data Warehousing and Mining*, 3(3), 1-13.

2. Zhang, M. L., & Zhou, Z. H. (2014). A review on multi-label learning algorithms. *IEEE Transactions on Knowledge and Data Engineering*, 26(8), 1819-1837.

3. Read, J., Pfahringer, B., Holmes, G., & Frank, E. (2011). Classifier chains for multi-label classification. *Machine Learning*, 85(3), 333-359.

4. Buscema, M., Terzi, S., & Tastle, W. (2010). Steel Plates Faults Dataset. UCI Machine Learning Repository.

5. Scikit-Multilearn Documentation: http://scikit.ml/
