# Module 4.3 Quick Reference: Multi-Label Classification

## What is Multi-Label Classification?

**Problem**: Assign **multiple labels** to a single instance simultaneously.

**Example** (Steel Manufacturing):
- Sample 1: {K_Scratch, Bumps} ✅ TWO defects
- Sample 2: {Stains} ✅ ONE defect  
- Sample 3: {Pastry, Z_Scratch, Dirtiness} ✅ THREE defects

**vs. Multi-Class**: Each instance has exactly ONE class.

## Key Concepts

### Terminology

| Term | Definition | Example |
|------|------------|---------|
| **Label Cardinality** | Average labels per instance | 1.5 labels/sample |
| **Label Density** | Cardinality / Total labels | 1.5/7 = 0.21 |
| **Subset Accuracy** | Exact match ratio | 0.75 (75% perfect predictions) |
| **Hamming Loss** | Fraction of wrong labels | 0.10 (10% error rate) |

### Representation

**Binary Indicator Matrix**:
```
Instance | Scratch | Stain | Bump
---------|---------|-------|------
   1     |    1    |   0   |  1     → {Scratch, Bump}
   2     |    0    |   1   |  0     → {Stain}
   3     |    1    |   1   |  1     → ALL defects
```

## Methods Comparison

### Binary Relevance (BR)

**Approach**: Train **one binary classifier per label**

**Pros**:
- ✅ Simple, scalable
- ✅ Parallelizable
- ✅ Works with any classifier

**Cons**:
- ❌ Ignores label correlations

**When to Use**:
- Many labels (>100)
- Labels are independent
- Speed is critical

**Code**:
```python
from sklearn.ensemble import RandomForestClassifier

pipeline = MultiLabelPipeline(
    method="binary_relevance",
    model="rf",
    n_estimators=100
)
pipeline.fit(X_train, y_train)
```

---

### Classifier Chains (CC)

**Approach**: **Chain classifiers** to capture dependencies

**Process**:
1. Classifier 1: P(Label_1 | features)
2. Classifier 2: P(Label_2 | features, Label_1)
3. Classifier 3: P(Label_3 | features, Label_1, Label_2)

**Pros**:
- ✅ Captures label correlations
- ✅ Often higher accuracy

**Cons**:
- ❌ Sensitive to label order
- ❌ Error propagation

**When to Use**:
- Strong label correlations
- Moderate number of labels (<20)

**Code**:
```python
pipeline = MultiLabelPipeline(
    method="classifier_chains",
    model="rf"
)
pipeline.fit(X_train, y_train)
```

---

### Native Multi-Output

**Approach**: Use classifier's **native multi-output support** (e.g., Random Forest)

**Pros**:
- ✅ Efficient (single model)
- ✅ Handles correlations naturally

**Cons**:
- ❌ Limited to compatible algorithms

**When to Use**:
- Random Forest or Extra Trees
- Moderate labels (<50)

**Code**:
```python
pipeline = MultiLabelPipeline(
    method="native",
    model="rf"
)
pipeline.fit(X_train, y_train)
```

## Evaluation Metrics

### Example-Based Metrics

| Metric | Description | Range | Better |
|--------|-------------|-------|--------|
| **Subset Accuracy** | Exact match ratio | [0, 1] | Higher ⬆ |
| **Hamming Loss** | Fraction of wrong labels | [0, 1] | Lower ⬇ |
| **Micro-F1** | F1 over all labels | [0, 1] | Higher ⬆ |
| **Macro-F1** | Average F1 per label | [0, 1] | Higher ⬆ |

### Subset Accuracy (Strictest)

**Definition**: Fraction of instances with **perfect predictions**

**Formula**:
```
Subset Accuracy = (# exact matches) / (# total instances)
```

**Example**:
```
True:  [1, 0, 1]  vs  Pred: [1, 0, 1]  ✅ Match  
True:  [0, 1, 0]  vs  Pred: [0, 1, 1]  ❌ No match
```

**Interpretation**:
- 0.85 = 85% of predictions are perfect
- Very harsh for many labels

---

### Hamming Loss (Per-Label Error)

**Definition**: Average fraction of **incorrectly predicted labels**

**Formula**:
```
Hamming Loss = (FP + FN) / (n_samples × n_labels)
```

**Example**:
```
True:  [1, 0, 1]  vs  Pred: [1, 0, 0]  → 1 error / 3 labels = 0.33
True:  [0, 1, 0]  vs  Pred: [0, 1, 0]  → 0 errors
Average: 0.33 / 2 = 0.165
```

**Interpretation**:
- 0.10 = 10% of all label predictions are wrong
- Lower is better

---

### Micro vs. Macro F1

**Micro-F1**: Aggregate TP/FP across **all labels**, then compute F1
- Dominated by frequent labels
- Use for imbalanced datasets

**Macro-F1**: Compute F1 **per label**, then average
- Equal weight to all labels
- Use when all labels are equally important

**Example**:
```python
from sklearn.metrics import f1_score

# Micro: Aggregate first
micro_f1 = f1_score(y_true, y_pred, average='micro')

# Macro: Average per-label F1
macro_f1 = f1_score(y_true, y_pred, average='macro')
```

## Steel Plates Dataset

### Overview

**Source**: UCI ML Repository (ID: 198)

**Domain**: Steel manufacturing quality control

**Size**: 1,941 steel plates

**Features**: 27 numeric
- Geometric (X/Y min/max, areas, perimeters)
- Luminosity (sum, min, max, index)
- Steel properties (type, thickness)
- Shape indices (edges, orientation)

**Labels**: 7 binary fault indicators

| Fault Type | Frequency | Description |
|------------|-----------|-------------|
| Pastry | 8.1% | Surface texture defects |
| Z_Scratch | 9.8% | Longitudinal scratches |
| K_Scratch | 20.1% | Transverse scratches |
| Stains | 3.7% | Discoloration defects |
| Dirtiness | 2.8% | Contamination |
| Bumps | 20.7% | Surface elevation defects |
| Other_Faults | 34.7% | Miscellaneous defects |

**Characteristics**:
- **Highly imbalanced** (2.8% to 34.7%)
- **Sparse multi-label** (avg 1.0 label per instance)
- Most samples have **1 label**, some have 2+

### Loading Dataset

```python
from pathlib import Path
import pandas as pd

data_root = Path("datasets/steel-plates")
X = pd.read_csv(data_root / "steel_plates_features.csv")
y = pd.read_csv(data_root / "steel_plates_targets.csv")

print(f"Features: {X.shape}")  # (1941, 27)
print(f"Labels: {y.shape}")    # (1941, 7)
```

## CLI Commands

### Train on Steel Plates

```bash
# Binary Relevance
python 4.3-multilabel-pipeline.py train \
    --dataset steel_plates \
    --method binary_relevance \
    --model rf \
    --n-estimators 200 \
    --model-out models/steel_br.joblib

# Classifier Chains
python 4.3-multilabel-pipeline.py train \
    --dataset steel_plates \
    --method classifier_chains \
    --model-out models/steel_cc.joblib

# Native Multi-Output
python 4.3-multilabel-pipeline.py train \
    --dataset steel_plates \
    --method native \
    --model-out models/steel_native.joblib
```

### Evaluate Model

```bash
python 4.3-multilabel-pipeline.py evaluate \
    --model-path models/steel_br.joblib \
    --dataset steel_plates
```

**Output**:
```json
{
  "metrics": {
    "subset_accuracy": 0.82,
    "hamming_loss": 0.06,
    "micro_f1": 0.89,
    "macro_f1": 0.75,
    "per_label": {
      "K_Scratch": {"f1": 0.85, "support": 78},
      "Bumps": {"f1": 0.88, "support": 80}
    }
  }
}
```

### Predict Single Instance

```bash
python 4.3-multilabel-pipeline.py predict \
    --model-path models/steel_br.joblib \
    --input-json '{"X_Min": 42, "X_Max": 308, "Y_Min": 15, ...}'
```

**Output**:
```json
{
  "predicted_labels": ["K_Scratch", "Bumps"],
  "label_scores": {
    "Pastry": 0.12,
    "K_Scratch": 0.87,
    "Bumps": 0.74,
    ...
  }
}
```

### Analyze Dataset

```bash
python 4.3-multilabel-pipeline.py analyze \
    --dataset steel_plates
```

**Output**:
```json
{
  "analysis": {
    "n_samples": 1941,
    "n_labels": 7,
    "label_cardinality": 1.0,
    "label_density": 0.14,
    "label_frequencies": {
      "Pastry": 0.081,
      "K_Scratch": 0.201,
      ...
    },
    "top_co_occurrences": [
      {"label1": "K_Scratch", "label2": "Bumps", "count": 45}
    ]
  }
}
```

## Common Patterns

### Pattern 1: Handling Label Imbalance

**Problem**: Rare labels (Dirtiness: 2.8%) are hard to learn

**Solutions**:

1. **Class Weights**:
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight

weights = compute_class_weight('balanced', classes=[0, 1], y=y_train[:, j])
clf = RandomForestClassifier(class_weight={0: weights[0], 1: weights[1]})
```

2. **Threshold Tuning**:
```python
# Lower threshold for rare labels
thresholds = [0.3 if freq < 0.05 else 0.5 for freq in label_frequencies]
y_pred = (y_proba >= thresholds).astype(int)
```

3. **Oversampling Rare Labels**:
```python
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_resample(X_train, y_train[:, j])
```

---

### Pattern 2: Ensemble of Classifier Chains

**Problem**: Classifier chains are sensitive to label order

**Solution**: Train multiple chains with random orders, then vote

```python
from sklearn.ensemble import VotingClassifier

# Train 5 chains with different random orders
chains = []
for i in range(5):
    cc = ClassifierChainsClassifier(random_order=True)
    cc.fit(X_train, y_train)
    chains.append(cc)

# Aggregate predictions by voting
y_pred_ensemble = np.mean([cc.predict(X_test) for cc in chains], axis=0)
y_pred_ensemble = (y_pred_ensemble > 0.5).astype(int)
```

---

### Pattern 3: Per-Label Analysis

**Problem**: Need to identify which labels are hard to predict

**Solution**: Compute metrics per label

```python
from sklearn.metrics import f1_score

for j, label in enumerate(label_names):
    f1 = f1_score(y_test[:, j], y_pred[:, j])
    support = y_test[:, j].sum()
    print(f"{label:15s} F1: {f1:.3f}  Support: {support}")
```

**Output**:
```
Pastry          F1: 0.650  Support: 16
Z_Scratch       F1: 0.720  Support: 19
K_Scratch       F1: 0.850  Support: 78  ✅ Good
Stains          F1: 0.580  Support: 7   ⚠️ Low support
Dirtiness       F1: 0.450  Support: 5   ⚠️ Hard label
Bumps           F1: 0.880  Support: 80  ✅ Good
Other_Faults    F1: 0.910  Support: 135 ✅ Excellent
```

## Semiconductor Applications

### Use Case 1: Wafer Defect Detection

**Scenario**: Wafers can have multiple defect patterns

**Labels**: {Center, Edge, Scratch, Particle, Discoloration}

**Method**: Classifier Chains (defects often co-occur)

**Metrics**: Micro-F1 (frequent defects more critical)

---

### Use Case 2: Equipment Health Monitoring

**Scenario**: Equipment can have multiple failing subsystems

**Labels**: {Pump_Failure, Heater_Failure, Valve_Leak, Chamber_Contamination}

**Method**: Binary Relevance (subsystems independent)

**Metrics**: Subset Accuracy (need all failures identified)

---

### Use Case 3: Process Fault Diagnosis

**Scenario**: Multiple process faults simultaneously

**Labels**: {Temp_Deviation, Pressure_Instability, Chemical_Imbalance, Timing_Error}

**Method**: Native Multi-Output (Random Forest)

**Metrics**: Macro-F1 (all faults equally important)

## Best Practices

1. **Start Simple**: Binary Relevance with Random Forest (fast baseline)
2. **Analyze Labels**: Check correlations before choosing method
3. **Handle Imbalance**: Use class weights or threshold tuning
4. **Use Multiple Metrics**: Subset Accuracy + Hamming Loss + Micro/Macro-F1
5. **Per-Label Analysis**: Identify hard-to-predict labels
6. **Cross-Validation**: Use iterative stratification for multi-label
7. **Threshold Tuning**: Optimize decision thresholds per label

## Troubleshooting

**Problem**: Low Subset Accuracy (e.g., 0.35)

**Solution**: Subset accuracy is harsh. Check Hamming Loss and F1 scores.

---

**Problem**: Some labels have F1 = 0

**Solution**: Rare labels with few samples. Use oversampling or class weights.

---

**Problem**: Classifier Chains worse than Binary Relevance

**Solution**: Label order may be suboptimal. Try Ensemble of Classifier Chains.

---

**Problem**: Long training time

**Solution**: Use Binary Relevance (parallelizable) or reduce n_estimators.

## Summary Cheatsheet

| **Aspect** | **Binary Relevance** | **Classifier Chains** | **Native** |
|------------|---------------------|----------------------|-----------|
| **Complexity** | Simple | Medium | Simple |
| **Correlations** | ❌ Ignored | ✅ Modeled | ✅ Modeled |
| **Scalability** | ✅ Excellent | ⚠️ Moderate | ✅ Good |
| **Parallelizable** | ✅ Yes | ❌ No | ⚠️ Limited |
| **Best For** | Many labels, independent | Correlated labels | Moderate labels, efficiency |

**Metrics**:
- **Subset Accuracy**: Exact match (strict)
- **Hamming Loss**: Per-label error (lenient)
- **Micro-F1**: Frequent labels weighted more
- **Macro-F1**: All labels weighted equally

**Dataset**:
- Steel Plates: 1,941 samples, 27 features, 7 labels
- Highly imbalanced (2.8% to 34.7%)
- Use class weights or threshold tuning

## Further Reading

- **Tsoumakas & Katakis (2007)**: Multi-Label Classification Overview
- **Read et al. (2011)**: Classifier Chains for Multi-Label Classification
- **Scikit-Multilearn Docs**: http://scikit.ml/
- **Module 4.3 Fundamentals**: Complete theory and mathematics
