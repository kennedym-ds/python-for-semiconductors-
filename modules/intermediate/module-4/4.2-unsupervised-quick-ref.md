# 4.2 Unsupervised Learning Quick Reference

Fast cheat sheet for clustering + anomaly detection in semiconductor manufacturing feature spaces.

## Core CLI Patterns

Train (synthetic teaching dataset):

```bash
python 4.2-unsupervised-pipeline.py train --dataset synthetic_process --model kmeans --k 6 --save kmeans.joblib
```

Evaluate saved model:

```bash
python 4.2-unsupervised-pipeline.py evaluate --model-path kmeans.joblib --dataset synthetic_process
```

Predict for single record (JSON inline):

```bash
python 4.2-unsupervised-pipeline.py predict --model-path kmeans.joblib --input-json '{"f1":0.12,"f2":-1.3,"f3":0.44,"f4":1.02,"f5":-0.2}'
```

Key flags:

- `--model {kmeans,gmm,dbscan,iso_forest,kmeans_iso}`
- `--k <int>` clusters (kmeans/gmm)
- `--pca-variance 0.95` retain variance (disable with `--no-pca`)
- `--dbscan-eps 0.9 --dbscan-min-samples 12`
- `--iso-estimators 200`

## Output JSON (train/evaluate)

```json
{
  "status": "trained|evaluated",
  "model": "kmeans",
  "metrics": { "silhouette": 0.42, "largest_cluster_fraction": 0.31 },
  "metadata": { "model": "kmeans", "pca_variance": 0.95 },
  "cluster_distribution": {"0":0.18,"1":0.16},
  "warnings": ["degenerate_cluster:..." ]
}
```

## Internal Validation Metrics

| Metric | Meaning | Desired Direction | Notes |
|--------|---------|-------------------|-------|
| silhouette | Separation vs cohesion | Higher (<=1) | <0.05 => low structure warning |
| calinski_harabasz | Between / within dispersion | Higher | Sensitive to cluster count |
| davies_bouldin | Avg worst similarity | Lower (>=0) | 0 is best |
| cluster_size_entropy | Evenness of cluster sizes (0-1) | Higher | 0 = one giant cluster |
| largest_cluster_fraction | Fraction of dominant cluster | Lower | >0.85 => degenerate warning |
| anomaly_ratio | Fraction labeled -1 (noise) | Contextual | >0.15 => high anomaly drift |

## Model Selection Guidance

| Scenario | Recommended Model | Rationale |
|----------|-------------------|-----------|
| Rough segmentation, speed | kmeans | Stable, fast, baseline |
| Elliptical latent structure | gmm | Uses covariance, soft clustering |
| Irregular density regions + noise | dbscan | Detects arbitrary shapes, isolates noise |
| Pure anomaly scoring | iso_forest | High-dimensional isolation-based detection |
| Clusters + anomaly scores | kmeans_iso | Separate structure + residual outliers |

## Manufacturing Guardrails

- `largest_cluster_fraction > 0.85` => Possible collapse / tool drift dominating
- `silhouette < 0.05` => Structure weak; re-examine scaling or dimensionality
- `anomaly_ratio > 0.15` => Global drift / instability (check equipment state)

## Synthetic Dataset Anatomy

Columns: f1..fN plus indicators:

- `is_drift`: sample inside drifted process window
- `is_injected_anomaly`: injected extreme outlier

Use drift/anomaly flags only for instructional validation (not fed to model).

## Rapid Troubleshooting

| Symptom | Check | Fix |
|---------|-------|-----|
| All samples single cluster | largest_cluster_fraction ~1 | Increase k / adjust eps / enable PCA |
| Many -1 noise labels (DBSCAN) | anomaly_ratio high | Increase eps or decrease min_samples |
| Silhouette NaN | <2 effective clusters | Adjust parameters / model choice |
| High anomaly_ratio (isolation) | Upstream process shift | Recalibrate baseline, re-fit after confirming stability |

## Adding Real Data Later

1. Load dataset to DataFrame with only numeric engineered features.
2. Run `train` with appropriate model.
3. Track metrics & warnings baseline over time.
4. Version persisted models in `temp_models/` (or dedicated registry path).

## Minimal Python API Usage

```python
from pathlib import Path
import pandas as pd
from 4_2_unsupervised_pipeline import UnsupervisedPipeline

X = pd.read_csv('my_features.csv')
pipe = UnsupervisedPipeline(model='kmeans', n_clusters=6).fit(X)
metrics = pipe.evaluate(X)
pipe.save(Path('kmeans_model.joblib'))
```

## When to Disable PCA

- Very low feature dimensionality (<8)
- Features already orthogonal / engineered
- Need interpretability directly in original space

## Interpret Cluster Size Entropy

- ~1.0: balanced clusters
- 0.6â€“0.8: mild imbalance (often acceptable)
- <0.4: potential mode collapse / parameter issue

## Next Extensions (Future Modules)

- HDBSCAN for variable density
- UMAP embedding for visualization
- Consensus clustering for stability
- Time-window drift tracking

---
Keep outputs machine-readable (JSON) for integration in later MLOps modules.
