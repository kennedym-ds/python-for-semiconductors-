# Module 11.1 ‚Äì Edge AI for Inline Inspection Quick Reference

## üéØ Key Concepts

**Edge AI**: Deploy AI models directly on devices at the point of data generation to minimize latency and bandwidth requirements.

**Target Performance**: Sub-millisecond inference latency with 99.9% uptime reliability for production semiconductor inspection.

**Core Technologies**: Model quantization, ONNX deployment, real-time streaming, container orchestration.

## üèóÔ∏è Pipeline Overview

```python
# Complete edge AI inspection workflow
from modules.cutting_edge.module_11 import EdgeAIInspectionPipeline

# 1. Train model
config = EdgeAIInspectionConfig(target_latency_ms=50.0)
pipeline = EdgeAIInspectionPipeline(config)
pipeline.fit(X_train, y_train)

# 2. Quantize for edge deployment
onnx_path = pipeline.quantize_model(target_device="cpu")

# 3. Deploy with streaming processor
processor = StreamingDataProcessor(config)
processor.start_streaming(model_session)
```

## üìä Hardware Platforms

### CPU-Based Edge Devices
| Platform | Use Case | Latency | Power | Cost |
|----------|----------|---------|-------|------|
| Intel Xeon-D | High-performance | 10-50ms | 45W | $$$ |
| ARM Cortex-A78 | Power-efficient | 20-100ms | 5W | $ |
| Raspberry Pi 4 | Development/Testing | 50-200ms | 3W | $ |

### GPU-Based Edge Devices  
| Platform | Use Case | Latency | Power | Cost |
|----------|----------|---------|-------|------|
| NVIDIA Jetson Orin | Deep learning | 1-10ms | 30W | $$$ |
| NVIDIA Jetson Xavier | Computer vision | 5-20ms | 20W | $$ |
| Intel NCS2 | Ultra-low power | 10-50ms | 2W | $ |

### Specialized AI Accelerators
| Platform | Use Case | Latency | Power | Cost |
|----------|----------|---------|-------|------|
| Google Coral TPU | TensorFlow models | 2-15ms | 2W | $ |
| Intel Movidius | Vision processing | 5-30ms | 1W | $ |
| FPGA (Xilinx) | Custom acceleration | <1ms | 10-50W | $$$$ |

## üîß Model Optimization

### Quantization Quick Setup
```python
# Dynamic quantization (easiest)
import torch.quantization as quant
quantized_model = quant.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# ONNX quantization
from onnxruntime.quantization import quantize_dynamic
quantize_dynamic("model.onnx", "quantized_model.onnx")
```

### Model Pruning
```python
import torch.nn.utils.prune as prune

# Remove 30% of smallest weights
prune.l1_unstructured(model.conv1, name='weight', amount=0.3)
prune.remove(model.conv1, 'weight')  # Make permanent
```

### Knowledge Distillation
```python
# Train small student model from large teacher
teacher_output = teacher_model(x)
student_output = student_model(x)

distillation_loss = F.kl_div(
    F.log_softmax(student_output/T, dim=1),
    F.softmax(teacher_output/T, dim=1)
) * T**2
```

## üöÄ ONNX Deployment

### Convert Models to ONNX
```python
# PyTorch to ONNX
torch.onnx.export(model, dummy_input, "model.onnx", opset_version=11)

# Scikit-learn to ONNX  
from skl2onnx import convert_sklearn
onnx_model = convert_sklearn(sklearn_model, initial_types=[...])
```

### Optimized ONNX Runtime
```python
import onnxruntime as ort

# High-performance session configuration
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = 4
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

session = ort.InferenceSession("model.onnx", sess_options)
```

### Execution Providers
```python
# CPU (default)
providers = ['CPUExecutionProvider']

# NVIDIA GPU
providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  

# TensorRT (NVIDIA optimized)
providers = ['TensorrtExecutionProvider', 'CUDAExecutionProvider']

# Intel OpenVINO
providers = ['OpenVINOExecutionProvider', 'CPUExecutionProvider']

session = ort.InferenceSession("model.onnx", providers=providers)
```

## ‚ö° Real-Time Streaming

### Async Processing Pattern
```python
import asyncio
import queue

class StreamingProcessor:
    def __init__(self, model_session, max_latency_ms=50):
        self.model_session = model_session
        self.max_latency_ms = max_latency_ms
        self.queue = asyncio.Queue(maxsize=1000)
        
    async def process_stream(self):
        while True:
            # Batch processing with timeout
            batch = []
            deadline = time.time() + self.max_latency_ms/1000
            
            while time.time() < deadline and len(batch) < 32:
                try:
                    item = await asyncio.wait_for(
                        self.queue.get(), 
                        timeout=deadline - time.time()
                    )
                    batch.append(item)
                except asyncio.TimeoutError:
                    break
                    
            if batch:
                await self._process_batch(batch)
```

### Kafka Integration
```python
from kafka import KafkaConsumer, KafkaProducer

consumer = KafkaConsumer(
    'wafer_images',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

for message in consumer:
    # Process wafer image
    result = process_image(message.value)
    
    # Send result
    producer.send('inspection_results', result)
```

## üìà Performance Monitoring

### Latency Tracking
```python
class LatencyMonitor:
    def __init__(self):
        self.latencies = []
        
    def record(self, latency_ms):
        self.latencies.append(latency_ms)
        if len(self.latencies) > 1000:
            self.latencies = self.latencies[-1000:]
            
    def get_stats(self):
        return {
            'mean': np.mean(self.latencies),
            'p95': np.percentile(self.latencies, 95),
            'p99': np.percentile(self.latencies, 99)
        }
```

### Memory Optimization
```python
# Pre-allocate buffers to avoid garbage collection
class OptimizedProcessor:
    def __init__(self, batch_size=32, image_size=(224, 224)):
        # Pre-allocate input batch buffer
        self.input_buffer = np.zeros(
            (batch_size, 1, *image_size), 
            dtype=np.float32
        )
        
        # Object pool for temporary arrays
        self.temp_pool = queue.Queue()
        for _ in range(100):
            self.temp_pool.put(np.zeros(image_size, dtype=np.uint8))
```

## üè≠ Fab Integration

### MES Integration
```python
class MESReporter:
    def __init__(self, mes_endpoint, api_key):
        self.mes_url = mes_endpoint
        self.headers = {'Authorization': f'Bearer {api_key}'}
        
    def report_inspection(self, wafer_id, defect_type, confidence):
        payload = {
            'wafer_id': wafer_id,
            'defect_type': defect_type,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        }
        
        response = requests.post(
            f'{self.mes_url}/api/inspection_results',
            json=payload,
            headers=self.headers,
            timeout=2.0  # Fast timeout for real-time
        )
        
        return response.status_code == 200
```

### Process Control Actions
```python
# Automated responses based on inspection results
def handle_inspection_result(result):
    if result['defect_type'] == 'critical' and result['confidence'] > 0.9:
        # Immediate process hold
        mes.trigger_hold(result['lot_id'], reason='Critical defect detected')
        
    elif result['confidence'] < 0.5:
        # Flag for manual review
        mes.flag_for_review(result['wafer_id'], reason='Low confidence')
        
    # Update SPC charts
    spc.update_control_chart(result['defect_type'], result['confidence'])
```

## üê≥ Container Deployment

### Docker Configuration
```dockerfile
FROM python:3.11-slim

# Install ONNX Runtime
RUN pip install onnxruntime numpy opencv-python

# Copy model and application
COPY model.onnx /models/
COPY app.py /app/

# Set resource limits
ENV OMP_NUM_THREADS=4
ENV MALLOC_TRIM_THRESHOLD_=100000

EXPOSE 8080
CMD ["python", "/app/app.py"]
```

### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-ai-inspection
spec:
  replicas: 3
  selector:
    matchLabels:
      app: edge-ai-inspection
  template:
    metadata:
      labels:
        app: edge-ai-inspection
    spec:
      containers:
      - name: inspector
        image: edge-ai-inspection:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        env:
        - name: TARGET_LATENCY_MS
          value: "50"
        - name: CONFIDENCE_THRESHOLD
          value: "0.7"
        ports:
        - containerPort: 8080
      nodeSelector:
        hardware: edge-device
```

## üîí Security Best Practices

### Model Verification
```python
import hashlib

def verify_model_integrity(model_path, expected_hash):
    """Verify model hasn't been tampered with"""
    with open(model_path, 'rb') as f:
        model_data = f.read()
        actual_hash = hashlib.sha256(model_data).hexdigest()
        
    if actual_hash != expected_hash:
        raise SecurityError("Model integrity check failed")
    
    return True
```

### Secure Communication
```python
import ssl
import requests

# Use TLS for all API communications
session = requests.Session()
session.verify = True  # Verify SSL certificates
session.headers.update({'User-Agent': 'EdgeAI-Inspector/1.0'})

# Certificate pinning for critical connections
context = ssl.create_default_context()
context.check_hostname = False
context.verify_mode = ssl.CERT_REQUIRED
```

## ‚ö†Ô∏è Troubleshooting Guide

### High Latency Issues
```python
# Check common latency problems
def diagnose_latency():
    issues = []
    
    # Model complexity
    if model_size_mb > 100:
        issues.append("Model too large - consider quantization")
        
    # Batch size
    if batch_size > 1:
        issues.append("Batch processing adds latency - use batch_size=1")
        
    # Memory allocation
    if gc_count > 10:
        issues.append("Frequent GC - pre-allocate buffers")
        
    return issues
```

### Memory Issues
```python
# Monitor memory usage
import psutil

def check_memory():
    memory = psutil.virtual_memory()
    if memory.percent > 90:
        logger.warning(f"High memory usage: {memory.percent}%")
        
    # Check for memory leaks
    if memory.available < 100 * 1024 * 1024:  # < 100MB
        logger.error("Low memory - possible leak")
```

### Model Loading Problems
```python
def validate_onnx_model(model_path):
    """Validate ONNX model before deployment"""
    try:
        import onnx
        model = onnx.load(model_path)
        onnx.checker.check_model(model)
        
        # Test inference
        session = ort.InferenceSession(model_path)
        dummy_input = np.random.randn(1, 64).astype(np.float32)
        result = session.run(None, {'input': dummy_input})
        
        return True
    except Exception as e:
        logger.error(f"Model validation failed: {e}")
        return False
```

## üìã Performance Targets

### Latency Requirements
- **Critical Processes**: < 10ms (FPGA/specialized hardware)
- **Standard Inspection**: < 50ms (GPU/optimized CPU)
- **Batch Processing**: < 100ms (CPU-only systems)

### Throughput Targets
- **High-volume Fabs**: > 1000 wafers/hour
- **Standard Production**: > 500 wafers/hour  
- **Development Lines**: > 100 wafers/hour

### Reliability Standards
- **Uptime**: 99.9% (< 8.7 hours downtime/year)
- **Error Rate**: < 0.1% false classifications
- **Recovery Time**: < 30 seconds after failure

## üéõÔ∏è Configuration Examples

### High-Performance Setup
```python
config = EdgeAIInspectionConfig(
    model_type="efficient_net",
    target_latency_ms=10.0,
    edge_device="gpu",
    quantization_method="static",
    batch_size=1,
    memory_limit_mb=1024
)
```

### Low-Power Setup  
```python
config = EdgeAIInspectionConfig(
    model_type="mobilenet_v3",
    target_latency_ms=100.0,
    edge_device="cpu",
    quantization_method="dynamic", 
    batch_size=4,
    memory_limit_mb=256
)
```

### Development Setup
```python
config = EdgeAIInspectionConfig(
    model_type="random_forest",
    target_latency_ms=50.0,
    edge_device="cpu",
    batch_size=1,
    memory_limit_mb=512,
    enable_debugging=True
)
```

## üîÑ Command-Line Usage

```bash
# Train edge AI model
python 11.1-edge-ai-inspection-pipeline.py train \
    --model random_forest \
    --target-latency 50 \
    --save edge_model.joblib

# Quantize for deployment
python 11.1-edge-ai-inspection-pipeline.py quantize \
    --model-path edge_model.joblib \
    --target-device cpu \
    --output quantized.onnx

# Test streaming performance
python 11.1-edge-ai-inspection-pipeline.py stream \
    --model-path quantized.onnx \
    --latency-target 50 \
    --kafka-topic wafer_images

# Generate deployment config
python 11.1-edge-ai-inspection-pipeline.py deploy \
    --model-path quantized.onnx \
    --edge-config deployment.json
```

## üèÜ Best Practices

1. **Start Simple**: Begin with CPU deployment before moving to specialized hardware
2. **Measure First**: Profile performance before optimizing
3. **Batch Wisely**: Balance latency vs. throughput with appropriate batch sizes
4. **Monitor Continuously**: Track latency, memory, and error rates in production
5. **Plan for Failures**: Implement fallback mechanisms and graceful degradation
6. **Security First**: Verify model integrity and use secure communications
7. **Test Thoroughly**: Validate performance under realistic fab conditions

## üîç Key Metrics to Track

- **P95 Inference Latency**: < target threshold (e.g., 50ms)
- **Memory Usage**: < configured limit (e.g., 512MB)
- **CPU Utilization**: 60-80% (avoid saturation)
- **Error Rate**: < 0.1% false classifications
- **Uptime**: > 99.9% availability
- **Throughput**: Wafers processed per hour
- **Model Accuracy**: Maintain production quality standards