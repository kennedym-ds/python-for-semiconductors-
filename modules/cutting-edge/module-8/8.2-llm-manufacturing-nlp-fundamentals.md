# Module 8.2 - LLMs for Manufacturing NLP: Fundamentals

## Overview

This module introduces Natural Language Processing (NLP) and Large Language Model (LLM) techniques specifically applied to manufacturing environments. We focus on processing text data common in semiconductor fabrication facilities, including maintenance logs, shift reports, and equipment alerts.

## Manufacturing Text Data Types

### 1. Maintenance Logs
- **Equipment status updates**: "Pump P-101 completed routine maintenance check"
- **Alarm reports**: "Reactor R-204 pressure reading elevated by 5°C from baseline"
- **Incident reports**: "CVD-301 emergency shutdown due to contamination detection"

### 2. Shift Reports
- **Production summaries**: Daily/nightly operations overview
- **Yield metrics**: Process performance indicators
- **Tool status**: Equipment availability and issues
- **Personnel actions**: Technician interventions and maintenance

### 3. Process Notes
- **Parameter adjustments**: Manual process modifications
- **Quality observations**: Visual inspection results
- **Troubleshooting logs**: Problem diagnosis and resolution

## NLP Fundamentals for Manufacturing

### Tokenization
Breaking text into meaningful units (words, subwords, or characters):

```python
# Word tokenization
text = "Etcher E-301 showing unusual vibration patterns"
tokens = text.split()  # ['Etcher', 'E-301', 'showing', 'unusual', 'vibration', 'patterns']

# Manufacturing-specific considerations:
# - Equipment IDs: "P-101", "R-204" should remain intact
# - Technical terms: "CVD", "PVD", "RIE" are domain-specific
# - Units: "°C", "Torr", "sccm" need special handling
```

### Text Preprocessing for Manufacturing

1. **Equipment ID Normalization**
   ```python
   import re
   
   def normalize_equipment_id(text):
       # Convert various formats to standard
       text = re.sub(r'([A-Z]+)-?(\d+)', r'\1-\2', text)
       return text
   ```

2. **Technical Abbreviation Expansion**
   ```python
   manufacturing_terms = {
       'CVD': 'Chemical Vapor Deposition',
       'PVD': 'Physical Vapor Deposition', 
       'RIE': 'Reactive Ion Etching',
       'CMP': 'Chemical Mechanical Planarization'
   }
   ```

3. **Unit Standardization**
   ```python
   def standardize_units(text):
       # Convert temperature units
       text = re.sub(r'(\d+)\s*degrees?\s*[Cc]', r'\1°C', text)
       text = re.sub(r'(\d+)\s*[Ff]ahrenheit', r'\1°F', text)
       return text
   ```

## Feature Extraction Methods

### 1. Classical Approaches

#### TF-IDF (Term Frequency-Inverse Document Frequency)
Best for structured manufacturing reports where keyword frequency matters:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Manufacturing-optimized TF-IDF
vectorizer = TfidfVectorizer(
    max_features=1000,
    stop_words='english',
    ngram_range=(1, 2),  # Include bigrams for "emergency shutdown"
    min_df=2,  # Ignore very rare terms
    max_df=0.95  # Ignore too common terms
)
```

**Advantages:**
- Fast training and inference
- Interpretable features (can see which words matter)
- No external dependencies
- Works well with limited data

**Use Cases:**
- Equipment type classification
- Severity level detection
- Keyword-based alert routing

#### Bag of Words + N-grams
Captures important manufacturing phrases:

```python
# Important manufacturing bigrams
manufacturing_phrases = [
    "emergency shutdown", "parameter drift", "process deviation",
    "contamination detected", "vacuum loss", "temperature spike"
]
```

### 2. Modern Transformer Approaches

#### Pre-trained Models
For manufacturing NLP, smaller models often work better:

- **DistilBERT**: Faster, lighter version of BERT
- **TinyBERT**: Even smaller for edge deployment
- **RoBERTa-base**: Robust optimization of BERT

#### Domain Adaptation Strategies

1. **Vocabulary Extension**
   ```python
   # Add manufacturing-specific tokens
   manufacturing_vocab = [
       "sccm", "torr", "angstrom", "wafer", "fab", "cleanroom",
       "CVD", "PVD", "etch", "litho", "CMP", "implant"
   ]
   ```

2. **Continued Pre-training**
   Fine-tune on manufacturing text corpus before task-specific training.

3. **Task-specific Fine-tuning**
   Train on labeled manufacturing data for classification/summarization.

## Key NLP Tasks in Manufacturing

### 1. Text Classification

#### Severity Level Classification
```python
# 3-class severity
severity_labels = {
    0: "Low",      # Routine maintenance, normal operations
    1: "Medium",   # Parameter drift, minor issues
    2: "High"      # Emergency shutdown, critical failures
}

# Features that indicate high severity
high_severity_keywords = [
    "emergency", "critical", "shutdown", "failure", "alarm",
    "exceeded", "contamination", "overheating"
]
```

#### Equipment Area Classification
```python
# 5-class tool area
tool_areas = {
    0: "Wet Bench",    # Chemical processing
    1: "Lithography",  # Pattern definition
    2: "Etch",         # Material removal
    3: "Deposition",   # Material addition
    4: "Metrology"     # Measurement and inspection
}
```

#### Root Cause Classification
```python
# Common failure modes
failure_modes = [
    "mechanical", "electrical", "thermal", "contamination",
    "process", "operator_error", "material_defect"
]
```

### 2. Text Summarization

#### Extractive Summarization
Select important sentences from original text:

```python
def extractive_summary(text, n_sentences=2):
    sentences = text.split('.')
    # Score sentences by keyword importance
    scores = []
    for sentence in sentences:
        score = count_important_keywords(sentence)
        scores.append(score)
    
    # Return top N sentences
    top_indices = sorted(range(len(scores)), 
                        key=lambda i: scores[i], reverse=True)[:n_sentences]
    return '. '.join([sentences[i] for i in sorted(top_indices)])
```

#### Abstractive Summarization
Generate new text that captures key information:

```python
# Using transformers
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary = summarizer(shift_report, max_length=50, min_length=10)
```

### 3. Named Entity Recognition (NER)

Extract structured information from unstructured text:

```python
# Manufacturing entities
entity_types = {
    "EQUIPMENT": ["Pump P-101", "Reactor R-204"],
    "PARAMETER": ["temperature", "pressure", "flow rate"],
    "VALUE": ["450°C", "2.5 Torr", "120 sccm"],
    "TIME": ["day shift", "22:30", "2 hours"],
    "PERSON": ["Technician John", "Operator Sarah"]
}
```

## Evaluation Metrics

### Classification Metrics

1. **Standard Metrics**
   - **Accuracy**: Overall correct predictions
   - **F1-Score**: Harmonic mean of precision and recall
   - **ROC-AUC**: Area under ROC curve

2. **Manufacturing-Specific Metrics**
   - **PWS (Prediction Within Spec)**: Percentage meeting tolerance
   - **Estimated Loss**: Cost impact of misclassification
   
   ```python
   def calculate_estimated_loss(y_true, y_pred, cost_matrix):
       # High severity missed as low = high cost
       costs = {
           (2, 0): 1000,  # Critical missed as routine
           (2, 1): 500,   # Critical missed as medium  
           (1, 0): 100,   # Medium missed as routine
           # Other misclassifications have lower costs
       }
       
       total_cost = 0
       for true, pred in zip(y_true, y_pred):
           if (true, pred) in costs:
               total_cost += costs[(true, pred)]
       
       return total_cost / len(y_true)
   ```

### Summarization Metrics

1. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**
   ```python
   # ROUGE-1: Unigram overlap
   # ROUGE-2: Bigram overlap  
   # ROUGE-L: Longest common subsequence
   
   def rouge_1(reference, candidate):
       ref_words = set(reference.lower().split())
       cand_words = set(candidate.lower().split())
       overlap = len(ref_words.intersection(cand_words))
       return overlap / len(ref_words) if ref_words else 0
   ```

2. **Manufacturing-Specific Metrics**
   - **Key Information Retention**: Captures critical details
   - **Actionability Score**: Summary enables decision-making
   
   ```python
   def actionability_score(summary):
       # Check for actionable elements
       actionable_terms = [
           "requires", "investigate", "repair", "replace", 
           "monitor", "adjust", "calibrate"
       ]
       
       score = sum(1 for term in actionable_terms 
                  if term in summary.lower())
       return min(score / 3, 1.0)  # Normalize to [0,1]
   ```

## Privacy and Security Considerations

### Data Sensitivity
Manufacturing data often contains:
- **Proprietary process parameters**
- **Equipment performance data**
- **Production metrics**
- **Personnel information**

### Mitigation Strategies

1. **Data Anonymization**
   ```python
   def anonymize_text(text):
       # Replace specific equipment IDs with generic ones
       text = re.sub(r'[A-Z]+-\d+', 'EQUIPMENT-XXX', text)
       
       # Remove personal names
       text = re.sub(r'Technician \w+', 'Technician XXX', text)
       
       # Mask specific values while preserving patterns
       text = re.sub(r'\d+\.?\d*°C', 'XXX°C', text)
       
       return text
   ```

2. **On-Premise Deployment**
   - Host models locally instead of cloud APIs
   - Use smaller, efficient models that run on facility hardware
   - Implement secure model serving infrastructure

3. **Differential Privacy**
   ```python
   def add_noise_to_embeddings(embeddings, epsilon=1.0):
       # Add calibrated noise to protect individual data points
       noise_scale = 2.0 / epsilon  # Based on sensitivity
       noise = np.random.laplace(0, noise_scale, embeddings.shape)
       return embeddings + noise
   ```

## Prompt Engineering for Manufacturing

### Effective Prompting Patterns

1. **Task-Specific Instructions**
   ```python
   classification_prompt = """
   Classify the following maintenance log by severity level:
   - Low (0): Routine maintenance, normal operations
   - Medium (1): Minor issues, parameter drift
   - High (2): Emergency situations, critical failures
   
   Log: {text}
   Severity: """
   ```

2. **Few-Shot Examples**
   ```python
   few_shot_prompt = """
   Examples:
   Log: "Pump P-101 completed routine check" → Severity: Low
   Log: "Reactor showing temperature drift" → Severity: Medium  
   Log: "Emergency shutdown due to contamination" → Severity: High
   
   Log: {text} → Severity: """
   ```

3. **Chain-of-Thought Reasoning**
   ```python
   cot_prompt = """
   Analyze this maintenance log step by step:
   1. Identify the equipment mentioned
   2. Determine what happened
   3. Assess the urgency level
   4. Classify the severity
   
   Log: {text}
   Analysis: """
   ```

### Guardrails and Validation

1. **Output Format Validation**
   ```python
   def validate_classification(output):
       valid_severities = ["Low", "Medium", "High", "0", "1", "2"]
       return any(severity in output for severity in valid_severities)
   ```

2. **Confidence Thresholding**
   ```python
   def check_confidence(prediction_proba, threshold=0.8):
       max_confidence = max(prediction_proba)
       if max_confidence < threshold:
           return "UNCERTAIN"
       return prediction_proba.argmax()
   ```

3. **Human-in-the-Loop**
   ```python
   def require_human_review(prediction, confidence):
       # Critical decisions need human oversight
       if prediction == "High" and confidence < 0.9:
           return True
       if prediction == "UNCERTAIN":
           return True
       return False
   ```

## Implementation Best Practices

### 1. Start Simple
- Begin with classical methods (TF-IDF + Logistic Regression)
- Establish baseline performance
- Understand data characteristics and common patterns

### 2. Progressive Enhancement
- Add preprocessing specific to manufacturing domain
- Experiment with n-grams and feature engineering
- Consider ensemble methods combining multiple approaches

### 3. Model Selection Criteria
- **Latency requirements**: Real-time vs. batch processing
- **Accuracy needs**: Safety-critical vs. informational
- **Resource constraints**: Edge devices vs. server deployment
- **Interpretability**: Regulatory compliance requirements

### 4. Continuous Learning
```python
class ContinualLearningPipeline:
    def __init__(self):
        self.model = None
        self.feedback_buffer = []
    
    def update_with_feedback(self, text, true_label, predicted_label):
        if true_label != predicted_label:
            self.feedback_buffer.append((text, true_label))
            
        # Retrain when enough new data accumulated
        if len(self.feedback_buffer) >= 100:
            self.retrain()
    
    def retrain(self):
        # Combine original training data with feedback
        # Retrain model with updated dataset
        pass
```

## Deployment Considerations

### Edge Deployment
For fab floor deployment where network connectivity may be limited:

1. **Model Compression**
   - Quantization: Reduce model precision
   - Pruning: Remove unnecessary parameters
   - Distillation: Train smaller model to mimic larger one

2. **Efficient Inference**
   ```python
   # Use ONNX for optimized inference
   import onnxruntime as ort
   
   session = ort.InferenceSession("model.onnx")
   prediction = session.run(None, {"input": text_features})
   ```

### Integration with Manufacturing Systems

1. **MES Integration**
   - Connect with Manufacturing Execution Systems
   - Trigger automated responses based on text analysis
   - Log predictions for quality tracking

2. **Alert Systems**
   ```python
   def trigger_alert(prediction, confidence, text):
       if prediction == "High" and confidence > 0.8:
           # Send immediate notification to maintenance team
           send_priority_alert(text, prediction)
       elif prediction == "Medium":
           # Add to maintenance queue
           add_to_queue(text, priority="medium")
   ```

This foundation provides the theoretical and practical knowledge needed to implement NLP solutions in manufacturing environments, balancing accuracy, efficiency, and safety requirements specific to semiconductor fabrication facilities.