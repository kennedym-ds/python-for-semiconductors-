{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9742253e",
      "metadata": {},
      "source": [
        "# 9.3 Real-time Inference & Model Serving Interactive Notebook\n",
        "\n",
        "This notebook provides hands-on implementation of real-time ML inference systems for semiconductor manufacturing. Learn to build production-ready serving infrastructure with low latency, high throughput, and reliability.\n",
        "\n",
        "## Outline:\n",
        "1. Import Required Libraries\n",
        "2. Understanding Real-time vs Batch Inference\n",
        "3. Build Simple FastAPI Model Server\n",
        "4. Implement Request Caching with TTL\n",
        "5. Dynamic Batching for Throughput\n",
        "6. Latency Monitoring (p50, p95, p99)\n",
        "7. Async Request Processing\n",
        "8. Model Versioning & A/B Testing\n",
        "9. Health Checks & Readiness Probes\n",
        "10. Load Testing & Benchmarking\n",
        "11. Production Deployment Considerations\n",
        "12. Summary & Best Practices\n",
        "\n",
        "> **Semiconductor Context**: Inline inspection systems require real-time defect detection (<50ms) to keep up with production speed. This notebook shows how to build serving infrastructure meeting strict latency and throughput requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e11aaa3",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "Import libraries for model serving, API development, caching, and monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5018f60a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import time\n",
        "import asyncio\n",
        "import json\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta\n",
        "from collections import deque\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "%matplotlib inline\n",
        "\n",
        "# FastAPI (will be used in later sections)\n",
        "# Note: Install with: pip install fastapi uvicorn\n",
        "try:\n",
        "    from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "    from pydantic import BaseModel\n",
        "    HAS_FASTAPI = True\n",
        "except ImportError:\n",
        "    print(\"\u26a0\ufe0f FastAPI not installed. Install with: pip install fastapi uvicorn\")\n",
        "    HAS_FASTAPI = False\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print('\u2713 Libraries imported successfully')\n",
        "print(f'Random seed: {RANDOM_SEED}')\n",
        "print(f'FastAPI available: {HAS_FASTAPI}')\n",
        "\n",
        "# Helper function\n",
        "def section(title: str):\n",
        "    print(f\"\\n{'='*len(title)}\\n{title}\\n{'='*len(title)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe30d74d",
      "metadata": {},
      "source": [
        "## 2. Understanding Real-time vs Batch Inference\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect | Batch Inference | Real-time Inference |\n",
        "|--------|----------------|---------------------|\n",
        "| **Latency** | Minutes to hours | Milliseconds to seconds |\n",
        "| **Throughput** | High (thousands/sec) | Variable (depends on request rate) |\n",
        "| **Use Case** | Offline analysis, reports | Online decisions, interactive |\n",
        "| **Complexity** | Simple (just loop) | High (servers, caching, monitoring) |\n",
        "| **Cost** | Lower (can batch) | Higher (always running) |\n",
        "\n",
        "### Semiconductor Manufacturing Applications:\n",
        "\n",
        "**Real-time (Inline Inspection):**\n",
        "- Defect detection during production (<50ms)\n",
        "- Process control adjustments (immediate feedback)\n",
        "- Equipment health monitoring (continuous)\n",
        "- Yield prediction for WIP wafers (on-demand)\n",
        "\n",
        "**Batch Processing (Offline Analysis):**\n",
        "- Daily yield reports\n",
        "- Historical trend analysis\n",
        "- Model retraining on accumulated data\n",
        "- Root cause analysis (can wait hours)\n",
        "\n",
        "### Performance Requirements:\n",
        "- **Latency targets**: p99 < 50ms for inline, < 200ms for interactive\n",
        "- **Throughput targets**: 100-1000 req/sec for fab-wide deployment\n",
        "- **Availability**: 99.9% uptime (production can't stop)\n",
        "- **SLA tracking**: Monitor p50, p95, p99 latency percentiles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6aff2fb",
      "metadata": {},
      "source": [
        "## 3. Generate Sample Model & Data\n",
        "\n",
        "Create a simple defect detection model for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581ff91d",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Generate Sample Defect Detection Model')\n",
        "\n",
        "def generate_synthetic_wafer_data(n_samples: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generate synthetic wafer process data for defect prediction.\n",
        "    \n",
        "    Features: temperature, pressure, gas_flow, particle_count, humidity\n",
        "    Target: defect (0=pass, 1=fail)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(RANDOM_SEED)\n",
        "    \n",
        "    # Generate features\n",
        "    X = np.column_stack([\n",
        "        rng.normal(25, 5, n_samples),    # temperature (C)\n",
        "        rng.normal(1.0, 0.2, n_samples), # pressure (atm)\n",
        "        rng.normal(100, 20, n_samples),  # gas_flow (sccm)\n",
        "        rng.normal(50, 15, n_samples),   # particle_count\n",
        "        rng.normal(45, 10, n_samples)    # humidity (%)\n",
        "    ])\n",
        "    \n",
        "    # Generate labels with realistic decision boundary\n",
        "    defect_score = (\n",
        "        0.5 * (X[:, 0] - 25) / 5 +  # Temperature deviation\n",
        "        0.3 * (X[:, 3] - 50) / 15 +  # Particle count\n",
        "        -0.2 * (X[:, 4] - 45) / 10   # Low humidity increases defects\n",
        "    )\n",
        "    y = (defect_score + rng.normal(0, 0.3, n_samples) > 0.5).astype(int)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Generate and split data\n",
        "X, y = generate_synthetic_wafer_data(n_samples=1000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Defect rate: {y.mean()*100:.1f}%\")\n",
        "\n",
        "# Train simple model\n",
        "print(\"\\n\ud83d\udd04 Training Random Forest model...\")\n",
        "model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=RANDOM_SEED)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_acc = model.score(X_train_scaled, y_train)\n",
        "test_acc = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"\u2713 Training complete\")\n",
        "print(f\"Train accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test accuracy:  {test_acc:.3f}\")\n",
        "\n",
        "# Save model (for serving)\n",
        "model_path = Path('defect_model.joblib')\n",
        "scaler_path = Path('defect_scaler.joblib')\n",
        "joblib.dump(model, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"\\n\u2713 Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a719e5",
      "metadata": {},
      "source": [
        "## 4. Simple Model Server with Latency Tracking\n",
        "\n",
        "Build a basic model serving class that tracks inference latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc26b6fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Simple Model Server with Latency Tracking')\n",
        "\n",
        "@dataclass\n",
        "class LatencyMetrics:\n",
        "    \"\"\"Track latency metrics for monitoring.\"\"\"\n",
        "    latencies: deque = field(default_factory=lambda: deque(maxlen=1000))\n",
        "    \n",
        "    def record(self, latency_ms: float):\n",
        "        \"\"\"Record a latency measurement.\"\"\"\n",
        "        self.latencies.append(latency_ms)\n",
        "    \n",
        "    def get_percentiles(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate latency percentiles.\"\"\"\n",
        "        if not self.latencies:\n",
        "            return {'p50': 0, 'p95': 0, 'p99': 0, 'mean': 0}\n",
        "        \n",
        "        latencies_array = np.array(self.latencies)\n",
        "        return {\n",
        "            'p50': np.percentile(latencies_array, 50),\n",
        "            'p95': np.percentile(latencies_array, 95),\n",
        "            'p99': np.percentile(latencies_array, 99),\n",
        "            'mean': np.mean(latencies_array),\n",
        "            'count': len(self.latencies)\n",
        "        }\n",
        "\n",
        "class ModelServer:\n",
        "    \"\"\"Simple model serving class with latency tracking.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: Path, scaler_path: Path):\n",
        "        self.model = joblib.load(model_path)\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        self.metrics = LatencyMetrics()\n",
        "        print(f\"\u2713 Model loaded from {model_path}\")\n",
        "    \n",
        "    def predict(self, features: np.ndarray) -> Dict:\n",
        "        \"\"\"Make prediction with latency tracking.\"\"\"\n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        # Preprocess\n",
        "        features_scaled = self.scaler.transform(features.reshape(1, -1))\n",
        "        \n",
        "        # Predict\n",
        "        prediction = self.model.predict(features_scaled)[0]\n",
        "        probability = self.model.predict_proba(features_scaled)[0]\n",
        "        \n",
        "        # Calculate latency\n",
        "        latency_ms = (time.perf_counter() - start_time) * 1000\n",
        "        self.metrics.record(latency_ms)\n",
        "        \n",
        "        return {\n",
        "            'prediction': int(prediction),\n",
        "            'probability': float(probability[1]),\n",
        "            'latency_ms': latency_ms\n",
        "        }\n",
        "    \n",
        "    def get_metrics(self) -> Dict:\n",
        "        \"\"\"Get current latency metrics.\"\"\"\n",
        "        return self.metrics.get_percentiles()\n",
        "\n",
        "# Test server\n",
        "server = ModelServer(model_path, scaler_path)\n",
        "\n",
        "print(\"\\n\ud83e\uddea Testing inference...\\n\")\n",
        "for i in range(5):\n",
        "    test_features = X_test[i]\n",
        "    result = server.predict(test_features)\n",
        "    print(f\"Sample {i+1}: prediction={result['prediction']}, \"\n",
        "          f\"probability={result['probability']:.3f}, \"\n",
        "          f\"latency={result['latency_ms']:.2f}ms\")\n",
        "\n",
        "# Run benchmark\n",
        "print(\"\\n\ud83c\udfc3 Running latency benchmark (100 requests)...\")\n",
        "for i in range(100):\n",
        "    idx = np.random.randint(0, len(X_test))\n",
        "    server.predict(X_test[idx])\n",
        "\n",
        "metrics = server.get_metrics()\n",
        "print(\"\\n\ud83d\udcca Latency Metrics:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Mean:  {metrics['mean']:.2f} ms\")\n",
        "print(f\"p50:   {metrics['p50']:.2f} ms\")\n",
        "print(f\"p95:   {metrics['p95']:.2f} ms\")\n",
        "print(f\"p99:   {metrics['p99']:.2f} ms\")\n",
        "print(f\"Count: {metrics['count']} requests\")\n",
        "\n",
        "# Visualize latency distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(server.metrics.latencies, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(metrics['p50'], color='green', linestyle='--', linewidth=2, label=f\"p50: {metrics['p50']:.1f}ms\")\n",
        "plt.axvline(metrics['p95'], color='orange', linestyle='--', linewidth=2, label=f\"p95: {metrics['p95']:.1f}ms\")\n",
        "plt.axvline(metrics['p99'], color='red', linestyle='--', linewidth=2, label=f\"p99: {metrics['p99']:.1f}ms\")\n",
        "plt.xlabel('Latency (ms)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Inference Latency Distribution')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(list(server.metrics.latencies), color='navy', alpha=0.6)\n",
        "plt.axhline(metrics['p95'], color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
        "plt.xlabel('Request Number')\n",
        "plt.ylabel('Latency (ms)')\n",
        "plt.title('Latency Over Time (with p95 line)')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Interpretation:\")\n",
        "print(\"- p50 (median): Typical latency for most requests\")\n",
        "print(\"- p95: 95% of requests faster than this\")\n",
        "print(\"- p99: 99% of requests faster than this (SLA target)\")\n",
        "print(\"- Use p99 for SLAs (protects against tail latency)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dec67d7",
      "metadata": {},
      "source": [
        "## 5. Request Caching with TTL\n",
        "\n",
        "Implement caching to avoid redundant inference for identical requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252cf445",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Request Caching with TTL')\n",
        "\n",
        "@dataclass\n",
        "class CacheEntry:\n",
        "    \"\"\"Cache entry with timestamp for TTL.\"\"\"\n",
        "    result: Dict\n",
        "    timestamp: datetime\n",
        "\n",
        "class CachedModelServer(ModelServer):\n",
        "    \"\"\"Model server with request caching.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: Path, scaler_path: Path, cache_ttl_seconds: int = 60):\n",
        "        super().__init__(model_path, scaler_path)\n",
        "        self.cache: Dict[str, CacheEntry] = {}\n",
        "        self.cache_ttl = timedelta(seconds=cache_ttl_seconds)\n",
        "        self.cache_hits = 0\n",
        "        self.cache_misses = 0\n",
        "        print(f\"\u2713 Caching enabled (TTL: {cache_ttl_seconds}s)\")\n",
        "    \n",
        "    def _make_cache_key(self, features: np.ndarray) -> str:\n",
        "        \"\"\"Create cache key from features.\"\"\"\n",
        "        # Round to reduce cache misses from floating point precision\n",
        "        rounded = np.round(features, decimals=2)\n",
        "        return str(rounded.tobytes())\n",
        "    \n",
        "    def _is_cache_valid(self, entry: CacheEntry) -> bool:\n",
        "        \"\"\"Check if cache entry is still valid.\"\"\"\n",
        "        return datetime.now() - entry.timestamp < self.cache_ttl\n",
        "    \n",
        "    def predict(self, features: np.ndarray, use_cache: bool = True) -> Dict:\n",
        "        \"\"\"Make prediction with caching.\"\"\"\n",
        "        if use_cache:\n",
        "            cache_key = self._make_cache_key(features)\n",
        "            \n",
        "            # Check cache\n",
        "            if cache_key in self.cache:\n",
        "                entry = self.cache[cache_key]\n",
        "                if self._is_cache_valid(entry):\n",
        "                    self.cache_hits += 1\n",
        "                    result = entry.result.copy()\n",
        "                    result['cache_hit'] = True\n",
        "                    result['latency_ms'] = 0.1  # Cache lookup time\n",
        "                    return result\n",
        "                else:\n",
        "                    # Cache entry expired\n",
        "                    del self.cache[cache_key]\n",
        "        \n",
        "        # Cache miss - compute prediction\n",
        "        self.cache_misses += 1\n",
        "        result = super().predict(features)\n",
        "        result['cache_hit'] = False\n",
        "        \n",
        "        # Store in cache\n",
        "        if use_cache:\n",
        "            cache_key = self._make_cache_key(features)\n",
        "            self.cache[cache_key] = CacheEntry(result, datetime.now())\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def get_cache_stats(self) -> Dict:\n",
        "        \"\"\"Get cache statistics.\"\"\"\n",
        "        total_requests = self.cache_hits + self.cache_misses\n",
        "        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'cache_hits': self.cache_hits,\n",
        "            'cache_misses': self.cache_misses,\n",
        "            'hit_rate': hit_rate,\n",
        "            'cache_size': len(self.cache)\n",
        "        }\n",
        "\n",
        "# Test caching\n",
        "cached_server = CachedModelServer(model_path, scaler_path, cache_ttl_seconds=30)\n",
        "\n",
        "print(\"\\n\ud83e\uddea Testing cache performance...\\n\")\n",
        "\n",
        "# First request (cache miss)\n",
        "test_sample = X_test[0]\n",
        "result1 = cached_server.predict(test_sample)\n",
        "print(f\"Request 1: latency={result1['latency_ms']:.2f}ms, cache_hit={result1['cache_hit']}\")\n",
        "\n",
        "# Second request with same features (cache hit)\n",
        "result2 = cached_server.predict(test_sample)\n",
        "print(f\"Request 2: latency={result2['latency_ms']:.2f}ms, cache_hit={result2['cache_hit']}\")\n",
        "\n",
        "# Simulate workload with repeated requests\n",
        "print(\"\\n\ud83c\udfc3 Simulating workload (50% repeated requests)...\")\n",
        "unique_samples = X_test[:20]\n",
        "\n",
        "for _ in range(200):\n",
        "    # 50% chance of using a repeated sample\n",
        "    if np.random.random() < 0.5:\n",
        "        idx = np.random.randint(0, len(unique_samples))\n",
        "        cached_server.predict(unique_samples[idx])\n",
        "    else:\n",
        "        idx = np.random.randint(0, len(X_test))\n",
        "        cached_server.predict(X_test[idx])\n",
        "\n",
        "# Show cache statistics\n",
        "cache_stats = cached_server.get_cache_stats()\n",
        "print(\"\\n\ud83d\udcca Cache Statistics:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Cache hits:    {cache_stats['cache_hits']:>6}\")\n",
        "print(f\"Cache misses:  {cache_stats['cache_misses']:>6}\")\n",
        "print(f\"Hit rate:      {cache_stats['hit_rate']:>6.1%}\")\n",
        "print(f\"Cache size:    {cache_stats['cache_size']:>6} entries\")\n",
        "\n",
        "# Calculate speedup from caching\n",
        "avg_inference_time = 0.5  # Assume 0.5ms average\n",
        "avg_cache_time = 0.1  # Cache lookup time\n",
        "time_saved = cache_stats['cache_hits'] * (avg_inference_time - avg_cache_time)\n",
        "total_time = cache_stats['cache_hits'] * avg_cache_time + cache_stats['cache_misses'] * avg_inference_time\n",
        "speedup = ((cache_stats['cache_hits'] + cache_stats['cache_misses']) * avg_inference_time) / total_time\n",
        "\n",
        "print(f\"\\nEstimated speedup: {speedup:.2f}x\")\n",
        "print(f\"Time saved: ~{time_saved:.1f}ms total\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Caching Benefits:\")\n",
        "print(\"- Reduces latency for repeated requests\")\n",
        "print(\"- Lowers compute load on model\")\n",
        "print(\"- TTL prevents stale predictions\")\n",
        "print(\"- Most effective when requests have patterns (e.g., periodic monitoring)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c447b714",
      "metadata": {},
      "source": [
        "## 6. Dynamic Batching for Throughput\n",
        "\n",
        "Batch multiple requests together to improve GPU utilization and throughput.\n",
        "\n",
        "**Key concepts:**\n",
        "- Collect requests up to max batch size or timeout\n",
        "- Process batch in single inference call\n",
        "- Return individual results to each requester\n",
        "- Trade-off: Small latency increase for much higher throughput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af114f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Dynamic Batching')\n",
        "\n",
        "class BatchedModelServer:\n",
        "    \"\"\"Model server with dynamic batching.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: Path, scaler_path: Path, \n",
        "                 max_batch_size: int = 32, max_wait_ms: float = 10):\n",
        "        self.model = joblib.load(model_path)\n",
        "        self.scaler = joblib.load(scaler_path)\n",
        "        self.max_batch_size = max_batch_size\n",
        "        self.max_wait_ms = max_wait_ms\n",
        "        self.batch_sizes = []\n",
        "        print(f\"\u2713 Batching enabled (max_size={max_batch_size}, max_wait={max_wait_ms}ms)\")\n",
        "    \n",
        "    def predict_batch(self, features_list: List[np.ndarray]) -> List[Dict]:\n",
        "        \"\"\"Process a batch of requests.\"\"\"\n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        # Stack features into batch\n",
        "        features_batch = np.vstack(features_list)\n",
        "        features_scaled = self.scaler.transform(features_batch)\n",
        "        \n",
        "        # Batch inference\n",
        "        predictions = self.model.predict(features_scaled)\n",
        "        probabilities = self.model.predict_proba(features_scaled)\n",
        "        \n",
        "        batch_latency_ms = (time.perf_counter() - start_time) * 1000\n",
        "        per_sample_latency = batch_latency_ms / len(features_list)\n",
        "        \n",
        "        # Record batch size for statistics\n",
        "        self.batch_sizes.append(len(features_list))\n",
        "        \n",
        "        # Format results\n",
        "        results = []\n",
        "        for pred, prob in zip(predictions, probabilities):\n",
        "            results.append({\n",
        "                'prediction': int(pred),\n",
        "                'probability': float(prob[1]),\n",
        "                'batch_size': len(features_list),\n",
        "                'batch_latency_ms': batch_latency_ms,\n",
        "                'per_sample_latency_ms': per_sample_latency\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def get_batch_stats(self) -> Dict:\n",
        "        \"\"\"Get batching statistics.\"\"\"\n",
        "        if not self.batch_sizes:\n",
        "            return {'mean_batch_size': 0, 'total_batches': 0}\n",
        "        \n",
        "        return {\n",
        "            'mean_batch_size': np.mean(self.batch_sizes),\n",
        "            'max_batch_size': np.max(self.batch_sizes),\n",
        "            'total_batches': len(self.batch_sizes),\n",
        "            'total_samples': sum(self.batch_sizes)\n",
        "        }\n",
        "\n",
        "# Test batching\n",
        "batch_server = BatchedModelServer(model_path, scaler_path, max_batch_size=32)\n",
        "\n",
        "print(\"\\n\ud83e\uddea Testing batch inference...\\n\")\n",
        "\n",
        "# Single sample (batch size 1)\n",
        "results = batch_server.predict_batch([X_test[0]])\n",
        "print(f\"Batch size 1: latency={results[0]['batch_latency_ms']:.2f}ms\")\n",
        "\n",
        "# Small batch\n",
        "results = batch_server.predict_batch(X_test[:8].tolist())\n",
        "print(f\"Batch size 8: batch_latency={results[0]['batch_latency_ms']:.2f}ms, \"\n",
        "      f\"per_sample={results[0]['per_sample_latency_ms']:.2f}ms\")\n",
        "\n",
        "# Large batch\n",
        "results = batch_server.predict_batch(X_test[:32].tolist())\n",
        "print(f\"Batch size 32: batch_latency={results[0]['batch_latency_ms']:.2f}ms, \"\n",
        "      f\"per_sample={results[0]['per_sample_latency_ms']:.2f}ms\")\n",
        "\n",
        "# Benchmark different batch sizes\n",
        "print(\"\\n\ud83c\udfc3 Benchmarking batch sizes...\")\n",
        "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
        "latencies = []\n",
        "throughputs = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    batch = X_test[:bs].tolist()\n",
        "    \n",
        "    # Warmup\n",
        "    batch_server.predict_batch(batch)\n",
        "    \n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(10):\n",
        "        start = time.perf_counter()\n",
        "        batch_server.predict_batch(batch)\n",
        "        times.append((time.perf_counter() - start) * 1000)\n",
        "    \n",
        "    avg_latency = np.mean(times)\n",
        "    throughput = (bs / avg_latency) * 1000  # samples/sec\n",
        "    \n",
        "    latencies.append(avg_latency)\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "# Visualize batch performance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Latency vs batch size\n",
        "axes[0].plot(batch_sizes, latencies, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
        "axes[0].set_xlabel('Batch Size')\n",
        "axes[0].set_ylabel('Batch Latency (ms)')\n",
        "axes[0].set_title('Batch Latency vs Batch Size')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Throughput vs batch size\n",
        "axes[1].plot(batch_sizes, throughputs, marker='s', linewidth=2, markersize=8, color='coral')\n",
        "axes[1].set_xlabel('Batch Size')\n",
        "axes[1].set_ylabel('Throughput (samples/sec)')\n",
        "axes[1].set_title('Throughput vs Batch Size')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Batch Performance Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Batch Size':>12} {'Latency (ms)':>15} {'Throughput (samples/s)':>25}\")\n",
        "print(\"=\"*60)\n",
        "for bs, lat, tput in zip(batch_sizes, latencies, throughputs):\n",
        "    print(f\"{bs:>12} {lat:>15.2f} {tput:>25.1f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Batching Benefits:\")\n",
        "print(\"- Dramatically increases throughput (10-30x typical)\")\n",
        "print(\"- Better GPU/CPU utilization\")\n",
        "print(\"- Small per-request latency increase acceptable for high-load scenarios\")\n",
        "print(\"- Essential for serving at scale (1000s requests/sec)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68beb9dc",
      "metadata": {},
      "source": [
        "## 7. Summary & Best Practices\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "**Latency Optimization:**\n",
        "- Track p50, p95, p99 percentiles (not just mean)\n",
        "- Use p99 for SLA targets (protects tail latency)\n",
        "- Optimize model (quantization, pruning) for edge deployment\n",
        "- Pre-load models to avoid cold start\n",
        "\n",
        "**Throughput Optimization:**\n",
        "- Use dynamic batching for 10-30x throughput gain\n",
        "- Adjust batch size vs latency trade-off per use case\n",
        "- Leverage GPU acceleration when available\n",
        "- Consider async processing for I/O-bound operations\n",
        "\n",
        "**Caching Strategy:**\n",
        "- Implement TTL caching for repeated requests\n",
        "- Monitor cache hit rates (>30% is good)\n",
        "- Use content-based cache keys\n",
        "- Set TTL based on model update frequency\n",
        "\n",
        "**Production Deployment:**\n",
        "- Implement health checks and readiness probes\n",
        "- Version models for A/B testing\n",
        "- Monitor latency, throughput, error rates\n",
        "- Use load balancers for horizontal scaling\n",
        "- Plan for graceful degradation\n",
        "\n",
        "### Semiconductor Manufacturing Recommendations:\n",
        "\n",
        "1. **Inline Inspection (<50ms)**: Optimize for latency with edge deployment\n",
        "2. **Interactive Analysis (<200ms)**: Use caching + batching\n",
        "3. **High Throughput (1000s/sec)**: Dynamic batching + horizontal scaling\n",
        "4. **Mission Critical**: Implement redundancy, monitoring, auto-scaling\n",
        "\n",
        "### Next Steps:\n",
        "- Explore FastAPI implementation in 9.3-realtime-quick-ref.md\n",
        "- Review assessment questions in assessments/module-9/9.3-questions.json\n",
        "- Check module-9 fundamentals for deep theory\n",
        "- Try production deployment with Docker + Kubernetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe73441",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Notebook Complete!')\n",
        "\n",
        "print(\"\u2705 You have completed the Real-time Inference & Model Serving tutorial!\")\n",
        "print(\"\\nKey Skills Acquired:\")\n",
        "print(\"  \u2022 Understanding real-time vs batch inference trade-offs\")\n",
        "print(\"  \u2022 Building model servers with latency tracking\")\n",
        "print(\"  \u2022 Implementing request caching with TTL\")\n",
        "print(\"  \u2022 Dynamic batching for throughput optimization\")\n",
        "print(\"  \u2022 Monitoring p50/p95/p99 latency percentiles\")\n",
        "print(\"  \u2022 Production deployment considerations\")\n",
        "print(\"\\n\ud83d\udcda Next Steps:\")\n",
        "print(\"  \u2022 Explore 9.3-realtime-inference-quick-ref.md for FastAPI examples\")\n",
        "print(\"  \u2022 Review assessment questions in assessments/module-9/9.3-questions.json\")\n",
        "print(\"  \u2022 Check module-9 fundamentals for serving architecture patterns\")\n",
        "print(\"  \u2022 Deploy to production with Docker, Kubernetes, cloud platforms\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
