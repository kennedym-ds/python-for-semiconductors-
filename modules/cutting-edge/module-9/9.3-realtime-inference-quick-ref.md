# 9.3 Real-time Inference & Model Serving - Quick Reference

## Overview

Real-time model serving enables low-latency ML predictions for production applications. Critical for semiconductor inline inspection (<50ms latency) and interactive analysis.

**Key Concepts:**
- Latency: Time per prediction (measure p50, p95, p99)
- Throughput: Predictions per second
- Caching: Avoid redundant inference
- Batching: Process multiple requests together
- Monitoring: Track performance metrics

---

## FastAPI Model Server

### Basic Server

```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

app = FastAPI()
model = joblib.load('model.joblib')
scaler = joblib.load('scaler.joblib')

class PredictionRequest(BaseModel):
    features: list[float]

class PredictionResponse(BaseModel):
    prediction: int
    probability: float
    latency_ms: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    import time
    start = time.perf_counter()

    # Preprocess
    features = np.array(request.features).reshape(1, -1)
    features_scaled = scaler.transform(features)

    # Predict
    pred = model.predict(features_scaled)[0]
    prob = model.predict_proba(features_scaled)[0]

    latency_ms = (time.perf_counter() - start) * 1000

    return PredictionResponse(
        prediction=int(pred),
        probability=float(prob[1]),
        latency_ms=latency_ms
    )

@app.get("/health")
async def health():
    return {"status": "healthy"}
```

**Run server:**
```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

**Test endpoint:**
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"features": [25.0, 1.0, 100.0, 50.0, 45.0]}'
```

---

## Latency Monitoring

### Track Percentiles

```python
from collections import deque
import numpy as np

class LatencyTracker:
    def __init__(self, max_size=1000):
        self.latencies = deque(maxlen=max_size)

    def record(self, latency_ms: float):
        self.latencies.append(latency_ms)

    def get_percentiles(self):
        if not self.latencies:
            return {}
        arr = np.array(self.latencies)
        return {
            'p50': np.percentile(arr, 50),
            'p95': np.percentile(arr, 95),
            'p99': np.percentile(arr, 99),
            'mean': np.mean(arr)
        }

tracker = LatencyTracker()

# In prediction endpoint
@app.post("/predict")
async def predict(request: PredictionRequest):
    start = time.perf_counter()
    # ... prediction logic ...
    latency_ms = (time.perf_counter() - start) * 1000
    tracker.record(latency_ms)
    return result

@app.get("/metrics")
async def metrics():
    return tracker.get_percentiles()
```

**Why percentiles?**
- p50 (median): Typical latency
- p95: 95% of requests faster
- p99: SLA target (protects tail latency)

---

## Caching with TTL

### In-Memory Cache

```python
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import Dict, Optional

@dataclass
class CacheEntry:
    result: Dict
    timestamp: datetime

class CachedServer:
    def __init__(self, ttl_seconds: int = 60):
        self.cache: Dict[str, CacheEntry] = {}
        self.ttl = timedelta(seconds=ttl_seconds)

    def _make_key(self, features: list) -> str:
        # Round to reduce floating point issues
        rounded = [round(f, 2) for f in features]
        return str(rounded)

    def _is_valid(self, entry: CacheEntry) -> bool:
        return datetime.now() - entry.timestamp < self.ttl

    def get(self, features: list) -> Optional[Dict]:
        key = self._make_key(features)
        if key in self.cache:
            entry = self.cache[key]
            if self._is_valid(entry):
                return entry.result
            del self.cache[key]
        return None

    def set(self, features: list, result: Dict):
        key = self._make_key(features)
        self.cache[key] = CacheEntry(result, datetime.now())

cache = CachedServer(ttl_seconds=30)

@app.post("/predict")
async def predict(request: PredictionRequest):
    # Check cache
    cached = cache.get(request.features)
    if cached:
        return cached

    # Compute prediction
    result = compute_prediction(request.features)

    # Store in cache
    cache.set(request.features, result)
    return result
```

**When to use:**
- Repeated requests for same inputs
- Results don't change frequently
- Cost of inference > cost of cache lookup

---

## Dynamic Batching

### Batch Multiple Requests

```python
import asyncio
from typing import List

class BatchProcessor:
    def __init__(self, max_batch_size=32, max_wait_ms=10):
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms
        self.pending: List = []
        self.lock = asyncio.Lock()

    async def add_request(self, features: np.ndarray):
        async with self.lock:
            self.pending.append(features)

            # Process if batch full or waited long enough
            if len(self.pending) >= self.max_batch_size:
                return await self._process_batch()

        # Wait for more requests or timeout
        await asyncio.sleep(self.max_wait_ms / 1000)
        async with self.lock:
            return await self._process_batch()

    async def _process_batch(self):
        if not self.pending:
            return None

        # Batch inference
        batch = np.vstack(self.pending)
        predictions = model.predict(batch)

        # Clear pending
        self.pending = []
        return predictions

processor = BatchProcessor()

@app.post("/predict")
async def predict(request: PredictionRequest):
    features = np.array(request.features)
    prediction = await processor.add_request(features)
    return {"prediction": int(prediction)}
```

**Benefits:**
- 10-30x throughput improvement
- Better GPU/CPU utilization
- Small latency trade-off

**Trade-offs:**
- Increased per-request latency (wait time)
- More complex implementation
- Better for high-load scenarios

---

## Model Versioning & A/B Testing

### Multiple Model Versions

```python
from typing import Dict

class ModelRegistry:
    def __init__(self):
        self.models: Dict[str, any] = {}
        self.default_version = None

    def register(self, version: str, model_path: str):
        self.models[version] = joblib.load(model_path)
        if self.default_version is None:
            self.default_version = version

    def get(self, version: Optional[str] = None):
        version = version or self.default_version
        return self.models.get(version)

registry = ModelRegistry()
registry.register('v1.0', 'model_v1.joblib')
registry.register('v1.1', 'model_v1.1.joblib')

@app.post("/predict")
async def predict(
    request: PredictionRequest,
    model_version: Optional[str] = None
):
    model = registry.get(model_version)
    if model is None:
        raise HTTPException(404, "Model version not found")

    # Use specific model version
    prediction = model.predict(...)
    return {"prediction": prediction, "version": model_version}
```

### A/B Testing

```python
import random

class ABTester:
    def __init__(self, version_a: str, version_b: str, traffic_split: float = 0.5):
        self.version_a = version_a
        self.version_b = version_b
        self.traffic_split = traffic_split

    def get_version(self) -> str:
        return self.version_a if random.random() < self.traffic_split else self.version_b

ab_test = ABTester('v1.0', 'v1.1', traffic_split=0.9)  # 90% v1.0, 10% v1.1

@app.post("/predict")
async def predict(request: PredictionRequest):
    version = ab_test.get_version()
    model = registry.get(version)
    # ... use model ...
    return {"prediction": result, "version": version}
```

---

## Health Checks & Monitoring

### Readiness & Liveness Probes

```python
from datetime import datetime

class HealthChecker:
    def __init__(self):
        self.startup_time = datetime.now()
        self.last_prediction = None
        self.error_count = 0

    def is_ready(self) -> bool:
        """Check if service is ready to accept traffic."""
        # Model loaded, recent successful prediction
        return (
            model is not None and
            (self.last_prediction is None or
             (datetime.now() - self.last_prediction).seconds < 300)
        )

    def is_alive(self) -> bool:
        """Check if service is running."""
        return self.error_count < 10  # Threshold

health = HealthChecker()

@app.get("/health/live")
async def liveness():
    if health.is_alive():
        return {"status": "alive"}
    raise HTTPException(503, "Service unhealthy")

@app.get("/health/ready")
async def readiness():
    if health.is_ready():
        return {"status": "ready"}
    raise HTTPException(503, "Service not ready")

@app.post("/predict")
async def predict(request: PredictionRequest):
    try:
        result = compute_prediction(request.features)
        health.last_prediction = datetime.now()
        return result
    except Exception as e:
        health.error_count += 1
        raise
```

---

## Load Balancing & Scaling

### Nginx Load Balancer

**nginx.conf:**
```nginx
upstream model_servers {
    least_conn;  # Route to server with fewest connections
    server 127.0.0.1:8001;
    server 127.0.0.1:8002;
    server 127.0.0.1:8003;
}

server {
    listen 80;

    location /predict {
        proxy_pass http://model_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### Horizontal Scaling

**Run multiple instances:**
```bash
# Start 3 model servers on different ports
uvicorn main:app --port 8001 --workers 4 &
uvicorn main:app --port 8002 --workers 4 &
uvicorn main:app --port 8003 --workers 4 &

# Start nginx load balancer
nginx -c nginx.conf
```

---

## Docker Deployment

### Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy model files
COPY model.joblib scaler.joblib ./
COPY main.py .

# Expose port
EXPOSE 8000

# Run server
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Build and run:**
```bash
docker build -t model-server .
docker run -p 8000:8000 model-server
```

### Docker Compose (Multi-service)

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  model-server-1:
    build: .
    ports:
      - "8001:8000"
    environment:
      - MODEL_VERSION=v1.0

  model-server-2:
    build: .
    ports:
      - "8002:8000"
    environment:
      - MODEL_VERSION=v1.0

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - model-server-1
      - model-server-2
```

**Run:**
```bash
docker-compose up -d
```

---

## Performance Benchmarking

### Load Testing with Locust

**locustfile.py:**
```python
from locust import HttpUser, task, between
import random

class ModelUser(HttpUser):
    wait_time = between(0.1, 0.5)  # Random wait between requests

    @task
    def predict(self):
        # Generate random features
        features = [random.uniform(0, 100) for _ in range(5)]

        self.client.post("/predict", json={
            "features": features
        })

    @task(2)  # Higher weight - check metrics more often
    def metrics(self):
        self.client.get("/metrics")
```

**Run load test:**
```bash
# Test with 100 users, spawn rate 10/sec
locust -f locustfile.py --host=http://localhost:8000 \
       --users 100 --spawn-rate 10 --run-time 60s
```

**Metrics to track:**
- Requests per second (RPS)
- Latency percentiles (p50, p95, p99)
- Error rate
- CPU/memory usage

---

## Best Practices Summary

### Latency Optimization
- ✅ Pre-load models (avoid cold start)
- ✅ Use model quantization (INT8)
- ✅ Optimize preprocessing pipeline
- ✅ Profile code to find bottlenecks
- ✅ Use async processing for I/O

### Throughput Optimization
- ✅ Enable dynamic batching
- ✅ Use GPU acceleration
- ✅ Horizontal scaling (multiple instances)
- ✅ Connection pooling
- ✅ Optimize batch size

### Reliability
- ✅ Implement health checks
- ✅ Graceful degradation
- ✅ Circuit breakers for dependencies
- ✅ Request timeouts
- ✅ Retry logic with exponential backoff

### Monitoring
- ✅ Track p50/p95/p99 latency
- ✅ Monitor throughput (RPS)
- ✅ Log errors and exceptions
- ✅ Alert on SLA violations
- ✅ Dashboard for real-time metrics

### Semiconductor-Specific
- ✅ **Inline inspection**: p99 < 50ms (edge deployment)
- ✅ **Interactive analysis**: p95 < 200ms (caching + batching)
- ✅ **Batch processing**: Optimize for throughput
- ✅ **High availability**: 99.9%+ uptime
- ✅ **Data privacy**: On-premise deployment

---

## Troubleshooting Guide

| Issue | Symptoms | Solutions |
|-------|----------|-----------|
| **High latency** | p99 > target | Profile code, optimize model, enable caching |
| **Low throughput** | RPS below target | Enable batching, horizontal scaling, GPU |
| **Memory leaks** | Memory grows over time | Fix object retention, restart periodically |
| **Cold starts** | First request slow | Pre-load models, warmup requests |
| **Inconsistent latency** | High p99 vs p50 | Check GC pauses, CPU throttling, network |

---

## Resources

**Documentation:**
- FastAPI: https://fastapi.tiangolo.com/
- Uvicorn: https://www.uvicorn.org/
- Locust: https://locust.io/

**Related Content:**
- 9.3-realtime-inference-analysis.ipynb - Interactive tutorial
- assessments/module-9/9.3-questions.json - Practice questions
- Module 9 fundamentals - Deep theory

**Production Deployment:**
- Docker: https://docs.docker.com/
- Kubernetes: https://kubernetes.io/docs/
- Nginx: https://nginx.org/en/docs/
