{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 9.2 \u2013 Monitoring & Maintenance Interactive Demo\n",
        "\n",
        "This notebook demonstrates model monitoring and maintenance techniques for semiconductor manufacturing, including:\n",
        "\n",
        "- \ud83d\udd2c **MLflow experiment tracking** with parameters, metrics, and artifacts\n",
        "- \ud83d\udcca **Drift detection** using PSI, KS-test, and Wasserstein distance\n",
        "- \ud83d\udea8 **Alert systems** with manufacturing-specific thresholds\n",
        "- \ud83d\udcc8 **Performance monitoring** with PWS and Estimated Loss metrics\n",
        "- \ud83c\udfaf **Interactive visualizations** for drift analysis\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "1. How to set up comprehensive model monitoring\n",
        "2. Different types of drift and how to detect them\n",
        "3. Manufacturing-specific metrics and their interpretation\n",
        "4. How to create effective alert systems\n",
        "5. Best practices for maintaining models in production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Import our monitoring pipeline\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append(str(Path.cwd()))\n",
        "exec(open('9.2-monitoring-maintenance-pipeline.py').read())\n",
        "\n",
        "print(\"\u2705 Imports successful!\")\n",
        "print(f\"\ud83d\udccd Working directory: {Path.cwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. MLflow Setup and Basic Tracking\n",
        "\n",
        "Let's start by setting up MLflow for experiment tracking. MLflow helps us keep track of different model versions, their parameters, and performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up MLflow (if available)\n",
        "if HAS_MLFLOW:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    \n",
        "    # Set experiment\n",
        "    mlflow.set_experiment(\"semiconductor_monitoring_demo\")\n",
        "    print(\"\u2705 MLflow tracking enabled\")\n",
        "    print(f\"\ud83d\udcca Experiment: {mlflow.get_experiment_by_name('semiconductor_monitoring_demo').experiment_id}\")\nelse:\n",
        "    print(\"\u26a0\ufe0f  MLflow not available - using local tracking only\")\n",
        "\n",
        "# Create monitoring pipeline\n",
        "pipeline = MonitoringPipeline()\n",
        "if HAS_MLFLOW:\n",
        "    pipeline.enable_mlflow(\"semiconductor_monitoring_demo\")\n",
        "\n",
        "print(\"\ud83d\udd27 Monitoring pipeline created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Synthetic Semiconductor Data\n",
        "\n",
        "We'll create synthetic semiconductor process data that mimics real-world yield prediction scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate baseline training data\n",
        "np.random.seed(42)\n",
        "baseline_data = generate_yield_process(n=1000, seed=42)\n",
        "\n",
        "print(f\"\ud83d\udcca Generated dataset shape: {baseline_data.shape}\")\n",
        "print(f\"\ud83d\udccb Features: {list(baseline_data.columns)}\")\n",
        "print(f\"\ud83c\udfaf Target range: {baseline_data['target'].min():.2f} - {baseline_data['target'].max():.2f}\")\n",
        "\n",
        "# Display basic statistics\n",
        "baseline_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the baseline data\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Baseline Semiconductor Process Data', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot main process parameters\n",
        "process_params = ['temperature', 'pressure', 'flow', 'time', 'target']\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "for i, (param, color) in enumerate(zip(process_params, colors)):\n",
        "    ax = axes[i//3, i%3]\n",
        "    ax.hist(baseline_data[param], bins=30, alpha=0.7, color=color, edgecolor='black')\n",
        "    ax.set_title(f'{param.title()} Distribution')\n",
        "    ax.set_xlabel(param.title())\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Correlation plot\n",
        "ax = axes[1, 2]\n",
        "correlation_matrix = baseline_data[process_params].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)\n",
        "ax.set_title('Feature Correlations')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udcc8 Baseline data visualization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Baseline Model with Monitoring\n",
        "\n",
        "Let's train a baseline model and establish our monitoring framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data\n",
        "feature_cols = [col for col in baseline_data.columns if col != 'target']\n",
        "X_baseline = baseline_data[feature_cols]\n",
        "y_baseline = baseline_data['target'].values\n",
        "\n",
        "# Split for training and validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_baseline, y_baseline, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Training set: {X_train.shape}\")\n",
        "print(f\"\ud83d\udcca Validation set: {X_val.shape}\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\n\ud83d\udd04 Training baseline model...\")\n",
        "pipeline.fit(X_train, y_train, model_type=\"ridge\", alpha=1.0)\n",
        "\n",
        "# Evaluate baseline performance\n",
        "baseline_metrics = pipeline.evaluate(X_val, y_val)\n",
        "print(\"\\n\u2705 Baseline model trained successfully!\")\n",
        "print(f\"\ud83d\udcc8 Baseline MAE: {baseline_metrics['mae']:.3f}\")\n",
        "print(f\"\ud83d\udcc8 Baseline RMSE: {baseline_metrics['rmse']:.3f}\")\n",
        "print(f\"\ud83d\udcc8 Baseline R\u00b2: {baseline_metrics['r2']:.3f}\")\n",
        "print(f\"\ud83c\udfaf Prediction Within Spec: {baseline_metrics['pws_percent']:.1f}%\")\n",
        "print(f\"\ud83d\udcb0 Estimated Loss: ${baseline_metrics['estimated_loss']:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Model Performance\n",
        "\n",
        "Let's visualize how well our baseline model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions for visualization\n",
        "y_pred_val = pipeline.predict(X_val)\n",
        "\n",
        "# Create performance visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Baseline Model Performance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Predicted vs Actual\n",
        "ax1 = axes[0, 0]\n",
        "ax1.scatter(y_val, y_pred_val, alpha=0.6, color='blue')\n",
        "min_val, max_val = min(y_val.min(), y_pred_val.min()), max(y_val.max(), y_pred_val.max())\n",
        "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "ax1.set_xlabel('Actual Yield')\n",
        "ax1.set_ylabel('Predicted Yield')\n",
        "ax1.set_title('Predicted vs Actual Values')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Residuals plot\n",
        "ax2 = axes[0, 1]\n",
        "residuals = y_pred_val - y_val\n",
        "ax2.scatter(y_pred_val, residuals, alpha=0.6, color='green')\n",
        "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "ax2.set_xlabel('Predicted Values')\n",
        "ax2.set_ylabel('Residuals')\n",
        "ax2.set_title('Residuals Plot')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Error distribution\n",
        "ax3 = axes[1, 0]\n",
        "ax3.hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
        "ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "ax3.set_xlabel('Prediction Error')\n",
        "ax3.set_ylabel('Frequency')\n",
        "ax3.set_title('Error Distribution')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Performance metrics summary\n",
        "ax4 = axes[1, 1]\n",
        "metrics_names = ['MAE', 'RMSE', 'R\u00b2', 'PWS %', 'Est. Loss ($K)']\n",
        "metrics_values = [\n",
        "    baseline_metrics['mae'],\n",
        "    baseline_metrics['rmse'],\n",
        "    baseline_metrics['r2'],\n",
        "    baseline_metrics['pws_percent'],\n",
        "    baseline_metrics['estimated_loss'] / 1000\n",
        "]\n",
        "\n",
        "bars = ax4.bar(metrics_names, metrics_values, color=['red', 'blue', 'green', 'orange', 'purple'], alpha=0.7)\n",
        "ax4.set_title('Performance Metrics Summary')\n",
        "ax4.set_ylabel('Value')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, metrics_values):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "             f'{value:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udcca Baseline model performance analysis complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Simulate Data Drift\n",
        "\n",
        "Now let's simulate various types of drift that can occur in semiconductor manufacturing and see how our monitoring system detects them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate different types of drifted data\n",
        "print(\"\ud83d\udd04 Generating drift scenarios...\")\n",
        "\n",
        "# Scenario 1: Minor drift (equipment aging)\n",
        "minor_drift_data = generate_drift_data(baseline_data, drift_strength=0.2)\n",
        "\n",
        "# Scenario 2: Major drift (process recipe change)\n",
        "major_drift_data = generate_drift_data(baseline_data, drift_strength=0.8)\n",
        "\n",
        "# Scenario 3: Sudden shift (equipment maintenance)\n",
        "sudden_shift_data = baseline_data.copy()\n",
        "sudden_shift_data['temperature'] += 10  # 10\u00b0C temperature increase\n",
        "sudden_shift_data['pressure'] += 0.2    # 0.2 Torr pressure increase\n",
        "\n",
        "# Recalculate engineered features for sudden shift\n",
        "sudden_shift_data['temp_centered'] = sudden_shift_data['temperature'] - sudden_shift_data['temperature'].mean()\n",
        "sudden_shift_data['pressure_sq'] = sudden_shift_data['pressure'] ** 2\n",
        "sudden_shift_data['temp_flow_inter'] = sudden_shift_data['temperature'] * sudden_shift_data['flow']\n",
        "\n",
        "print(\"\u2705 Drift scenarios generated:\")\n",
        "print(\"   \ud83d\udcc9 Minor drift (equipment aging)\")\n",
        "print(\"   \ud83d\udcc9 Major drift (recipe change)\")\n",
        "print(\"   \ud83d\udcc9 Sudden shift (maintenance event)\")\n",
        "\n",
        "# Display drift characteristics\n",
        "scenarios = {\n",
        "    'Baseline': baseline_data,\n",
        "    'Minor Drift': minor_drift_data,\n",
        "    'Major Drift': major_drift_data,\n",
        "    'Sudden Shift': sudden_shift_data\n",
        "}\n",
        "\n",
        "drift_summary = pd.DataFrame({\n",
        "    scenario: {\n",
        "        'Temp Mean': data['temperature'].mean(),\n",
        "        'Temp Std': data['temperature'].std(),\n",
        "        'Pressure Mean': data['pressure'].mean(),\n",
        "        'Pressure Std': data['pressure'].std()\n",
        "    }\n",
        "    for scenario, data in scenarios.items()\n",
        "}).T\n",
        "\n",
        "print(\"\\n\ud83d\udcca Drift Summary:\")\n",
        "print(drift_summary.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Drift Patterns\n",
        "\n",
        "Let's visualize how different types of drift affect our process parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive drift visualization\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
        "fig.suptitle('Drift Detection Analysis: Process Parameter Changes', fontsize=16, fontweight='bold')\n",
        "\n",
        "parameters = ['temperature', 'pressure']\n",
        "colors = ['red', 'orange', 'darkred', 'blue']\n",
        "scenario_names = ['Baseline', 'Minor Drift', 'Major Drift', 'Sudden Shift']\n",
        "\n",
        "for i, param in enumerate(parameters):\n",
        "    # Distribution comparison\n",
        "    ax_dist = axes[i, 0]\n",
        "    for j, (scenario, data) in enumerate(scenarios.items()):\n",
        "        ax_dist.hist(data[param], bins=30, alpha=0.6, label=scenario, \n",
        "                    color=colors[j], density=True)\n",
        "    ax_dist.set_title(f'{param.title()} Distribution Comparison')\n",
        "    ax_dist.set_xlabel(param.title())\n",
        "    ax_dist.set_ylabel('Density')\n",
        "    ax_dist.legend()\n",
        "    ax_dist.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot comparison\n",
        "    ax_box = axes[i, 1]\n",
        "    box_data = [data[param] for data in scenarios.values()]\n",
        "    box_plot = ax_box.boxplot(box_data, labels=scenario_names, patch_artist=True)\n",
        "    for patch, color in zip(box_plot['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "    ax_box.set_title(f'{param.title()} Box Plot Comparison')\n",
        "    ax_box.set_ylabel(param.title())\n",
        "    ax_box.tick_params(axis='x', rotation=45)\n",
        "    ax_box.grid(True, alpha=0.3)\n",
        "\n",
        "# Create a comprehensive drift metrics comparison\n",
        "ax_metrics = axes[2, :]\n",
        "ax_metrics = plt.subplot(3, 1, 3)  # Span full width\n",
        "\n",
        "# Calculate drift metrics for each scenario\n",
        "drift_metrics_results = {}\n",
        "reference_data = baseline_data[feature_cols]\n",
        "\n",
        "for scenario_name, scenario_data in scenarios.items():\n",
        "    if scenario_name == 'Baseline':\n",
        "        continue\n",
        "    \n",
        "    current_data = scenario_data[feature_cols]\n",
        "    drift_info = detect_drift(reference_data, current_data, DriftConfig())\n",
        "    \n",
        "    # Extract key metrics\n",
        "    temp_psi = drift_info['drift_scores'].get('temperature_psi', 0)\n",
        "    pressure_psi = drift_info['drift_scores'].get('pressure_psi', 0)\n",
        "    temp_ks_p = drift_info['drift_scores'].get('temperature_ks_p', 1)\n",
        "    pressure_ks_p = drift_info['drift_scores'].get('pressure_ks_p', 1)\n",
        "    \n",
        "    drift_metrics_results[scenario_name] = {\n",
        "        'Temperature PSI': temp_psi,\n",
        "        'Pressure PSI': pressure_psi,\n",
        "        'Temperature KS p-value': temp_ks_p,\n",
        "        'Pressure KS p-value': pressure_ks_p\n",
        "    }\n",
        "\n",
        "# Plot drift metrics\n",
        "metrics_df = pd.DataFrame(drift_metrics_results).T\n",
        "metrics_df[['Temperature PSI', 'Pressure PSI']].plot(kind='bar', ax=ax_metrics, \n",
        "                                                    color=['red', 'blue'], alpha=0.7)\n",
        "ax_metrics.axhline(y=0.2, color='orange', linestyle='--', linewidth=2, label='PSI Warning Threshold')\n",
        "ax_metrics.axhline(y=0.1, color='green', linestyle='--', linewidth=2, label='PSI Normal Threshold')\n",
        "ax_metrics.set_title('Drift Detection Metrics (PSI Scores)')\n",
        "ax_metrics.set_ylabel('PSI Score')\n",
        "ax_metrics.set_xlabel('Drift Scenario')\n",
        "ax_metrics.legend()\n",
        "ax_metrics.grid(True, alpha=0.3)\n",
        "ax_metrics.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udcca Drift visualization complete\")\n",
        "print(\"\\n\ud83d\udcc8 Drift Metrics Summary:\")\n",
        "print(metrics_df.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comprehensive Drift Detection\n",
        "\n",
        "Let's run our comprehensive drift detection system on each scenario and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model performance under different drift conditions\n",
        "print(\"\ud83d\udd0d Evaluating model performance under drift conditions...\\n\")\n",
        "\n",
        "drift_evaluation_results = {}\n",
        "\n",
        "for scenario_name, scenario_data in scenarios.items():\n",
        "    if scenario_name == 'Baseline':\n",
        "        continue\n",
        "    \n",
        "    print(f\"\ud83d\udcca Evaluating {scenario_name}...\")\n",
        "    \n",
        "    # Prepare data\n",
        "    X_drift = scenario_data[feature_cols]\n",
        "    y_drift = scenario_data['target'].values\n",
        "    \n",
        "    # Use a subset for faster evaluation\n",
        "    n_samples = min(400, len(X_drift))\n",
        "    X_drift_subset = X_drift.iloc[:n_samples]\n",
        "    y_drift_subset = y_drift[:n_samples]\n",
        "    \n",
        "    # Evaluate with drift detection\n",
        "    eval_results = pipeline.evaluate(X_drift_subset, y_drift_subset)\n",
        "    drift_evaluation_results[scenario_name] = eval_results\n",
        "    \n",
        "    # Print key findings\n",
        "    print(f\"   \ud83d\udcc8 MAE: {eval_results['mae']:.3f} (baseline: {baseline_metrics['mae']:.3f})\")\n",
        "    print(f\"   \ud83d\udcc8 PWS: {eval_results['pws_percent']:.1f}% (baseline: {baseline_metrics['pws_percent']:.1f}%)\")\n",
        "    print(f\"   \ud83d\udea8 Drift detected: {any(eval_results['alert_flags'].values())}\")\n",
        "    print(f\"   \ud83d\udcca Active alerts: {sum(eval_results['alert_flags'].values())}\")\n",
        "    print()\n",
        "\n",
        "print(\"\u2705 Drift evaluation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Alert Dashboard Simulation\n",
        "\n",
        "Let's create a dashboard-style visualization showing how our alert system would look in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create alert dashboard visualization\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "fig.suptitle('Semiconductor Model Monitoring Dashboard', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Create grid layout\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Overall Health Score\n",
        "ax_health = fig.add_subplot(gs[0, 0])\n",
        "health_scores = []\n",
        "scenario_labels = []\n",
        "\n",
        "for scenario_name, results in drift_evaluation_results.items():\n",
        "    # Calculate health score (0-100)\n",
        "    performance_score = max(0, 100 - (results['mae'] / baseline_metrics['mae'] - 1) * 100)\n",
        "    drift_penalty = sum(results['alert_flags'].values()) * 10\n",
        "    health_score = max(0, performance_score - drift_penalty)\n",
        "    \n",
        "    health_scores.append(health_score)\n",
        "    scenario_labels.append(scenario_name)\n",
        "\n",
        "colors_health = ['green' if score > 80 else 'orange' if score > 50 else 'red' for score in health_scores]\n",
        "bars = ax_health.bar(scenario_labels, health_scores, color=colors_health, alpha=0.7)\n",
        "ax_health.set_title('Model Health Score', fontweight='bold')\n",
        "ax_health.set_ylabel('Health Score (0-100)')\n",
        "ax_health.set_ylim(0, 100)\n",
        "ax_health.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add score labels\n",
        "for bar, score in zip(bars, health_scores):\n",
        "    height = bar.get_height()\n",
        "    ax_health.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                   f'{score:.0f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. Performance Degradation\n",
        "ax_perf = fig.add_subplot(gs[0, 1])\n",
        "mae_values = [results['mae'] for results in drift_evaluation_results.values()]\n",
        "baseline_mae_line = [baseline_metrics['mae']] * len(mae_values)\n",
        "\n",
        "ax_perf.plot(scenario_labels, mae_values, 'o-', color='red', linewidth=2, markersize=8, label='Current MAE')\n",
        "ax_perf.plot(scenario_labels, baseline_mae_line, '--', color='blue', linewidth=2, label='Baseline MAE')\n",
        "ax_perf.fill_between(scenario_labels, baseline_mae_line, \n",
        "                     [x * 1.15 for x in baseline_mae_line], alpha=0.2, color='orange', label='Warning Zone')\n",
        "ax_perf.set_title('Performance Trend', fontweight='bold')\n",
        "ax_perf.set_ylabel('Mean Absolute Error')\n",
        "ax_perf.legend(loc='upper left')\n",
        "ax_perf.tick_params(axis='x', rotation=45)\n",
        "ax_perf.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Alert Summary\n",
        "ax_alerts = fig.add_subplot(gs[0, 2])\n",
        "alert_counts = [sum(results['alert_flags'].values()) for results in drift_evaluation_results.values()]\n",
        "colors_alerts = ['green' if count == 0 else 'orange' if count < 5 else 'red' for count in alert_counts]\n",
        "\n",
        "bars = ax_alerts.bar(scenario_labels, alert_counts, color=colors_alerts, alpha=0.7)\n",
        "ax_alerts.set_title('Active Alerts', fontweight='bold')\n",
        "ax_alerts.set_ylabel('Number of Alerts')\n",
        "ax_alerts.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 4. Drift Heatmap\n",
        "ax_heatmap = fig.add_subplot(gs[1, :])\n",
        "drift_matrix = []\n",
        "drift_features = ['temperature_psi', 'pressure_psi', 'flow_psi', 'time_psi']\n",
        "\n",
        "for scenario_name, results in drift_evaluation_results.items():\n",
        "    drift_row = [results['drift_scores'].get(feature, 0) for feature in drift_features]\n",
        "    drift_matrix.append(drift_row)\n",
        "\n",
        "im = ax_heatmap.imshow(drift_matrix, cmap='RdYlGn_r', aspect='auto', vmin=0, vmax=0.5)\n",
        "ax_heatmap.set_xticks(range(len(drift_features)))\n",
        "ax_heatmap.set_xticklabels([f.replace('_psi', '').title() for f in drift_features])\n",
        "ax_heatmap.set_yticks(range(len(scenario_labels)))\n",
        "ax_heatmap.set_yticklabels(scenario_labels)\n",
        "ax_heatmap.set_title('Drift Detection Heatmap (PSI Scores)', fontweight='bold')\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax_heatmap, orientation='horizontal', pad=0.1, shrink=0.8)\n",
        "cbar.set_label('PSI Score')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(scenario_labels)):\n",
        "    for j in range(len(drift_features)):\n",
        "        text = ax_heatmap.text(j, i, f'{drift_matrix[i][j]:.3f}',\n",
        "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "# 5. Manufacturing Metrics\n",
        "ax_mfg = fig.add_subplot(gs[2, :])\n",
        "pws_values = [results['pws_percent'] for results in drift_evaluation_results.values()]\n",
        "loss_values = [results['estimated_loss']/1000 for results in drift_evaluation_results.values()]\n",
        "\n",
        "ax_mfg_twin = ax_mfg.twinx()\n",
        "\n",
        "line1 = ax_mfg.plot(scenario_labels, pws_values, 'o-', color='green', linewidth=3, \n",
        "                    markersize=10, label='PWS %')\n",
        "line2 = ax_mfg_twin.plot(scenario_labels, loss_values, 's-', color='red', linewidth=3, \n",
        "                         markersize=10, label='Est. Loss ($K)')\n",
        "\n",
        "ax_mfg.set_xlabel('Drift Scenario')\n",
        "ax_mfg.set_ylabel('Prediction Within Spec (%)', color='green')\n",
        "ax_mfg_twin.set_ylabel('Estimated Loss ($K)', color='red')\n",
        "ax_mfg.set_title('Manufacturing Impact Metrics', fontweight='bold')\n",
        "ax_mfg.tick_params(axis='x', rotation=45)\n",
        "ax_mfg.grid(True, alpha=0.3)\n",
        "\n",
        "# Add combined legend\n",
        "lines1, labels1 = ax_mfg.get_legend_handles_labels()\n",
        "lines2, labels2 = ax_mfg_twin.get_legend_handles_labels()\n",
        "ax_mfg.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udcca Alert dashboard visualization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Production Deployment Simulation\n",
        "\n",
        "Let's simulate how this monitoring system would work in a production environment with streaming data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate production monitoring with streaming data\n",
        "print(\"\ud83c\udfed Simulating production monitoring...\")\n",
        "\n",
        "# Create time series data simulating a production day\n",
        "import datetime\n",
        "\n",
        "# Generate hourly data for 24 hours\n",
        "time_points = pd.date_range(start='2024-01-01 00:00:00', periods=24, freq='1H')\n",
        "monitoring_log = []\n",
        "\n",
        "for i, timestamp in enumerate(time_points):\n",
        "    # Simulate gradual drift throughout the day\n",
        "    drift_factor = i / 24.0 * 0.3  # Gradual drift over 24 hours\n",
        "    \n",
        "    # Generate data with increasing drift\n",
        "    hourly_data = generate_yield_process(n=50, seed=42 + i)\n",
        "    if drift_factor > 0:\n",
        "        hourly_data = generate_drift_data(hourly_data, drift_strength=drift_factor)\n",
        "    \n",
        "    # Evaluate model performance\n",
        "    X_hourly = hourly_data[feature_cols]\n",
        "    y_hourly = hourly_data['target'].values\n",
        "    \n",
        "    eval_results = pipeline.evaluate(X_hourly, y_hourly)\n",
        "    \n",
        "    # Log results\n",
        "    log_entry = {\n",
        "        'timestamp': timestamp,\n",
        "        'hour': i,\n",
        "        'mae': eval_results['mae'],\n",
        "        'rmse': eval_results['rmse'],\n",
        "        'pws_percent': eval_results['pws_percent'],\n",
        "        'estimated_loss': eval_results['estimated_loss'],\n",
        "        'drift_detected': any(eval_results['alert_flags'].values()),\n",
        "        'alert_count': sum(eval_results['alert_flags'].values()),\n",
        "        'temp_psi': eval_results['drift_scores'].get('temperature_psi', 0),\n",
        "        'pressure_psi': eval_results['drift_scores'].get('pressure_psi', 0)\n",
        "    }\n",
        "    monitoring_log.append(log_entry)\n",
        "\n",
        "# Convert to DataFrame\n",
        "monitoring_df = pd.DataFrame(monitoring_log)\n",
        "monitoring_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "print(f\"\u2705 Generated {len(monitoring_df)} hours of monitoring data\")\n",
        "print(f\"\ud83d\udea8 Drift detected in {monitoring_df['drift_detected'].sum()} hours\")\n",
        "print(f\"\ud83d\udcca Average alerts per hour: {monitoring_df['alert_count'].mean():.1f}\")\n",
        "\n",
        "# Display sample of monitoring log\n",
        "print(\"\\n\ud83d\udccb Sample Monitoring Log:\")\n",
        "print(monitoring_df[['hour', 'mae', 'pws_percent', 'drift_detected', 'alert_count']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize production monitoring timeline\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 16))\n",
        "fig.suptitle('24-Hour Production Monitoring Timeline', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Performance metrics over time\n",
        "ax1 = axes[0]\n",
        "ax1_twin = ax1.twinx()\n",
        "\n",
        "line1 = ax1.plot(monitoring_df.index, monitoring_df['mae'], 'b-', linewidth=2, label='MAE')\n",
        "line2 = ax1_twin.plot(monitoring_df.index, monitoring_df['pws_percent'], 'g-', linewidth=2, label='PWS %')\n",
        "\n",
        "# Add baseline reference\n",
        "ax1.axhline(y=baseline_metrics['mae'], color='blue', linestyle='--', alpha=0.5, label='Baseline MAE')\n",
        "ax1_twin.axhline(y=baseline_metrics['pws_percent'], color='green', linestyle='--', alpha=0.5, label='Baseline PWS')\n",
        "\n",
        "ax1.set_ylabel('Mean Absolute Error', color='blue')\n",
        "ax1_twin.set_ylabel('Prediction Within Spec (%)', color='green')\n",
        "ax1.set_title('Model Performance Over Time')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Combined legend\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax1_twin.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "# 2. Drift scores over time\n",
        "ax2 = axes[1]\n",
        "ax2.plot(monitoring_df.index, monitoring_df['temp_psi'], 'r-', linewidth=2, label='Temperature PSI')\n",
        "ax2.plot(monitoring_df.index, monitoring_df['pressure_psi'], 'b-', linewidth=2, label='Pressure PSI')\n",
        "ax2.axhline(y=0.2, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Warning Threshold')\n",
        "ax2.axhline(y=0.1, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Normal Threshold')\n",
        "ax2.set_ylabel('PSI Score')\n",
        "ax2.set_title('Drift Detection Scores')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Alert timeline\n",
        "ax3 = axes[2]\n",
        "alert_colors = ['red' if count > 5 else 'orange' if count > 0 else 'green' \n",
        "                for count in monitoring_df['alert_count']]\n",
        "bars = ax3.bar(monitoring_df.index, monitoring_df['alert_count'], \n",
        "               color=alert_colors, alpha=0.7, width=0.8/24)\n",
        "ax3.set_ylabel('Number of Alerts')\n",
        "ax3.set_title('Alert Activity Timeline')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Business impact\n",
        "ax4 = axes[3]\n",
        "cumulative_loss = monitoring_df['estimated_loss'].cumsum() / 1000  # Convert to thousands\n",
        "ax4.plot(monitoring_df.index, cumulative_loss, 'r-', linewidth=3, label='Cumulative Loss')\n",
        "ax4.fill_between(monitoring_df.index, 0, cumulative_loss, alpha=0.3, color='red')\n",
        "ax4.set_ylabel('Cumulative Loss ($K)')\n",
        "ax4.set_xlabel('Time')\n",
        "ax4.set_title('Business Impact Over Time')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Format x-axis for all subplots\n",
        "for ax in axes:\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udcca Production monitoring timeline visualization complete\")\n",
        "print(f\"\ud83d\udcb0 Total estimated loss over 24 hours: ${cumulative_loss.iloc[-1]:,.0f}K\")\n",
        "print(f\"\ud83d\udcc8 Performance degradation: {((monitoring_df['mae'].iloc[-1] / baseline_metrics['mae']) - 1) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Insights and Recommendations\n",
        "\n",
        "Let's summarize the key insights from our monitoring analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive monitoring summary\n",
        "print(\"\ud83d\udcca MONITORING & MAINTENANCE ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\ud83c\udfaf BASELINE MODEL PERFORMANCE:\")\n",
        "print(f\"   \u2022 MAE: {baseline_metrics['mae']:.3f}\")\n",
        "print(f\"   \u2022 RMSE: {baseline_metrics['rmse']:.3f}\")\n",
        "print(f\"   \u2022 R\u00b2: {baseline_metrics['r2']:.3f}\")\n",
        "print(f\"   \u2022 PWS: {baseline_metrics['pws_percent']:.1f}%\")\n",
        "print(f\"   \u2022 Estimated Loss: ${baseline_metrics['estimated_loss']:,.0f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udea8 DRIFT DETECTION RESULTS:\")\n",
        "for scenario_name, results in drift_evaluation_results.items():\n",
        "    performance_change = ((results['mae'] / baseline_metrics['mae']) - 1) * 100\n",
        "    pws_change = results['pws_percent'] - baseline_metrics['pws_percent']\n",
        "    \n",
        "    print(f\"\\n   \ud83d\udcc9 {scenario_name.upper()}:\")\n",
        "    print(f\"      \u2022 Performance change: {performance_change:+.1f}%\")\n",
        "    print(f\"      \u2022 PWS change: {pws_change:+.1f}%\")\n",
        "    print(f\"      \u2022 Drift alerts: {sum(results['alert_flags'].values())}\")\n",
        "    print(f\"      \u2022 Max PSI score: {max(v for k, v in results['drift_scores'].items() if 'psi' in k):.3f}\")\n",
        "\n",
        "print(\"\\n\u26a1 PRODUCTION SIMULATION INSIGHTS:\")\n",
        "print(f\"   \u2022 Hours with drift detected: {monitoring_df['drift_detected'].sum()}/24\")\n",
        "print(f\"   \u2022 Peak alert count: {monitoring_df['alert_count'].max()}\")\n",
        "print(f\"   \u2022 Performance degradation at end: {((monitoring_df['mae'].iloc[-1] / baseline_metrics['mae']) - 1) * 100:.1f}%\")\n",
        "print(f\"   \u2022 Total business impact: ${cumulative_loss.iloc[-1]:,.0f}K\")\n",
        "\n",
        "print(\"\\n\ud83d\udd27 RECOMMENDED ACTIONS:\")\n",
        "print(\"   1. \ud83c\udfaf Set PSI thresholds:\")\n",
        "print(\"      \u2022 Normal: < 0.1 (green)\")\n",
        "print(\"      \u2022 Warning: 0.1-0.2 (yellow)\")\n",
        "print(\"      \u2022 Critical: > 0.2 (red)\")\n",
        "\n",
        "print(\"\\n   2. \ud83d\udcca Monitor key metrics:\")\n",
        "print(\"      \u2022 Model accuracy (MAE/RMSE)\")\n",
        "print(\"      \u2022 Prediction Within Spec (PWS)\")\n",
        "print(\"      \u2022 Business impact (Estimated Loss)\")\n",
        "print(\"      \u2022 Drift scores (PSI, KS-test, Wasserstein)\")\n",
        "\n",
        "print(\"\\n   3. \ud83d\udea8 Alert escalation levels:\")\n",
        "print(\"      \u2022 Info: PSI 0.1-0.15, performance degradation < 10%\")\n",
        "print(\"      \u2022 Warning: PSI 0.15-0.25, performance degradation 10-20%\")\n",
        "print(\"      \u2022 Critical: PSI > 0.25, performance degradation > 20%\")\n",
        "\n",
        "print(\"\\n   4. \ud83d\udd04 Maintenance schedule:\")\n",
        "print(\"      \u2022 Daily: Review alert logs and performance trends\")\n",
        "print(\"      \u2022 Weekly: Analyze drift patterns and update thresholds\")\n",
        "print(\"      \u2022 Monthly: Model retraining evaluation\")\n",
        "print(\"      \u2022 Quarterly: Comprehensive performance review\")\n",
        "\n",
        "print(\"\\n   5. \ud83c\udfed Production deployment:\")\n",
        "print(\"      \u2022 Use MLflow for experiment tracking\")\n",
        "print(\"      \u2022 Implement blue-green deployment\")\n",
        "print(\"      \u2022 Set up automated rollback triggers\")\n",
        "print(\"      \u2022 Monitor business KPIs continuously\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\u2705 ANALYSIS COMPLETE - READY FOR PRODUCTION DEPLOYMENT\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Model and Monitoring Configuration\n",
        "\n",
        "Finally, let's save our trained model and monitoring configuration for production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and monitoring configuration\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path('monitoring_output')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save the trained model\n",
        "model_path = output_dir / 'production_model.joblib'\n",
        "pipeline.save(model_path)\n",
        "print(f\"\ud83d\udcbe Model saved to: {model_path}\")\n",
        "\n",
        "# Save monitoring configuration\n",
        "monitoring_config = {\n",
        "    'drift_thresholds': {\n",
        "        'psi_warning': 0.1,\n",
        "        'psi_critical': 0.2,\n",
        "        'ks_p_warning': 0.05,\n",
        "        'ks_p_critical': 0.01,\n",
        "        'wasserstein_warning': 0.5,\n",
        "        'wasserstein_critical': 1.0\n",
        "    },\n",
        "    'performance_thresholds': {\n",
        "        'mae_warning_pct': 10,\n",
        "        'mae_critical_pct': 20,\n",
        "        'pws_warning_min': 90,\n",
        "        'pws_critical_min': 80\n",
        "    },\n",
        "    'alert_settings': {\n",
        "        'consecutive_violations': 2,\n",
        "        'evaluation_frequency_minutes': 15,\n",
        "        'reporting_frequency_hours': 1\n",
        "    },\n",
        "    'baseline_metrics': {\n",
        "        'mae': float(baseline_metrics['mae']),\n",
        "        'rmse': float(baseline_metrics['rmse']),\n",
        "        'r2': float(baseline_metrics['r2']),\n",
        "        'pws_percent': float(baseline_metrics['pws_percent']),\n",
        "        'estimated_loss': float(baseline_metrics['estimated_loss'])\n",
        "    }\n",
        "}\n",
        "\n",
        "config_path = output_dir / 'monitoring_config.json'\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(monitoring_config, f, indent=2)\n",
        "print(f\"\u2699\ufe0f  Monitoring config saved to: {config_path}\")\n",
        "\n",
        "# Save monitoring results\n",
        "results_path = output_dir / 'monitoring_results.csv'\n",
        "monitoring_df.to_csv(results_path)\n",
        "print(f\"\ud83d\udcca Monitoring results saved to: {results_path}\")\n",
        "\n",
        "# Create deployment script\n",
        "deployment_script = '''#!/bin/bash\n",
        "# Production deployment script for semiconductor monitoring\n",
        "\n",
        "echo \"\ud83d\ude80 Deploying Model Monitoring System...\"\n",
        "\n",
        "# Start MLflow server\n",
        "mlflow server --backend-store-uri sqlite:///production_mlflow.db \\\n",
        "              --default-artifact-root ./mlruns \\\n",
        "              --port 5000 &\n",
        "\n",
        "echo \"\ud83d\udcca MLflow server started on port 5000\"\n",
        "\n",
        "# Set environment variables\n",
        "export MLFLOW_TRACKING_URI=http://localhost:5000\n",
        "export MODEL_PATH=monitoring_output/production_model.joblib\n",
        "export CONFIG_PATH=monitoring_output/monitoring_config.json\n",
        "\n",
        "echo \"\u2705 Environment configured\"\n",
        "echo \"\ud83d\udd17 MLflow UI: http://localhost:5000\"\n",
        "echo \"\ud83d\udccb Ready for production monitoring!\"\n",
        "'''\n",
        "\n",
        "deploy_script_path = output_dir / 'deploy.sh'\n",
        "with open(deploy_script_path, 'w') as f:\n",
        "    f.write(deployment_script)\n",
        "deploy_script_path.chmod(0o755)  # Make executable\n",
        "print(f\"\ud83d\ude80 Deployment script saved to: {deploy_script_path}\")\n",
        "\n",
        "print(\"\\n\u2705 All artifacts saved successfully!\")\n",
        "print(f\"\ud83d\udcc1 Output directory: {output_dir.absolute()}\")\n",
        "print(\"\\n\ud83d\udd27 To deploy in production:\")\n",
        "print(f\"   cd {output_dir}\")\n",
        "print(\"   ./deploy.sh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Summary\n",
        "\n",
        "Congratulations! You've successfully completed the monitoring and maintenance module. Here's what you've learned:\n",
        "\n",
        "### \ud83d\udd2c **Technical Skills Gained**\n",
        "- Set up MLflow for experiment tracking and model registry\n",
        "- Implemented multiple drift detection methods (PSI, KS-test, Wasserstein)\n",
        "- Created manufacturing-specific metrics (PWS, Estimated Loss)\n",
        "- Built comprehensive alerting and monitoring dashboards\n",
        "- Simulated production deployment scenarios\n",
        "\n",
        "### \ud83d\udcca **Key Insights**\n",
        "- Different drift types require different detection strategies\n",
        "- Manufacturing metrics provide business-relevant model evaluation\n",
        "- Alert thresholds must balance sensitivity vs. false alarm rates\n",
        "- Continuous monitoring is essential for production ML systems\n",
        "- Proper tooling (MLflow) significantly improves model lifecycle management\n",
        "\n",
        "### \ud83d\ude80 **Next Steps**\n",
        "1. **Deploy** the monitoring system in your environment\n",
        "2. **Customize** thresholds based on your specific process requirements\n",
        "3. **Integrate** with existing manufacturing systems and dashboards\n",
        "4. **Train** your team on interpreting alerts and taking corrective actions\n",
        "5. **Establish** maintenance schedules and escalation procedures\n",
        "\n",
        "### \ud83d\udca1 **Best Practices Learned**\n",
        "- Monitor both statistical drift and business impact\n",
        "- Use multiple drift detection methods for robustness\n",
        "- Implement gradual alerting (info \u2192 warning \u2192 critical)\n",
        "- Track model lineage and approval workflows\n",
        "- Plan for graceful degradation and fallback strategies\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf89 You're now ready to implement robust model monitoring in production semiconductor manufacturing environments!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
