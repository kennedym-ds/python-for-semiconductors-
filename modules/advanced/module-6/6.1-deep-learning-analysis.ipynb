{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Module 6.1: Deep Learning Analysis - Interactive Notebook\n",
                "\n",
                "This notebook provides hands-on implementation of deep learning techniques for semiconductor manufacturing process engineering. We'll explore multi-layer perceptrons (MLPs) for tabular process data using both PyTorch and TensorFlow.\n",
                "\n",
                "## Learning Objectives\n",
                "- Implement MLPs for regression and classification tasks\n",
                "- Compare PyTorch vs TensorFlow implementations  \n",
                "- Apply regularization techniques to prevent overfitting\n",
                "- Optimize hyperparameters for semiconductor datasets\n",
                "- Evaluate models using manufacturing-specific metrics\n",
                "\n",
                "## Outline\n",
                "1. Environment Setup & Data Loading\n",
                "2. Neural Network Theory Visualization\n",
                "3. Simple MLP Implementation (PyTorch)\n",
                "4. TensorFlow Implementation Comparison\n",
                "5. Regularization Techniques Deep Dive\n",
                "6. Hyperparameter Optimization\n",
                "7. Manufacturing Metrics & Evaluation\n",
                "8. Production Pipeline Integration\n",
                "9. Case Study: Yield Prediction\n",
                "10. Advanced Topics & Next Steps"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-header",
            "metadata": {},
            "source": [
                "## 1. Environment Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core imports\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "RANDOM_SEED = 42\n",
                "np.random.seed(RANDOM_SEED)\n",
                "\n",
                "# Configure plotting\n",
                "plt.style.use('default')\n",
                "plt.rcParams['figure.figsize'] = (12, 8)\n",
                "plt.rcParams['font.size'] = 10\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"✓ Core libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "path-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up data paths (Module 6 is in advanced tier)\n",
                "from pathlib import Path\n",
                "DATA_DIR = Path('../../../datasets').resolve()\n",
                "\n",
                "print(f\"Data directory: {DATA_DIR}\")\n",
                "print(f\"Data directory exists: {DATA_DIR.exists()}\")\n",
                "\n",
                "# Import our pipeline module\n",
                "import sys\n",
                "sys.path.append('.')\n",
                "from importlib import import_module\n",
                "\n",
                "try:\n",
                "    pipeline_module = import_module('6-1-deep-learning-pipeline')\n",
                "    print(\"✓ Pipeline module loaded successfully\")\n",
                "except ImportError as e:\n",
                "    print(f\"⚠ Could not import pipeline: {e}\")\n",
                "    print(\"Will implement functions directly in notebook\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "deep-learning-imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deep learning framework imports with graceful fallbacks\n",
                "HAS_TORCH = False\n",
                "HAS_TF = False\n",
                "\n",
                "try:\n",
                "    import torch\n",
                "    import torch.nn as nn\n",
                "    import torch.optim as optim\n",
                "    import torch.nn.functional as F\n",
                "    from torch.utils.data import DataLoader, TensorDataset\n",
                "    \n",
                "    # Set deterministic behavior\n",
                "    torch.manual_seed(RANDOM_SEED)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(RANDOM_SEED)\n",
                "    \n",
                "    HAS_TORCH = True\n",
                "    print(\"✓ PyTorch loaded successfully\")\n",
                "    print(f\"  PyTorch version: {torch.__version__}\")\n",
                "    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"⚠ PyTorch not available\")\n",
                "\n",
                "try:\n",
                "    import tensorflow as tf\n",
                "    from tensorflow import keras\n",
                "    from tensorflow.keras import layers, Sequential\n",
                "    \n",
                "    # Set deterministic behavior\n",
                "    tf.random.set_seed(RANDOM_SEED) \n",
                "    \n",
                "    HAS_TF = True\n",
                "    print(\"✓ TensorFlow loaded successfully\")\n",
                "    print(f\"  TensorFlow version: {tf.__version__}\")\n",
                "    print(f\"  GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"⚠ TensorFlow not available\")\n",
                "\n",
                "# Scikit-learn for comparison and metrics\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "print(\"✓ Scikit-learn imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-gen-header",
            "metadata": {},
            "source": [
                "### Generate Synthetic Semiconductor Data\n",
                "\n",
                "We'll create realistic semiconductor process datasets for both regression (yield prediction) and classification (defect detection) tasks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-generation",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_semiconductor_yield_data(n_samples=1000, n_features=10, noise_level=0.1):\n",
                "    \"\"\"\n",
                "    Generate synthetic semiconductor yield data with realistic parameter relationships\n",
                "    \"\"\"\n",
                "    np.random.seed(RANDOM_SEED)\n",
                "    \n",
                "    # Generate process parameters\n",
                "    data = {}\n",
                "    \n",
                "    # Temperature parameters (key drivers)\n",
                "    data['temperature'] = np.random.normal(450, 20, n_samples)  # Celsius\n",
                "    data['temp_ramp_rate'] = np.random.normal(5, 1, n_samples)  # C/min\n",
                "    \n",
                "    # Pressure parameters  \n",
                "    data['pressure'] = np.random.normal(2.5, 0.3, n_samples)  # Torr\n",
                "    data['pump_speed'] = np.random.normal(100, 10, n_samples)  # L/s\n",
                "    \n",
                "    # Gas flow parameters\n",
                "    data['n2_flow'] = np.random.normal(200, 30, n_samples)  # sccm\n",
                "    data['ar_flow'] = np.random.normal(50, 10, n_samples)   # sccm\n",
                "    \n",
                "    # Time parameters\n",
                "    data['process_time'] = np.random.normal(300, 30, n_samples)  # seconds\n",
                "    data['dwell_time'] = np.random.normal(60, 10, n_samples)     # seconds\n",
                "    \n",
                "    # Equipment parameters\n",
                "    data['rf_power'] = np.random.normal(1500, 100, n_samples)   # Watts\n",
                "    data['chamber_cycles'] = np.random.poisson(10, n_samples)   # cycles since clean\n",
                "    \n",
                "    # Add additional features if needed\n",
                "    for i in range(n_features - len(data)):\n",
                "        data[f'param_{i+1}'] = np.random.normal(0, 1, n_samples)\n",
                "    \n",
                "    X = pd.DataFrame(data)\n",
                "    \n",
                "    # Create complex yield relationships\n",
                "    # Main effects\n",
                "    yield_base = 85.0  # Base yield %\n",
                "    \n",
                "    # Temperature effects (optimal around 450C)\n",
                "    temp_effect = -0.05 * (X['temperature'] - 450)**2 / 100\n",
                "    \n",
                "    # Pressure effects (optimal around 2.5 Torr)\n",
                "    pressure_effect = -2.0 * (X['pressure'] - 2.5)**2\n",
                "    \n",
                "    # Flow rate interaction\n",
                "    flow_effect = 0.01 * X['n2_flow'] * X['ar_flow'] / 10000\n",
                "    \n",
                "    # Time effects (diminishing returns)\n",
                "    time_effect = 0.02 * np.log(X['process_time'] / 300)\n",
                "    \n",
                "    # Chamber condition effects\n",
                "    chamber_effect = -0.3 * X['chamber_cycles']\n",
                "    \n",
                "    # RF power effects\n",
                "    rf_effect = 0.005 * (X['rf_power'] - 1500) / 100\n",
                "    \n",
                "    # Non-linear interactions\n",
                "    interaction_1 = -0.001 * X['temperature'] * X['pressure']\n",
                "    interaction_2 = 0.0001 * X['rf_power'] * X['process_time'] / 1000\n",
                "    \n",
                "    # Combine all effects\n",
                "    y = (yield_base + temp_effect + pressure_effect + flow_effect + \n",
                "         time_effect + chamber_effect + rf_effect + \n",
                "         interaction_1 + interaction_2)\n",
                "    \n",
                "    # Add noise\n",
                "    y += np.random.normal(0, noise_level * y.std(), n_samples)\n",
                "    \n",
                "    # Constrain to realistic yield range\n",
                "    y = np.clip(y, 60, 98)\n",
                "    \n",
                "    return X, y\n",
                "\n",
                "def generate_semiconductor_defect_data(n_samples=1000, n_features=8):\n",
                "    \"\"\"\n",
                "    Generate synthetic semiconductor defect classification data\n",
                "    \"\"\"\n",
                "    np.random.seed(RANDOM_SEED)\n",
                "    \n",
                "    # Process parameters\n",
                "    data = {}\n",
                "    data['etch_rate'] = np.random.normal(100, 15, n_samples)     # nm/min\n",
                "    data['selectivity'] = np.random.normal(20, 3, n_samples)    # ratio\n",
                "    data['uniformity'] = np.random.normal(2, 0.5, n_samples)    # % 1-sigma\n",
                "    data['particles'] = np.random.poisson(5, n_samples)         # count/wafer\n",
                "    data['endpoint_time'] = np.random.normal(45, 8, n_samples)  # seconds\n",
                "    data['gas_pressure'] = np.random.normal(15, 2, n_samples)   # mTorr\n",
                "    data['plasma_power'] = np.random.normal(800, 50, n_samples) # Watts\n",
                "    data['wafer_temp'] = np.random.normal(25, 5, n_samples)     # Celsius\n",
                "    \n",
                "    X = pd.DataFrame(data)\n",
                "    \n",
                "    # Define defect probabilities based on parameter combinations\n",
                "    # 0: No defect, 1: Micro-trenching, 2: Sidewall damage, 3: Incomplete etch\n",
                "    \n",
                "    defect_prob = np.zeros((n_samples, 4))\n",
                "    \n",
                "    # Base probabilities\n",
                "    defect_prob[:, 0] = 0.7  # No defect (most common)\n",
                "    \n",
                "    # Micro-trenching (high etch rate + low selectivity)\n",
                "    micro_trench_risk = ((X['etch_rate'] > 110) & (X['selectivity'] < 18)).astype(float)\n",
                "    defect_prob[:, 1] = 0.05 + 0.25 * micro_trench_risk\n",
                "    \n",
                "    # Sidewall damage (high power + particles)\n",
                "    sidewall_risk = ((X['plasma_power'] > 850) | (X['particles'] > 7)).astype(float)\n",
                "    defect_prob[:, 2] = 0.05 + 0.2 * sidewall_risk\n",
                "    \n",
                "    # Incomplete etch (low etch rate + high pressure)\n",
                "    incomplete_risk = ((X['etch_rate'] < 90) & (X['gas_pressure'] > 16)).astype(float)\n",
                "    defect_prob[:, 3] = 0.03 + 0.15 * incomplete_risk\n",
                "    \n",
                "    # Normalize probabilities\n",
                "    defect_prob = defect_prob / defect_prob.sum(axis=1, keepdims=True)\n",
                "    \n",
                "    # Sample defect categories\n",
                "    y = np.array([np.random.choice(4, p=p) for p in defect_prob])\n",
                "    \n",
                "    return X, y\n",
                "\n",
                "# Generate both datasets\n",
                "print(\"Generating synthetic semiconductor datasets...\")\n",
                "\n",
                "# Regression dataset (yield prediction)\n",
                "X_yield, y_yield = generate_semiconductor_yield_data(n_samples=1500, n_features=10)\n",
                "print(f\"✓ Yield dataset: {X_yield.shape[0]} samples, {X_yield.shape[1]} features\")\n",
                "print(f\"  Yield range: {y_yield.min():.1f}% - {y_yield.max():.1f}%\")\n",
                "\n",
                "# Classification dataset (defect detection)\n",
                "X_defects, y_defects = generate_semiconductor_defect_data(n_samples=1200, n_features=8)\n",
                "defect_names = ['No Defect', 'Micro-trenching', 'Sidewall Damage', 'Incomplete Etch']\n",
                "print(f\"✓ Defect dataset: {X_defects.shape[0]} samples, {X_defects.shape[1]} features\")\n",
                "print(f\"  Defect distribution:\")\n",
                "for i, name in enumerate(defect_names):\n",
                "    count = np.sum(y_defects == i)\n",
                "    print(f\"    {name}: {count} ({count/len(y_defects)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "visualization-header",
            "metadata": {},
            "source": [
                "## 2. Neural Network Theory Visualization\n",
                "\n",
                "Let's visualize key concepts in neural networks to build intuition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "theory-viz",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize activation functions\n",
                "x = np.linspace(-5, 5, 1000)\n",
                "\n",
                "# Define activation functions\n",
                "relu = np.maximum(0, x)\n",
                "sigmoid = 1 / (1 + np.exp(-x))\n",
                "tanh = np.tanh(x)\n",
                "leaky_relu = np.where(x > 0, x, 0.01 * x)\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "fig.suptitle('Activation Functions in Neural Networks', fontsize=16)\n",
                "\n",
                "axes[0,0].plot(x, relu, 'b-', linewidth=2)\n",
                "axes[0,0].set_title('ReLU: f(x) = max(0, x)')\n",
                "axes[0,0].grid(True, alpha=0.3)\n",
                "axes[0,0].set_ylabel('Output')\n",
                "\n",
                "axes[0,1].plot(x, sigmoid, 'r-', linewidth=2)\n",
                "axes[0,1].set_title('Sigmoid: f(x) = 1/(1+e^(-x))')\n",
                "axes[0,1].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1,0].plot(x, tanh, 'g-', linewidth=2)\n",
                "axes[1,0].set_title('Tanh: f(x) = tanh(x)')\n",
                "axes[1,0].grid(True, alpha=0.3)\n",
                "axes[1,0].set_xlabel('Input')\n",
                "axes[1,0].set_ylabel('Output')\n",
                "\n",
                "axes[1,1].plot(x, leaky_relu, 'm-', linewidth=2)\n",
                "axes[1,1].set_title('Leaky ReLU: f(x) = max(0.01x, x)')\n",
                "axes[1,1].grid(True, alpha=0.3)\n",
                "axes[1,1].set_xlabel('Input')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nActivation Function Properties:\")\n",
                "print(\"• ReLU: Fast, prevents vanishing gradients, can cause dead neurons\")\n",
                "print(\"• Sigmoid: Smooth, bounded [0,1], suffers from vanishing gradients\")\n",
                "print(\"• Tanh: Zero-centered [-1,1], faster convergence than sigmoid\")\n",
                "print(\"• Leaky ReLU: Prevents dead neurons, small gradient for negative inputs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loss-landscape",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize loss landscapes and optimization\n",
                "def create_loss_landscape():\n",
                "    \"\"\"Create a 2D loss landscape for visualization\"\"\"\n",
                "    x = np.linspace(-3, 3, 100)\n",
                "    y = np.linspace(-3, 3, 100)\n",
                "    X, Y = np.meshgrid(x, y)\n",
                "    \n",
                "    # Complex loss function with multiple local minima\n",
                "    Z = (X**2 + Y**2) + 0.5 * np.sin(3*X) * np.cos(3*Y) + 0.1 * (X**4 + Y**4)\n",
                "    \n",
                "    return X, Y, Z\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
                "\n",
                "# Loss landscape\n",
                "X, Y, Z = create_loss_landscape()\n",
                "contour = axes[0].contour(X, Y, Z, levels=20, cmap='viridis')\n",
                "axes[0].set_title('Neural Network Loss Landscape')\n",
                "axes[0].set_xlabel('Weight 1')\n",
                "axes[0].set_ylabel('Weight 2')\n",
                "plt.colorbar(contour, ax=axes[0], label='Loss')\n",
                "\n",
                "# Simulate different optimization paths\n",
                "# SGD path (more zigzag)\n",
                "sgd_path_x = np.array([2.5, 2.3, 2.0, 1.8, 1.5, 1.2, 0.9, 0.6, 0.3, 0.1, 0.0])\n",
                "sgd_path_y = np.array([2.2, 1.9, 1.8, 1.4, 1.2, 0.9, 0.7, 0.5, 0.3, 0.1, 0.0])\n",
                "\n",
                "# Adam path (more direct)\n",
                "adam_path_x = np.array([2.5, 2.1, 1.6, 1.2, 0.8, 0.4, 0.1, 0.0])\n",
                "adam_path_y = np.array([2.2, 1.8, 1.4, 1.0, 0.6, 0.3, 0.1, 0.0])\n",
                "\n",
                "axes[0].plot(sgd_path_x, sgd_path_y, 'ro-', alpha=0.7, label='SGD', markersize=4)\n",
                "axes[0].plot(adam_path_x, adam_path_y, 'bo-', alpha=0.7, label='Adam', markersize=4)\n",
                "axes[0].plot(0, 0, 'g*', markersize=15, label='Global Minimum')\n",
                "axes[0].legend()\n",
                "\n",
                "# Learning curves comparison\n",
                "epochs = np.arange(1, 101)\n",
                "sgd_loss = 5 * np.exp(-0.03 * epochs) + 0.1 * np.sin(0.2 * epochs) + 0.2\n",
                "adam_loss = 5 * np.exp(-0.05 * epochs) + 0.05 * np.sin(0.1 * epochs) + 0.1\n",
                "momentum_loss = 5 * np.exp(-0.04 * epochs) + 0.08 * np.sin(0.15 * epochs) + 0.15\n",
                "\n",
                "axes[1].plot(epochs, sgd_loss, 'r-', label='SGD', linewidth=2)\n",
                "axes[1].plot(epochs, adam_loss, 'b-', label='Adam', linewidth=2)\n",
                "axes[1].plot(epochs, momentum_loss, 'g-', label='SGD + Momentum', linewidth=2)\n",
                "axes[1].set_title('Optimizer Convergence Comparison')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Training Loss')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "axes[1].set_yscale('log')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nOptimizer Characteristics:\")\n",
                "print(\"• SGD: Simple, can oscillate, sensitive to learning rate\")\n",
                "print(\"• Adam: Adaptive learning rates, fast convergence, good default choice\")\n",
                "print(\"• SGD + Momentum: Better than plain SGD, helps escape local minima\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pytorch-header",
            "metadata": {},
            "source": [
                "## 3. Simple MLP Implementation (PyTorch)\n",
                "\n",
                "Let's implement a multi-layer perceptron from scratch using PyTorch for yield prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pytorch-mlp",
            "metadata": {},
            "outputs": [],
            "source": [
                "if HAS_TORCH:\n",
                "    class MLPRegressor(nn.Module):\n",
                "        def __init__(self, input_dim, hidden_dims=[64, 32], dropout=0.3):\n",
                "            super(MLPRegressor, self).__init__()\n",
                "            \n",
                "            self.input_dim = input_dim\n",
                "            self.hidden_dims = hidden_dims\n",
                "            self.dropout = dropout\n",
                "            \n",
                "            # Build network layers\n",
                "            layers = []\n",
                "            prev_dim = input_dim\n",
                "            \n",
                "            for hidden_dim in hidden_dims:\n",
                "                layers.extend([\n",
                "                    nn.Linear(prev_dim, hidden_dim),\n",
                "                    nn.ReLU(),\n",
                "                    nn.Dropout(dropout)\n",
                "                ])\n",
                "                prev_dim = hidden_dim\n",
                "            \n",
                "            # Output layer (no activation for regression)\n",
                "            layers.append(nn.Linear(prev_dim, 1))\n",
                "            \n",
                "            self.network = nn.Sequential(*layers)\n",
                "            \n",
                "            # Initialize weights\n",
                "            self.apply(self._init_weights)\n",
                "        \n",
                "        def _init_weights(self, module):\n",
                "            if isinstance(module, nn.Linear):\n",
                "                nn.init.xavier_uniform_(module.weight)\n",
                "                nn.init.constant_(module.bias, 0)\n",
                "        \n",
                "        def forward(self, x):\n",
                "            return self.network(x).squeeze()\n",
                "        \n",
                "        def predict(self, x):\n",
                "            self.eval()\n",
                "            with torch.no_grad():\n",
                "                if isinstance(x, np.ndarray):\n",
                "                    x = torch.tensor(x, dtype=torch.float32)\n",
                "                return self.forward(x).numpy()\n",
                "    \n",
                "    # Training function\n",
                "    def train_pytorch_model(model, X_train, y_train, X_val, y_val, \n",
                "                           num_epochs=100, learning_rate=0.001, batch_size=32):\n",
                "        # Convert to PyTorch tensors\n",
                "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
                "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
                "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
                "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
                "        \n",
                "        # Create data loaders\n",
                "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
                "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
                "        \n",
                "        # Setup optimizer and loss\n",
                "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
                "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
                "        criterion = nn.MSELoss()\n",
                "        \n",
                "        # Training history\n",
                "        history = {'train_loss': [], 'val_loss': []}\n",
                "        \n",
                "        # Training loop\n",
                "        for epoch in range(num_epochs):\n",
                "            # Training phase\n",
                "            model.train()\n",
                "            train_loss = 0.0\n",
                "            \n",
                "            for batch_X, batch_y in train_loader:\n",
                "                optimizer.zero_grad()\n",
                "                outputs = model(batch_X)\n",
                "                loss = criterion(outputs, batch_y)\n",
                "                loss.backward()\n",
                "                \n",
                "                # Gradient clipping to prevent exploding gradients\n",
                "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "                \n",
                "                optimizer.step()\n",
                "                train_loss += loss.item()\n",
                "            \n",
                "            # Validation phase\n",
                "            model.eval()\n",
                "            with torch.no_grad():\n",
                "                val_outputs = model(X_val_tensor)\n",
                "                val_loss = criterion(val_outputs, y_val_tensor).item()\n",
                "            \n",
                "            # Update learning rate\n",
                "            scheduler.step(val_loss)\n",
                "            \n",
                "            # Record history\n",
                "            avg_train_loss = train_loss / len(train_loader)\n",
                "            history['train_loss'].append(avg_train_loss)\n",
                "            history['val_loss'].append(val_loss)\n",
                "            \n",
                "            # Print progress\n",
                "            if (epoch + 1) % 20 == 0:\n",
                "                print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
                "                      f'Train Loss: {avg_train_loss:.4f}, '\n",
                "                      f'Val Loss: {val_loss:.4f}, '\n",
                "                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
                "        \n",
                "        return history\n",
                "    \n",
                "    # Prepare data\n",
                "    scaler = StandardScaler()\n",
                "    X_yield_scaled = scaler.fit_transform(X_yield)\n",
                "    \n",
                "    # Train-validation split\n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_yield_scaled, y_yield, test_size=0.2, random_state=RANDOM_SEED\n",
                "    )\n",
                "    \n",
                "    print(f\"Training PyTorch MLP on yield data...\")\n",
                "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
                "    \n",
                "    # Create and train model\n",
                "    pytorch_model = MLPRegressor(input_dim=X_train.shape[1], \n",
                "                                hidden_dims=[128, 64, 32], \n",
                "                                dropout=0.3)\n",
                "    \n",
                "    print(f\"\\nModel architecture:\")\n",
                "    print(pytorch_model)\n",
                "    \n",
                "    # Train the model\n",
                "    history = train_pytorch_model(pytorch_model, X_train, y_train, X_val, y_val,\n",
                "                                 num_epochs=100, learning_rate=0.001, batch_size=32)\n",
                "    \n",
                "    print(\"\\n✓ PyTorch training completed!\")\n",
                "    \n",
                "else:\n",
                "    print(\"⚠ PyTorch not available - skipping PyTorch implementation\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}