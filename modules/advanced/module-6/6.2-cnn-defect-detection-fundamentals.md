# Module 6.2 â€“ CNN Fundamentals for Wafer Map Defect Detection

## 1. Introduction: Spatial Pattern Recognition in Manufacturing

In semiconductor manufacturing, wafer maps represent the spatial distribution of die (chip) pass/fail results across a circular silicon wafer. Unlike traditional tabular data, wafer maps contain rich spatial information that reveals systematic manufacturing issues, equipment problems, and process variations.

Convolutional Neural Networks (CNNs) excel at this type of spatial pattern recognition, automatically learning hierarchical feature representations from raw pixel data without manual feature engineering.

## 2. Common Wafer Defect Patterns

### 2.1 Systematic Patterns
- **Center defects**: Circular patterns in the wafer center, often due to spin-coating issues
- **Edge ring**: Ring-shaped failures at wafer periphery, typically from edge bead removal
- **Scratch**: Linear defects from mechanical handling or transport
- **Donut**: Ring patterns from non-uniform processing
- **Random**: Scattered failures with no clear spatial pattern

### 2.2 Manufacturing Root Causes
| Pattern | Likely Cause | Process Impact |
|---------|--------------|----------------|
| Center | Spin coating uniformity | Thickness variation |
| Edge ring | Edge bead removal, clamping | Edge exclusion zone |
| Scratch | Wafer handling, transport | Physical damage |
| Donut | Temperature gradient, gas flow | Radial non-uniformity |
| Loc | Localized contamination | Particle defects |

## 3. CNN Architecture for Defect Detection

### 3.1 Input Representation
```python
# Wafer map preprocessing
def preprocess_wafer_map(wafer_map, target_size=(64, 64)):
    """
    Convert raw wafer map to CNN input format
    
    Args:
        wafer_map: 2D array with die pass/fail status
        target_size: Resize target for consistent input
    
    Returns:
        Normalized 3D tensor (H, W, C)
    """
    # Resize to standard dimensions
    resized = resize(wafer_map, target_size)
    
    # Normalize to [0, 1] range
    normalized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)
    
    # Add channel dimension for single-channel input
    return normalized.reshape(*target_size, 1)
```

### 3.2 Network Architecture Guidelines

**Small Dataset Architecture (< 10K samples):**
```python
def build_lightweight_cnn(input_shape=(64, 64, 1), num_classes=5):
    """
    Lightweight CNN for small wafer map datasets
    Designed to avoid overfitting with limited data
    """
    model = Sequential([
        Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D(2, 2),
        Conv2D(32, (3, 3), activation='relu'),
        MaxPooling2D(2, 2),
        Conv2D(64, (3, 3), activation='relu'),
        GlobalAveragePooling2D(),  # Reduces overfitting vs Flatten
        Dropout(0.5),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    return model
```

**Transfer Learning Approach:**
- Use pre-trained networks (ResNet, VGG) when available
- Fine-tune final layers for wafer-specific patterns
- Requires careful learning rate scheduling

### 3.3 Data Augmentation Strategies

```python
def wafer_augmentations():
    """
    Wafer-appropriate data augmentation
    Preserves spatial relationship integrity
    """
    return ImageDataGenerator(
        rotation_range=15,      # Slight rotation (wafer orientation varies)
        width_shift_range=0.1,  # Small translations
        height_shift_range=0.1,
        zoom_range=0.1,         # Scale variations
        horizontal_flip=True,   # Mirror symmetry often valid
        vertical_flip=True,
        fill_mode='constant',   # Pad with background value
        cval=0                  # Use 0 for background
    )
```

## 4. Class Imbalance Handling

Wafer defect datasets are heavily imbalanced:
- Normal wafers: 70-90%
- Specific defect patterns: 1-5% each

### 4.1 Sampling Strategies
```python
# Weighted sampling for training
class_weights = {
    0: 1.0,      # Normal
    1: 15.0,     # Center
    2: 20.0,     # Edge ring
    3: 25.0,     # Scratch
    4: 30.0      # Donut
}

# Or use sklearn's compute_class_weight
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(
    'balanced', 
    classes=np.unique(y), 
    y=y
)
```

### 4.2 Loss Function Adaptations
- **Focal Loss**: Emphasizes hard-to-classify samples
- **Weighted CrossEntropy**: Apply class weights directly
- **Label Smoothing**: Reduces overconfidence on sparse classes

## 5. Evaluation Metrics for Manufacturing

### 5.1 Standard Classification Metrics
- **ROC-AUC**: Overall discriminative ability
- **PR-AUC**: Performance on positive classes (defects)
- **F1-score**: Balance precision/recall per class
- **Confusion Matrix**: Detailed error analysis

### 5.2 Manufacturing-Specific Metrics

**Prediction Within Specification (PWS):**
```python
def compute_pws(y_true, y_pred, tolerance=0.1):
    """
    Percentage of predictions within acceptable tolerance
    For manufacturing, 'close enough' predictions may be acceptable
    """
    # Convert to probabilities if needed
    if y_pred.ndim > 1:
        y_pred_class = np.argmax(y_pred, axis=1)
    else:
        y_pred_class = y_pred
    
    correct = (y_true == y_pred_class)
    return np.mean(correct) * 100
```

**Estimated Loss (Misclassification Cost):**
```python
def compute_estimated_loss(y_true, y_pred, cost_matrix):
    """
    Calculate business impact of misclassifications
    
    Args:
        cost_matrix: [true_class][pred_class] = cost
    """
    total_cost = 0
    for true_class in range(len(cost_matrix)):
        for pred_class in range(len(cost_matrix[0])):
            mask = (y_true == true_class) & (y_pred == pred_class)
            count = np.sum(mask)
            total_cost += count * cost_matrix[true_class][pred_class]
    
    return total_cost / len(y_true)
```

## 6. Model Interpretability and Explainability

### 6.1 Grad-CAM (Gradient-weighted Class Activation Mapping)
```python
def generate_gradcam(model, img, class_idx, last_conv_layer_name):
    """
    Generate Grad-CAM heatmap showing which regions 
    influence the prediction most
    """
    # Create model that maps inputs to activations + predictions
    grad_model = tf.keras.models.Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )
    
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img)
        loss = predictions[:, class_idx]
    
    # Gradient of class output w.r.t. feature maps
    grads = tape.gradient(loss, conv_outputs)
    
    # Global average pooling of gradients
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    
    # Weight feature maps by gradients
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    
    # Normalize and apply ReLU
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
```

### 6.2 Saliency Maps
Simple gradient-based visualization showing pixel importance:
```python
def compute_saliency(model, img, class_idx):
    """Generate saliency map using input gradients"""
    img_tensor = tf.Variable(img)
    with tf.GradientTape() as tape:
        predictions = model(img_tensor)
        loss = predictions[0][class_idx]
    
    gradients = tape.gradient(loss, img_tensor)
    return tf.math.abs(gradients).numpy()
```

## 7. Production Pipeline Considerations

### 7.1 Model Versioning and Persistence
```python
@dataclass
class CNNMetadata:
    model_type: str
    input_shape: Tuple[int, int, int]
    num_classes: int
    class_names: List[str]
    training_params: Dict[str, Any]
    performance_metrics: Dict[str, float]
    trained_at: str
    pytorch_version: Optional[str] = None
    
class CNNDefectPipeline:
    def save(self, path: Path):
        """Save model weights + metadata"""
        model_path = path.with_suffix('.pth')
        metadata_path = path.with_suffix('.json')
        
        # Save PyTorch model
        if hasattr(self, 'model'):
            torch.save(self.model.state_dict(), model_path)
        
        # Save metadata
        with open(metadata_path, 'w') as f:
            json.dump(asdict(self.metadata), f, indent=2)
```

### 7.2 Inference Optimization
- **Batch Prediction**: Process multiple wafer maps efficiently
- **Model Quantization**: Reduce model size for edge deployment
- **ONNX Export**: Cross-platform inference

### 7.3 Monitoring and Drift Detection
- **Feature Drift**: Monitor input distribution changes
- **Prediction Drift**: Track output confidence distributions
- **Performance Monitoring**: Track metrics over time

## 8. Common Pitfalls and Solutions

### 8.1 Data Leakage Prevention
- **Wafer-level Splits**: Ensure no wafer appears in both train/test
- **Temporal Ordering**: Respect time-based splits for deployment
- **Cross-contamination**: Avoid augmentations that mix classes

### 8.2 Overfitting Mitigation
- **Regularization**: Dropout, weight decay, batch normalization
- **Early Stopping**: Monitor validation metrics
- **Data Augmentation**: Synthetic sample generation
- **Architecture Size**: Start small, grow as needed

### 8.3 Class Imbalance Strategies
- **Stratified Sampling**: Maintain class ratios in splits
- **Focal Loss**: Address easy/hard sample imbalance
- **Ensemble Methods**: Combine multiple models

## 9. Synthetic Data Generation

For learning and testing purposes, generate synthetic wafer maps:

```python
def generate_synthetic_wafer_map(pattern='center', size=64, noise_level=0.1):
    """
    Generate synthetic wafer map with known defect pattern
    
    Args:
        pattern: 'center', 'edge', 'scratch', 'donut', 'random'
        size: Image dimensions (size x size)
        noise_level: Random noise to add realism
    """
    wafer = np.ones((size, size))  # Start with good dies
    center = size // 2
    
    if pattern == 'center':
        # Central circular defect
        y, x = np.ogrid[:size, :size]
        mask = (x - center)**2 + (y - center)**2 <= (size/6)**2
        wafer[mask] = 0
        
    elif pattern == 'edge':
        # Edge ring defect
        y, x = np.ogrid[:size, :size]
        dist = np.sqrt((x - center)**2 + (y - center)**2)
        mask = (dist >= size/2.2) & (dist <= size/2)
        wafer[mask] = 0
        
    elif pattern == 'scratch':
        # Linear scratch
        scratch_width = 2
        start_row = np.random.randint(0, size//4)
        end_row = size - np.random.randint(0, size//4)
        scratch_col = center + np.random.randint(-10, 10)
        wafer[start_row:end_row, 
              scratch_col:scratch_col+scratch_width] = 0
    
    # Add noise
    noise = np.random.random((size, size)) < noise_level
    wafer[noise] = 1 - wafer[noise]
    
    return wafer
```

## 10. Integration with Manufacturing Systems

### 10.1 Real-time Inference
- **Streaming Pipeline**: Process wafer maps as they arrive
- **Batch Processing**: Handle multiple wafers efficiently
- **API Integration**: REST/gRPC endpoints for system integration

### 10.2 Feedback Loops
- **Human-in-the-loop**: Expert review of uncertain predictions
- **Active Learning**: Identify samples for additional labeling
- **Model Retraining**: Periodic updates with new data

## 11. Future Extensions

- **Multi-scale Analysis**: Combine wafer-level and die-level features
- **Temporal Modeling**: Track pattern evolution over time
- **Federated Learning**: Collaborate across multiple fabs
- **Generative Models**: Synthetic defect pattern generation
- **3D Spatial Analysis**: Incorporate thickness/height maps

## 12. References and Further Reading

- He, K. et al. "Deep Residual Learning for Image Recognition" (ResNet)
- Selvaraju, R. et al. "Grad-CAM: Visual Explanations from Deep Networks"
- Lin, T. et al. "Focal Loss for Dense Object Detection"
- Goodfellow, I. et al. "Deep Learning" (textbook)
- Industry papers on wafer map analysis and semiconductor manufacturing

---

This foundation prepares you to implement robust CNN pipelines for wafer defect detection, combining deep learning theory with practical manufacturing considerations.