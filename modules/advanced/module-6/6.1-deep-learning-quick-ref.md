# Module 6.1: Deep Learning Quick Reference

## CLI Commands

### Train Models
```bash
# Basic PyTorch training
python 6.1-deep-learning-pipeline.py train --dataset synthetic_yield --backend pytorch --save model.pth

# TensorFlow with custom architecture
python 6.1-deep-learning-pipeline.py train --dataset synthetic_defects --backend tensorflow --task classification --hidden-layers 128 64 32 --epochs 50

# Advanced training with regularization
python 6.1-deep-learning-pipeline.py train --dataset synthetic_yield --backend pytorch --learning-rate 0.001 --dropout 0.4 --weight-decay 1e-4 --batch-size 64
```

### Evaluate Models
```bash
# Basic evaluation
python 6.1-deep-learning-pipeline.py evaluate --model-path model.pth --dataset synthetic_yield

# Cross-validation evaluation
python 6.1-deep-learning-pipeline.py evaluate --model-path model.joblib --dataset synthetic_defects --cv-folds 5
```

### Make Predictions
```bash
# Single prediction
python 6.1-deep-learning-pipeline.py predict --model-path model.pth --input-json '{"temperature":455, "pressure":2.6, "flow_rate":120}'

# Batch predictions from file
python 6.1-deep-learning-pipeline.py predict --model-path model.pth --input-csv process_data.csv --output predictions.json
```

## Core Architecture Patterns

### Multi-Layer Perceptron (MLP)
```python
# PyTorch implementation
import torch.nn as nn

class MLPRegressor(nn.Module):
    def __init__(self, input_dim=10, hidden_dims=[64, 32], dropout=0.3):
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
            
        layers.append(nn.Linear(prev_dim, 1))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

# TensorFlow implementation
from tensorflow.keras import Sequential, layers

def build_mlp(input_dim=10, hidden_dims=[64, 32], dropout=0.3):
    model = Sequential([layers.Input(shape=(input_dim,))])
    
    for hidden_dim in hidden_dims:
        model.add(layers.Dense(hidden_dim, activation='relu'))
        model.add(layers.Dropout(dropout))
    
    model.add(layers.Dense(1, activation='linear'))
    return model
```

## Optimization Algorithms

| Optimizer | Use Case | Learning Rate | Pros | Cons |
|-----------|----------|---------------|------|------|
| **SGD** | Simple problems | 0.01-0.1 | Memory efficient, stable | Slow convergence |
| **Adam** | Default choice | 0.001-0.01 | Fast, adaptive | Can overfit |
| **AdamW** | Large models | 0.0001-0.001 | Better regularization | More hyperparameters |
| **RMSprop** | RNNs/sequences | 0.001-0.01 | Good for sparse data | Can be unstable |

```python
# PyTorch optimizers
import torch.optim as optim

# Adam (recommended)
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# SGD with momentum
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)

# AdamW for better regularization
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```

## Regularization Quick Guide

### Dropout Rates by Layer Type
- **Input Layer**: 0.1-0.2 (light regularization)
- **Hidden Layers**: 0.3-0.5 (moderate to strong)
- **Output Layer**: Never use dropout

### Weight Decay (L2) Values
- **Small datasets (< 1K)**: 1e-3 to 1e-2
- **Medium datasets (1K-10K)**: 1e-4 to 1e-3
- **Large datasets (> 10K)**: 1e-5 to 1e-4

### Batch Normalization
```python
# PyTorch
layers.extend([
    nn.Linear(prev_dim, hidden_dim),
    nn.BatchNorm1d(hidden_dim),
    nn.ReLU(),
    nn.Dropout(dropout)
])

# TensorFlow
model.add(layers.Dense(hidden_dim))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dropout(dropout))
```

## Activation Functions

| Function | Formula | Range | Use Case |
|----------|---------|-------|----------|
| **ReLU** | max(0, x) | [0, ∞) | Hidden layers (default) |
| **Sigmoid** | 1/(1+e^(-x)) | (0, 1) | Binary classification output |
| **Tanh** | (e^x-e^(-x))/(e^x+e^(-x)) | (-1, 1) | Hidden layers (zero-centered) |
| **Softmax** | e^xi/∑e^xj | (0, 1), ∑=1 | Multi-class classification |
| **Linear** | x | (-∞, ∞) | Regression output |

## Loss Functions

### Regression Tasks
```python
# Mean Squared Error (default)
loss_fn = nn.MSELoss()

# Mean Absolute Error (robust to outliers)
loss_fn = nn.L1Loss()

# Huber Loss (combines MSE + MAE)
loss_fn = nn.SmoothL1Loss()

# Custom specification-aware loss
def spec_loss(pred, target, lsl=-3, usl=3, penalty=2.0):
    mse = (pred - target).pow(2)
    out_of_spec = ((pred < lsl) | (pred > usl)).float()
    return mse.mean() + penalty * (out_of_spec * mse).mean()
```

### Classification Tasks
```python
# Binary classification
loss_fn = nn.BCEWithLogitsLoss()

# Multi-class classification
loss_fn = nn.CrossEntropyLoss()

# Weighted classification (imbalanced classes)
weights = torch.tensor([1.0, 5.0])  # Weight rare class higher
loss_fn = nn.CrossEntropyLoss(weight=weights)
```

## Learning Rate Scheduling

```python
# Step decay
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# Exponential decay
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

# Reduce on plateau (recommended)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

# Cosine annealing
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
```

## Manufacturing Metrics

### Regression Metrics
```python
# Standard metrics
mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2 = r2_score(y_true, y_pred)

# Manufacturing-specific
def prediction_within_spec(y_true, y_pred, lsl, usl):
    in_spec = ((y_pred >= lsl) & (y_pred <= usl)).sum()
    return in_spec / len(y_pred)

def estimated_loss(y_true, y_pred, cost_per_unit=100, tolerance=0.1):
    errors = np.abs(y_true - y_pred)
    excessive_errors = np.maximum(0, errors - tolerance)
    return (excessive_errors * cost_per_unit).sum()
```

### Classification Metrics
```python
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score

# Multi-class metrics
report = classification_report(y_true, y_pred)
accuracy = accuracy_score(y_true, y_pred)

# Binary classification
roc_auc = roc_auc_score(y_true, y_pred_prob)
pr_auc = average_precision_score(y_true, y_pred_prob)
```

## Debugging Guide

### Common Issues and Solutions

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Exploding Gradients** | Loss becomes NaN | Lower learning rate, gradient clipping |
| **Vanishing Gradients** | Loss plateaus early | Use ReLU, batch norm, residual connections |
| **Overfitting** | Val loss > train loss | Add dropout, reduce model size, more data |
| **Underfitting** | Both losses high | Increase model capacity, reduce regularization |
| **Slow Convergence** | Loss decreases slowly | Increase learning rate, batch normalization |

### Gradient Clipping
```python
# PyTorch
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# TensorFlow
optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)
```

### Early Stopping
```python
# PyTorch implementation
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience
```

## Data Preprocessing

### Feature Scaling
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Standardization (recommended for MLPs)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# Min-Max scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_train)
```

### Train/Validation Split
```python
from sklearn.model_selection import train_test_split

# Random split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Time series split (for temporal data)
split_idx = int(0.8 * len(X))
X_train, X_val = X[:split_idx], X[split_idx:]
y_train, y_val = y[:split_idx], y[split_idx:]
```

## Model Selection Checklist

### Architecture Selection
- [ ] Start with 1-2 hidden layers
- [ ] Use 64-128 neurons per layer for tabular data
- [ ] Add layers only if underfitting
- [ ] Use dropout (0.3-0.5) to prevent overfitting

### Hyperparameter Tuning
- [ ] Learning rate: Start with 0.001
- [ ] Batch size: 32-128 (power of 2)
- [ ] Epochs: 50-200 with early stopping
- [ ] Weight decay: 1e-4 for regularization

### Training Process
- [ ] Use Adam optimizer as default
- [ ] Monitor both training and validation loss
- [ ] Implement early stopping (patience=10-20)
- [ ] Save best model based on validation metric

### Evaluation
- [ ] Use appropriate metrics for task (MAE/RMSE for regression)
- [ ] Include manufacturing-specific metrics (PWS, estimated loss)
- [ ] Perform cross-validation for robust estimates
- [ ] Test on truly held-out data

## Integration Patterns

### Scikit-learn Integration
```python
from sklearn.base import BaseEstimator, RegressorMixin

class MLPRegressor(BaseEstimator, RegressorMixin):
    def __init__(self, hidden_dims=[64, 32], learning_rate=0.001):
        self.hidden_dims = hidden_dims
        self.learning_rate = learning_rate
    
    def fit(self, X, y):
        # Initialize and train model
        self.model_ = self._build_model(X.shape[1])
        # Training logic here
        return self
    
    def predict(self, X):
        return self.model_(torch.tensor(X, dtype=torch.float32)).detach().numpy()
```

### Pipeline Integration
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Complete preprocessing + modeling pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('mlp', MLPRegressor(hidden_dims=[128, 64, 32]))
])

pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

## Common Error Messages

| Error | Cause | Fix |
|-------|-------|-----|
| `RuntimeError: size mismatch` | Wrong input dimensions | Check `input_dim` parameter |
| `RuntimeError: grad can be implicitly created only for scalar outputs` | Multi-output loss | Use `loss.backward()` for scalars only |
| `ValueError: Target size mismatch` | Wrong number of outputs | Adjust final layer size |
| `CUDA out of memory` | Batch size too large | Reduce batch size |

---

*Quick reference for Module 6.1 Deep Learning implementation. See fundamentals document for theory and notebook for hands-on examples.*