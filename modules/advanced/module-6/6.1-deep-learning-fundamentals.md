# Module 6.1: Deep Learning Fundamentals for Semiconductor Manufacturing

## üéØ Learning Objectives

After completing this module, you will:

- Understand neural network theory and its applications in semiconductor manufacturing
- Master multi-layer perceptron (MLP) architectures for tabular manufacturing data
- Apply backpropagation and gradient descent optimization to process parameter modeling
- Implement regularization techniques to prevent overfitting in manufacturing datasets
- Select appropriate architectures for regression vs classification tasks in semiconductor contexts
- Compare PyTorch vs TensorFlow implementations for production deployment

## üìñ Introduction to Deep Learning in Manufacturing

Deep learning extends traditional machine learning by automatically learning hierarchical feature representations from raw data. In semiconductor manufacturing, deep learning excels at:

- **Complex Parameter Interactions**: Modeling non-linear relationships between process parameters that traditional linear models miss
- **High-Dimensional Data**: Processing hundreds of sensor readings, metrology measurements, and control parameters simultaneously
- **Temporal Patterns**: Learning time-series patterns in process data and equipment sensor streams
- **Yield Prediction**: Predicting final device performance from early-stage process measurements
- **Anomaly Detection**: Identifying unusual equipment behavior or process excursions

## üßÆ Neural Network Mathematics

### Perceptron Foundation

A single perceptron performs linear classification:

```
y = œÉ(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)
```

Where:
- **x‚ÇÅ, x‚ÇÇ, ..., x‚Çô**: Input features (temperature, pressure, flow rate, etc.)
- **w‚ÇÅ, w‚ÇÇ, ..., w‚Çô**: Learned weights
- **b**: Bias term
- **œÉ**: Activation function (sigmoid, ReLU, tanh)

### Multi-Layer Perceptron (MLP)

MLPs stack multiple layers to learn non-linear patterns:

```
Layer 1: h‚ÇÅ = œÉ(W‚ÇÅx + b‚ÇÅ)
Layer 2: h‚ÇÇ = œÉ(W‚ÇÇh‚ÇÅ + b‚ÇÇ)
...
Output: y = œÉ(W‚Çí‚Çòh‚Çó‚Çã‚ÇÅ + b‚Çí·µ§‚Çú)
```

### Universal Approximation Theorem

Any continuous function can be approximated arbitrarily well by a feedforward network with:
- A single hidden layer
- Sufficient hidden units
- Non-linear activation functions

This theoretical foundation justifies why MLPs can model complex semiconductor process relationships.

### Activation Functions

**ReLU (Rectified Linear Unit):**
```
f(x) = max(0, x)
```
- **Advantages**: Fast computation, mitigates vanishing gradients
- **Use Case**: Hidden layers in most semiconductor applications

**Sigmoid:**
```
f(x) = 1 / (1 + e^(-x))
```
- **Advantages**: Smooth, differentiable, bounded output [0,1]
- **Use Case**: Binary classification (pass/fail, defect/normal)

**Tanh:**
```
f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```
- **Advantages**: Zero-centered output [-1,1], faster convergence
- **Use Case**: Hidden layers when features are centered

## üéØ Backpropagation Algorithm

Backpropagation efficiently computes gradients for weight updates using the chain rule.

### Forward Pass

1. **Input Layer**: Accept process parameters (temperature, pressure, etc.)
2. **Hidden Layers**: Compute activations layer by layer
3. **Output Layer**: Produce predictions (yield, defect probability)

### Backward Pass

1. **Output Error**: Compute loss gradient with respect to output
2. **Hidden Layer Gradients**: Propagate error backward using chain rule
3. **Weight Updates**: Update weights using computed gradients

### Mathematical Formulation

For layer l:
```
Œ¥‚Çó = (W‚Çó‚Çä‚ÇÅ·µÄŒ¥‚Çó‚Çä‚ÇÅ) ‚äô œÉ'(z‚Çó)
‚àáW‚Çó = Œ¥‚Çóh ‚Çó‚Çã‚ÇÅ·µÄ
‚àáb‚Çó = Œ¥‚Çó
```

Where:
- **Œ¥‚Çó**: Error signal at layer l
- **‚äô**: Element-wise multiplication
- **œÉ'**: Derivative of activation function

## ‚öôÔ∏è Optimization Algorithms

### Stochastic Gradient Descent (SGD)

Basic weight update rule:
```
w ‚Üê w - Œ∑‚àáw
```

**Advantages**: Simple, memory efficient
**Disadvantages**: Can get stuck in local minima, sensitive to learning rate

### Adam Optimizer

Combines momentum and adaptive learning rates:
```
m = Œ≤‚ÇÅm + (1-Œ≤‚ÇÅ)‚àáw
v = Œ≤‚ÇÇv + (1-Œ≤‚ÇÇ)(‚àáw)¬≤
w ‚Üê w - Œ∑(mÃÇ/(‚àövÃÇ + Œµ))
```

**Advantages**: Fast convergence, robust to hyperparameters
**Use Case**: Default choice for most semiconductor applications

### Learning Rate Scheduling

**Step Decay**: Reduce learning rate at fixed intervals
```python
lr = initial_lr * decay_rate^(epoch // step_size)
```

**Exponential Decay**: Smooth reduction over time
```python
lr = initial_lr * exp(-decay_rate * epoch)
```

## üõ°Ô∏è Regularization Techniques

### Dropout

Randomly set neurons to zero during training:
```python
# During training
if training:
    mask = bernoulli(p=keep_prob)
    output = input * mask / keep_prob
```

**Benefits**: Prevents co-adaptation, reduces overfitting
**Semiconductor Application**: Critical for high-dimensional sensor data

### Batch Normalization

Normalize inputs to each layer:
```
BN(x) = Œ≥((x - Œº)/œÉ) + Œ≤
```

Where:
- **Œº, œÉ**: Batch mean and standard deviation
- **Œ≥, Œ≤**: Learnable parameters

**Benefits**: Faster training, reduces internal covariate shift
**Manufacturing Context**: Stabilizes training with varying process conditions

### Weight Decay (L2 Regularization)

Add penalty term to loss function:
```
Loss = MSE + Œª‚àë(w·µ¢¬≤)
```

**Benefits**: Prevents large weights, improves generalization
**Parameter Selection**: Œª ‚àà [1e-5, 1e-3] for semiconductor datasets

## üè≠ Semiconductor-Specific Applications

### Process Parameter Modeling

**Input Features**:
- Temperature profiles (ramp rates, hold times, peak temperatures)
- Pressure settings (chamber pressure, gas flow rates)
- Power levels (RF power, DC bias)
- Time parameters (step duration, cycle times)
- Equipment state (chamber conditioning, maintenance cycles)

**Target Variables**:
- **Regression**: Yield percentage, critical dimensions, resistance values
- **Classification**: Pass/fail, defect categories, bin sorting

### Architecture Guidelines

**Small Datasets (< 1K samples)**:
```python
model = Sequential([
    Dense(64, activation='relu', input_dim=n_features),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')  # Regression
])
```

**Medium Datasets (1K-10K samples)**:
```python
model = Sequential([
    Dense(128, activation='relu', input_dim=n_features),
    BatchNormalization(),
    Dropout(0.4),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(n_classes, activation='softmax')  # Classification
])
```

**Large Datasets (> 10K samples)**:
```python
model = Sequential([
    Dense(256, activation='relu', input_dim=n_features),
    BatchNormalization(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.4),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(output_dim, activation=output_activation)
])
```

## üìä Manufacturing-Specific Loss Functions

### Weighted Mean Squared Error

For imbalanced yield data:
```python
def weighted_mse(y_true, y_pred, weights):
    return np.mean(weights * (y_true - y_pred)**2)
```

### Custom Semiconductor Loss

Penalize predictions outside specification limits:
```python
def spec_aware_loss(y_true, y_pred, lsl, usl, penalty=10.0):
    mse = (y_true - y_pred)**2
    out_of_spec = ((y_pred < lsl) | (y_pred > usl)).float()
    return mse + penalty * out_of_spec * mse
```

## üîß PyTorch vs TensorFlow Comparison

### PyTorch Advantages

- **Dynamic Computation Graphs**: Better for research and experimentation
- **Pythonic Design**: More intuitive for Python developers
- **Debugging**: Easier to debug with standard Python tools
- **Research Adoption**: Preferred in academic and research settings

```python
import torch
import torch.nn as nn

class MLPRegressor(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_dim = hidden_dim
            
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)
```

### TensorFlow Advantages

- **Production Deployment**: TensorFlow Serving, TensorFlow Lite
- **Ecosystem**: TensorBoard, TFX for MLOps pipelines
- **Performance**: Better optimization for large-scale deployment
- **Industry Adoption**: Preferred in production environments

```python
import tensorflow as tf
from tensorflow.keras import Sequential, layers

def build_mlp_regressor(input_dim, hidden_dims, output_dim):
    model = Sequential()
    model.add(layers.Input(shape=(input_dim,)))
    
    for hidden_dim in hidden_dims:
        model.add(layers.Dense(hidden_dim, activation='relu'))
        model.add(layers.Dropout(0.3))
    
    model.add(layers.Dense(output_dim, activation='linear'))
    return model
```

## üìà Model Selection Guidelines

### For Tabular Semiconductor Data

**Use MLPs when**:
- Mixed data types (continuous + categorical)
- Non-linear parameter interactions suspected
- Need for interpretability is moderate
- Dataset size > 1000 samples

**Avoid MLPs when**:
- Linear relationships dominate
- Very small datasets (< 100 samples)
- High interpretability required
- Simple baseline models perform well

### Architecture Selection Framework

1. **Start Simple**: Begin with 1-2 hidden layers
2. **Increase Complexity**: Add layers/neurons if underfitting
3. **Add Regularization**: Use dropout/batch norm if overfitting
4. **Hyperparameter Tuning**: Grid search on learning rate, architecture
5. **Ensemble Methods**: Combine multiple models for production

## üéØ Best Practices Summary

### Data Preprocessing
- **Standardization**: Critical for neural networks (mean=0, std=1)
- **Feature Engineering**: Create interaction terms for process parameters
- **Missing Values**: Use domain knowledge for imputation strategies

### Training Process
- **Learning Rate**: Start with 0.001, use scheduling
- **Batch Size**: 32-128 for most semiconductor datasets
- **Early Stopping**: Monitor validation loss to prevent overfitting
- **Reproducibility**: Set random seeds for consistent results

### Model Evaluation
- **Cross-Validation**: Use time-series splits for temporal data
- **Manufacturing Metrics**: PWS, estimated loss, specification limits
- **Statistical Tests**: Significance testing for model comparisons

### Production Deployment
- **Model Versioning**: Track model performance over time
- **Monitoring**: Watch for distribution drift in input features
- **A/B Testing**: Gradually roll out new models
- **Fallback Strategy**: Keep simpler baseline models as backup

---

*This fundamentals guide provides the theoretical foundation for Module 6.1. Continue with the interactive notebook for hands-on implementation and the pipeline script for production deployment.*