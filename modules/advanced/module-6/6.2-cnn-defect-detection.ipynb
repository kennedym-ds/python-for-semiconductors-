{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 6.2 - CNN for Wafer Map Defect Detection\n",
        "\n",
        "This notebook demonstrates convolutional neural networks for semiconductor wafer map defect classification. We'll work with synthetic wafer maps that simulate real manufacturing patterns.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand spatial pattern recognition in wafer maps\n",
        "- Implement CNN architectures for defect classification\n",
        "- Handle class imbalance in manufacturing data\n",
        "- Apply manufacturing-specific evaluation metrics\n",
        "- Explore model interpretability with visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our pipeline\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "from importlib import import_module\n",
        "\n",
        "# Import pipeline components\n",
        "try:\n",
        "    pipeline_module = import_module('6.2-cnn-defect-detection-pipeline')\n",
        "    print(\"\u2713 Pipeline module loaded successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u26a0 Could not import pipeline: {e}\")\n",
        "    print(\"Make sure you're running from the module-6 directory\")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Wafer Map Defect Patterns\n",
        "\n",
        "Wafer maps show the pass/fail status of dies across a circular silicon wafer. Different defect patterns indicate different manufacturing issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate examples of each defect pattern\n",
        "patterns = ['normal', 'center', 'edge', 'scratch', 'donut']\n",
        "pattern_descriptions = {\n",
        "    'normal': 'Random scattered failures (natural yield loss)',\n",
        "    'center': 'Central circular defect (spin coating issues)',\n",
        "    'edge': 'Edge ring pattern (edge bead removal)',\n",
        "    'scratch': 'Linear defect (mechanical handling)',\n",
        "    'donut': 'Ring-shaped pattern (temperature gradient)'\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "fig.suptitle('Wafer Map Defect Patterns', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, pattern in enumerate(patterns):\n",
        "    wafer = pipeline_module.generate_synthetic_wafer_map(\n",
        "        pattern=pattern, \n",
        "        size=64, \n",
        "        noise_level=0.03,\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    axes[i].imshow(wafer, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    axes[i].set_title(f'{pattern.title()}\\n{pattern_descriptions[pattern]}', \n",
        "                     fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "    \n",
        "    # Add circle to show wafer boundary\n",
        "    circle = plt.Circle((31.5, 31.5), 31.5, fill=False, color='blue', linewidth=1)\n",
        "    axes[i].add_patch(circle)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Color coding:\")\n",
        "print(\"\ud83d\udfe2 Green = Passing dies\")\n",
        "print(\"\ud83d\udfe1 Yellow = Marginal\")\n",
        "print(\"\ud83d\udd34 Red = Failing dies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Synthetic Training Dataset\n",
        "\n",
        "We'll create a synthetic dataset that mimics real wafer map patterns for training our CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic dataset\n",
        "print(\"Generating synthetic wafer map dataset...\")\n",
        "X_train, y_train, class_names = pipeline_module.generate_synthetic_dataset(\n",
        "    n_samples=400,  # Modest size for notebook\n",
        "    image_size=64,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {X_train.shape}\")\n",
        "print(f\"Labels shape: {y_train.shape}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "# Analyze class distribution\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "class_dist = pd.DataFrame({\n",
        "    'Class': [class_names[i] for i in unique],\n",
        "    'Count': counts,\n",
        "    'Percentage': counts / len(y_train) * 100\n",
        "})\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(class_dist)\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(class_dist['Class'], class_dist['Count'])\n",
        "plt.title('Sample Count by Class')\n",
        "plt.xlabel('Defect Pattern')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(class_dist['Count'], labels=class_dist['Class'], autopct='%1.1f%%')\n",
        "plt.title('Class Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Explore Sample Variations\n",
        "\n",
        "Let's look at multiple examples of each pattern to understand the natural variation in our synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show multiple examples of each class\n",
        "n_examples = 3\n",
        "fig, axes = plt.subplots(len(class_names), n_examples, figsize=(15, 12))\n",
        "fig.suptitle('Sample Variations by Defect Pattern', fontsize=16, fontweight='bold')\n",
        "\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    # Find samples of this class\n",
        "    class_mask = y_train == class_idx\n",
        "    class_samples = X_train[class_mask]\n",
        "    \n",
        "    for example_idx in range(n_examples):\n",
        "        if example_idx < len(class_samples):\n",
        "            sample = class_samples[example_idx]\n",
        "            axes[class_idx, example_idx].imshow(sample, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "        else:\n",
        "            axes[class_idx, example_idx].text(0.5, 0.5, 'No data', \n",
        "                                             ha='center', va='center', \n",
        "                                             transform=axes[class_idx, example_idx].transAxes)\n",
        "        \n",
        "        axes[class_idx, example_idx].axis('off')\n",
        "        \n",
        "        if example_idx == 0:\n",
        "            axes[class_idx, example_idx].set_ylabel(class_name.title(), \n",
        "                                                   fontsize=12, fontweight='bold')\n",
        "        \n",
        "        if class_idx == 0:\n",
        "            axes[class_idx, example_idx].set_title(f'Example {example_idx + 1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train CNN Model\n",
        "\n",
        "Now let's train our CNN model on the synthetic data. We'll use the production pipeline we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train CNN pipeline\n",
        "print(\"Training CNN model...\")\n",
        "print(f\"PyTorch available: {pipeline_module.HAS_TORCH}\")\n",
        "\n",
        "# Initialize pipeline\n",
        "cnn_pipeline = pipeline_module.CNNDefectPipeline(\n",
        "    model_type='simple_cnn',\n",
        "    num_classes=len(class_names),\n",
        "    epochs=10,  # More epochs for better learning\n",
        "    batch_size=32,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "cnn_pipeline.fit(X_train, y_train, class_names)\n",
        "\n",
        "print(\"\u2713 Model training completed\")\n",
        "print(f\"Model type: {cnn_pipeline.model_type}\")\n",
        "print(f\"Backend: {'PyTorch' if pipeline_module.HAS_TORCH else 'sklearn'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate Model Performance\n",
        "\n",
        "Let's evaluate our trained model using both standard ML metrics and manufacturing-specific metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model performance\n",
        "print(\"Evaluating model performance...\")\n",
        "metrics = cnn_pipeline.evaluate(X_train, y_train)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Performance Metrics:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy:           {metrics['accuracy']:.3f}\")\n",
        "print(f\"F1-Score (Macro):   {metrics['f1_macro']:.3f}\")\n",
        "print(f\"F1-Score (Weighted): {metrics['f1_weighted']:.3f}\")\n",
        "print(f\"ROC-AUC (OvR):      {metrics['roc_auc_ovr']:.3f}\")\n",
        "print(f\"PR-AUC (Macro):     {metrics['pr_auc_macro']:.3f}\")\n",
        "print(\"\\n\ud83c\udfed Manufacturing Metrics:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"PWS (Prediction Within Spec): {metrics['pws']:.1f}%\")\n",
        "print(f\"Estimated Loss:     {metrics['estimated_loss']:.3f}\")\n",
        "\n",
        "# Visualize metrics\n",
        "metric_names = ['Accuracy', 'F1-Macro', 'F1-Weighted', 'ROC-AUC', 'PR-AUC']\n",
        "metric_values = [metrics['accuracy'], metrics['f1_macro'], \n",
        "                metrics['f1_weighted'], metrics['roc_auc_ovr'], \n",
        "                metrics['pr_auc_macro']]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "bars = plt.bar(metric_names, metric_values, \n",
        "              color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
        "plt.title('Model Performance Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, metric_values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "manufacturing_metrics = ['PWS (%)', 'Est. Loss']\n",
        "manufacturing_values = [metrics['pws'], metrics['estimated_loss'] * 100]  # Scale loss for visualization\n",
        "plt.bar(manufacturing_metrics, manufacturing_values, color=['orange', 'red'])\n",
        "plt.title('Manufacturing-Specific Metrics')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Detailed Prediction Analysis\n",
        "\n",
        "Let's analyze the model's predictions in detail to understand its behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions and probabilities\n",
        "predictions = cnn_pipeline.predict(X_train)\n",
        "probabilities = cnn_pipeline.predict_proba(X_train)\n",
        "\n",
        "# Convert predictions back to numeric for analysis\n",
        "pred_numeric = cnn_pipeline.label_encoder.transform(predictions)\n",
        "\n",
        "# Create confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_train, pred_numeric)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (Counts)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix (Normalized)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Plot prediction confidence distribution\n",
        "plt.subplot(1, 3, 3)\n",
        "max_probs = np.max(probabilities, axis=1)\n",
        "plt.hist(max_probs, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.title('Prediction Confidence Distribution')\n",
        "plt.xlabel('Maximum Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(np.mean(max_probs), color='red', linestyle='--', \n",
        "           label=f'Mean: {np.mean(max_probs):.3f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\n\ud83d\udccb Detailed Classification Report:\")\n",
        "print(\"=\" * 50)\n",
        "print(classification_report(y_train, pred_numeric, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analyze Misclassifications\n",
        "\n",
        "Let's examine cases where the model made incorrect predictions to understand its limitations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find misclassified samples\n",
        "misclassified_mask = y_train != pred_numeric\n",
        "misclassified_indices = np.where(misclassified_mask)[0]\n",
        "\n",
        "print(f\"Total misclassifications: {len(misclassified_indices)} / {len(y_train)} ({100*len(misclassified_indices)/len(y_train):.1f}%)\")\n",
        "\n",
        "if len(misclassified_indices) > 0:\n",
        "    # Show some misclassified examples\n",
        "    n_show = min(8, len(misclassified_indices))\n",
        "    show_indices = misclassified_indices[:n_show]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    fig.suptitle('Misclassified Examples', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i, idx in enumerate(show_indices):\n",
        "        row, col = i // 4, i % 4\n",
        "        \n",
        "        true_label = class_names[y_train[idx]]\n",
        "        pred_label = class_names[pred_numeric[idx]]\n",
        "        confidence = np.max(probabilities[idx])\n",
        "        \n",
        "        axes[row, col].imshow(X_train[idx], cmap='RdYlGn', vmin=0, vmax=1)\n",
        "        axes[row, col].set_title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}', \n",
        "                                fontsize=10)\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(n_show, 8):\n",
        "        row, col = i // 4, i % 4\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze misclassification patterns\n",
        "    misclass_true = y_train[misclassified_mask]\n",
        "    misclass_pred = pred_numeric[misclassified_mask]\n",
        "    \n",
        "    print(\"\\nMost common misclassification patterns:\")\n",
        "    for true_class in range(len(class_names)):\n",
        "        for pred_class in range(len(class_names)):\n",
        "            if true_class != pred_class:\n",
        "                count = np.sum((misclass_true == true_class) & (misclass_pred == pred_class))\n",
        "                if count > 0:\n",
        "                    print(f\"  {class_names[true_class]} \u2192 {class_names[pred_class]}: {count} cases\")\nelse:\n    print(\"\ud83c\udf89 Perfect classification! No misclassifications found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test on New Synthetic Data\n",
        "\n",
        "Let's test our trained model on a fresh set of synthetic data to simulate real-world deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test data with different random seed\n",
        "print(\"Generating fresh test data...\")\n",
        "X_test, y_test, _ = pipeline_module.generate_synthetic_dataset(\n",
        "    n_samples=100,  # Smaller test set\n",
        "    image_size=64,\n",
        "    seed=123  # Different seed for true out-of-sample testing\n",
        ")\n",
        "\n",
        "print(f\"Test dataset shape: {X_test.shape}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_metrics = cnn_pipeline.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"\\n\ud83e\uddea Test Set Performance:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Accuracy:     {test_metrics['accuracy']:.3f}\")\n",
        "print(f\"F1-Macro:     {test_metrics['f1_macro']:.3f}\")\n",
        "print(f\"PWS:          {test_metrics['pws']:.1f}%\")\n",
        "print(f\"Est. Loss:    {test_metrics['estimated_loss']:.3f}\")\n",
        "\n",
        "# Compare train vs test performance\n",
        "comparison_metrics = ['accuracy', 'f1_macro', 'pws']\n",
        "train_values = [metrics[m] for m in comparison_metrics]\n",
        "test_values = [test_metrics[m] for m in comparison_metrics]\n",
        "\n",
        "x = np.arange(len(comparison_metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(x - width/2, train_values, width, label='Training', alpha=0.8)\n",
        "plt.bar(x + width/2, test_values, width, label='Test', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Training vs Test Performance')\n",
        "plt.xticks(x, [m.replace('_', ' ').title() for m in comparison_metrics])\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels\n",
        "for i, (train_val, test_val) in enumerate(zip(train_values, test_values)):\n",
        "    plt.text(i - width/2, train_val + 0.01, f'{train_val:.3f}', \n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "    plt.text(i + width/2, test_val + 0.01, f'{test_val:.3f}', \n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check for overfitting\n",
        "accuracy_diff = metrics['accuracy'] - test_metrics['accuracy']\n",
        "print(f\"\\n\ud83d\udcc8 Overfitting Analysis:\")\n",
        "print(f\"Accuracy difference (train - test): {accuracy_diff:.3f}\")\n",
        "if accuracy_diff > 0.1:\n",
        "    print(\"\u26a0\ufe0f  Potential overfitting detected (>10% accuracy drop)\")\n",
        "elif accuracy_diff > 0.05:\n",
        "    print(\"\ud83d\udfe1 Moderate overfitting (5-10% accuracy drop)\")\n",
        "else:\n",
        "    print(\"\u2705 Good generalization (minimal overfitting)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Persistence and Loading\n",
        "\n",
        "Demonstrate saving and loading the trained model for production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_path = Path('cnn_wafer_model.joblib')\n",
        "print(f\"Saving model to {model_path}...\")\n",
        "\n",
        "cnn_pipeline.save(model_path)\n",
        "print(f\"\u2713 Model saved successfully\")\n",
        "print(f\"File size: {model_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# Load the model\n",
        "print(\"\\nLoading saved model...\")\n",
        "loaded_pipeline = pipeline_module.CNNDefectPipeline.load(model_path)\n",
        "print(\"\u2713 Model loaded successfully\")\n",
        "\n",
        "# Verify the loaded model works\n",
        "sample_wafer = X_test[:1]  # Take first test sample\n",
        "original_pred = cnn_pipeline.predict(sample_wafer)\n",
        "loaded_pred = loaded_pipeline.predict(sample_wafer)\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d Verification:\")\n",
        "print(f\"Original model prediction: {original_pred[0]}\")\n",
        "print(f\"Loaded model prediction:   {loaded_pred[0]}\")\n",
        "print(f\"Predictions match: {original_pred[0] == loaded_pred[0]}\")\n",
        "\n",
        "# Show model metadata\n",
        "print(f\"\\n\ud83d\udccb Model Metadata:\")\n",
        "print(f\"Model type: {loaded_pipeline.metadata.model_type}\")\n",
        "print(f\"Training date: {loaded_pipeline.metadata.trained_at}\")\n",
        "print(f\"Input shape: {loaded_pipeline.metadata.input_shape}\")\n",
        "print(f\"Classes: {loaded_pipeline.metadata.class_names}\")\n",
        "print(f\"PyTorch available: {loaded_pipeline.metadata.pytorch_available}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Interactive Prediction Demo\n",
        "\n",
        "Let's create an interactive demonstration where we can generate specific patterns and see predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive prediction demonstration\n",
        "def predict_and_visualize(pattern, size=64, noise_level=0.05):\n",
        "    \"\"\"Generate a wafer map and predict its pattern\"\"\"\n",
        "    # Generate wafer map\n",
        "    wafer = pipeline_module.generate_synthetic_wafer_map(\n",
        "        pattern=pattern, size=size, noise_level=noise_level\n",
        "    )\n",
        "    \n",
        "    # Make prediction\n",
        "    wafer_batch = wafer.reshape(1, size, size)\n",
        "    prediction = loaded_pipeline.predict(wafer_batch)[0]\n",
        "    probabilities = loaded_pipeline.predict_proba(wafer_batch)[0]\n",
        "    \n",
        "    # Visualize results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Show wafer map\n",
        "    axes[0].imshow(wafer, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    axes[0].set_title(f'Input Wafer Map\\n(True pattern: {pattern})', fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Show prediction probabilities\n",
        "    bars = axes[1].bar(class_names, probabilities, \n",
        "                      color=['lightcoral' if cls == prediction else 'lightblue' \n",
        "                            for cls in class_names])\n",
        "    axes[1].set_title(f'Prediction Probabilities\\nPredicted: {prediction}', fontweight='bold')\n",
        "    axes[1].set_ylabel('Probability')\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, prob in zip(bars, probabilities):\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                    f'{prob:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print results\n",
        "    confidence = np.max(probabilities)\n",
        "    is_correct = pattern == prediction\n",
        "    \n",
        "    print(f\"\ud83c\udfaf True pattern: {pattern}\")\n",
        "    print(f\"\ud83e\udd16 Predicted: {prediction}\")\n",
        "    print(f\"\ud83d\udcca Confidence: {confidence:.3f}\")\n",
        "    print(f\"\u2705 Correct: {is_correct}\")\n",
        "    \n",
        "    return is_correct, confidence\n",
        "\n",
        "# Test on each pattern type\n",
        "print(\"\ud83e\uddea Testing model on each defect pattern:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results = []\n",
        "for pattern in patterns:\n",
        "    print(f\"\\nTesting {pattern} pattern:\")\n",
        "    is_correct, confidence = predict_and_visualize(pattern)\n",
        "    results.append((pattern, is_correct, confidence))\n",
        "\n",
        "# Summary\n",
        "correct_count = sum(1 for _, correct, _ in results if correct)\n",
        "avg_confidence = np.mean([conf for _, _, conf in results])\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 Summary:\")\n",
        "print(f\"Correct predictions: {correct_count}/{len(patterns)} ({100*correct_count/len(patterns):.1f}%)\")\n",
        "print(f\"Average confidence: {avg_confidence:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Manufacturing Insights and Recommendations\n",
        "\n",
        "Let's analyze what we've learned and provide actionable insights for semiconductor manufacturing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze model performance by defect type\n",
        "test_predictions = loaded_pipeline.predict(X_test)\n",
        "test_pred_numeric = loaded_pipeline.label_encoder.transform(test_predictions)\n",
        "test_probabilities = loaded_pipeline.predict_proba(X_test)\n",
        "\n",
        "# Calculate per-class performance\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "per_class_metrics = []\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_mask = y_test == i\n",
        "    if np.sum(class_mask) > 0:  # Only if we have samples of this class\n",
        "        precision = precision_score(y_test == i, test_pred_numeric == i)\n",
        "        recall = recall_score(y_test == i, test_pred_numeric == i)\n",
        "        f1 = f1_score(y_test == i, test_pred_numeric == i)\n",
        "        \n",
        "        # Average confidence for this class\n",
        "        class_confidences = np.max(test_probabilities[class_mask], axis=1)\n",
        "        avg_confidence = np.mean(class_confidences)\n",
        "        \n",
        "        per_class_metrics.append({\n",
        "            'Class': class_name,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-Score': f1,\n",
        "            'Avg_Confidence': avg_confidence,\n",
        "            'Sample_Count': np.sum(class_mask)\n",
        "        })\n",
        "\n",
        "# Create performance DataFrame\n",
        "performance_df = pd.DataFrame(per_class_metrics)\n",
        "print(\"\ud83d\udcca Per-Class Performance Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "print(performance_df.round(3))\n",
        "\n",
        "# Visualize per-class performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Per-Class Performance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Precision by class\n",
        "axes[0, 0].bar(performance_df['Class'], performance_df['Precision'], color='skyblue')\n",
        "axes[0, 0].set_title('Precision by Defect Type')\n",
        "axes[0, 0].set_ylabel('Precision')\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Recall by class\n",
        "axes[0, 1].bar(performance_df['Class'], performance_df['Recall'], color='lightgreen')\n",
        "axes[0, 1].set_title('Recall by Defect Type')\n",
        "axes[0, 1].set_ylabel('Recall')\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# F1-Score by class\n",
        "axes[1, 0].bar(performance_df['Class'], performance_df['F1-Score'], color='lightcoral')\n",
        "axes[1, 0].set_title('F1-Score by Defect Type')\n",
        "axes[1, 0].set_ylabel('F1-Score')\n",
        "axes[1, 0].set_ylim(0, 1)\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Confidence by class\n",
        "axes[1, 1].bar(performance_df['Class'], performance_df['Avg_Confidence'], color='gold')\n",
        "axes[1, 1].set_title('Average Confidence by Defect Type')\n",
        "axes[1, 1].set_ylabel('Average Confidence')\n",
        "axes[1, 1].set_ylim(0, 1)\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Manufacturing recommendations\n",
        "print(\"\\n\ud83c\udfed Manufacturing Recommendations:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Find best and worst performing classes\n",
        "best_f1_class = performance_df.loc[performance_df['F1-Score'].idxmax()]\n",
        "worst_f1_class = performance_df.loc[performance_df['F1-Score'].idxmin()]\n",
        "\n",
        "print(f\"\u2705 Best performance: {best_f1_class['Class']} (F1: {best_f1_class['F1-Score']:.3f})\")\n",
        "print(f\"\u26a0\ufe0f  Needs improvement: {worst_f1_class['Class']} (F1: {worst_f1_class['F1-Score']:.3f})\")\n",
        "\n",
        "# Low confidence warnings\n",
        "low_confidence_threshold = 0.8\n",
        "low_conf_classes = performance_df[performance_df['Avg_Confidence'] < low_confidence_threshold]\n",
        "if len(low_conf_classes) > 0:\n",
        "    print(f\"\\n\ud83d\udfe1 Low confidence classes (< {low_confidence_threshold}):\")\n",
        "    for _, row in low_conf_classes.iterrows():\n",
        "        print(f\"   - {row['Class']}: {row['Avg_Confidence']:.3f}\")\n",
        "    print(\"   \u2192 Consider collecting more training data for these patterns\")\n",
        "\n",
        "# Recall warnings (missed defects are costly)\n",
        "low_recall_threshold = 0.8\n",
        "low_recall_classes = performance_df[performance_df['Recall'] < low_recall_threshold]\n",
        "if len(low_recall_classes) > 0:\n",
        "    print(f\"\\n\ud83d\udd34 Low recall classes (< {low_recall_threshold}):\")\n",
        "    for _, row in low_recall_classes.iterrows():\n",
        "        print(f\"   - {row['Class']}: {row['Recall']:.3f}\")\n",
        "    print(\"   \u2192 High risk of missing critical defects - priority for improvement\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Next Steps:\")\n",
        "print(f\"   1. Deploy model for real-time wafer screening\")\n",
        "print(f\"   2. Implement confidence-based human review (< 0.8 confidence)\")\n",
        "print(f\"   3. Collect more data for underperforming classes\")\n",
        "print(f\"   4. Monitor model performance over time for drift\")\n",
        "print(f\"   5. Consider ensemble methods for critical applications\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Cleanup and Summary\n",
        "\n",
        "Clean up temporary files and summarize what we've accomplished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up saved model file\n",
        "if model_path.exists():\n",
        "    model_path.unlink()\n",
        "    print(f\"\ud83d\uddd1\ufe0f  Cleaned up temporary model file: {model_path}\")\n",
        "\n",
        "# Check for PyTorch model file\n",
        "pytorch_model_path = model_path.with_suffix('.pth')\n",
        "if pytorch_model_path.exists():\n",
        "    pytorch_model_path.unlink()\n",
        "    print(f\"\ud83d\uddd1\ufe0f  Cleaned up PyTorch model file: {pytorch_model_path}\")\n",
        "\n",
        "print(\"\\n\ud83c\udf89 Module 6.2 Complete - CNN Wafer Defect Detection\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n\ud83d\udcda What we accomplished:\")\n",
        "print(\"   \u2705 Generated synthetic wafer map dataset with 5 defect patterns\")\n",
        "print(\"   \u2705 Trained CNN model for spatial pattern recognition\")\n",
        "print(\"   \u2705 Evaluated performance with manufacturing-specific metrics\")\n",
        "print(\"   \u2705 Analyzed misclassifications and model behavior\")\n",
        "print(\"   \u2705 Demonstrated model persistence and loading\")\n",
        "print(\"   \u2705 Provided actionable manufacturing insights\")\n",
        "\n",
        "print(\"\\n\ud83d\udd27 Key skills learned:\")\n",
        "print(\"   \u2022 Spatial pattern recognition in manufacturing data\")\n",
        "print(\"   \u2022 CNN architecture design for small datasets\")\n",
        "print(\"   \u2022 Class imbalance handling techniques\")\n",
        "print(\"   \u2022 Manufacturing-specific evaluation metrics\")\n",
        "print(\"   \u2022 Model interpretability and confidence analysis\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Ready for production:\")\n",
        "print(\"   \u2022 Use the CLI pipeline for batch processing\")\n",
        "print(\"   \u2022 Implement confidence-based human review\")\n",
        "print(\"   \u2022 Monitor for data drift and model performance\")\n",
        "print(\"   \u2022 Scale with real WM-811K dataset when available\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Next modules:\")\n",
        "print(\"   \u2022 Module 6.3: Advanced CNN architectures\")\n",
        "print(\"   \u2022 Module 7.1: Multi-scale analysis\")\n",
        "print(\"   \u2022 Module 8.1: Model explainability (Grad-CAM)\")\n",
        "print(\"   \u2022 Module 9.1: Production deployment\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
