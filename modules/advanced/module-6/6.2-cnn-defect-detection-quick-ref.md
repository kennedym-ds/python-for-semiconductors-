# 6.2 CNN Defect Detection Quick Reference

Fast cheat sheet for CNN-based wafer map defect classification in semiconductor manufacturing.

## Core CLI Patterns

Train simple CNN (synthetic teaching dataset):

```bash
python 6.2-cnn-defect-detection-pipeline.py train --dataset synthetic_wafer --model simple_cnn --epochs 5 --save cnn_model.joblib
```

Evaluate saved model:

```bash
python 6.2-cnn-defect-detection-pipeline.py evaluate --model-path cnn_model.joblib --dataset synthetic_wafer
```

Predict single wafer map:

```bash
python 6.2-cnn-defect-detection-pipeline.py predict --model-path cnn_model.joblib --input-image wafer_sample.npy
```

Predict on full dataset:

```bash
python 6.2-cnn-defect-detection-pipeline.py predict --model-path cnn_model.joblib --dataset synthetic_wafer
```

## Key Command Line Flags

**Training parameters:**
- `--model {simple_cnn,cnn}` - Model architecture (auto-fallback to sklearn)
- `--epochs <int>` - Training epochs (default: 5)
- `--batch-size <int>` - Batch size (default: 32)
- `--learning-rate <float>` - Learning rate (default: 0.001)
- `--fallback-model {random_forest,svm,logistic}` - Sklearn fallback when PyTorch unavailable

**Dataset options:**
- `--dataset {synthetic_wafer,synthetic_small,wm811k}` - Dataset choice
- `synthetic_wafer`: 500 samples, balanced classes
- `synthetic_small`: 100 samples for quick testing
- `wm811k`: Real WM-811K wafer maps (if available)

## Output JSON (train/evaluate)

```json
{
  "status": "trained|evaluated",
  "model_type": "simple_cnn",
  "metrics": {
    "accuracy": 0.92,
    "f1_macro": 0.89,
    "f1_weighted": 0.91,
    "roc_auc_ovr": 0.98,
    "pr_auc_macro": 0.87,
    "pws": 92.0,
    "estimated_loss": 0.15
  },
  "metadata": {
    "model_type": "simple_cnn",
    "input_shape": [64, 64, 1],
    "num_classes": 5,
    "class_names": ["normal", "center", "edge", "scratch", "donut"],
    "pytorch_available": true
  },
  "pytorch_available": true
}
```

## Defect Pattern Types

| Pattern | Description | Typical Cause |
|---------|-------------|---------------|
| **normal** | No defects, random scattered fails | Natural yield loss |
| **center** | Circular pattern at wafer center | Spin coating, chuck issues |
| **edge** | Ring pattern at wafer edge | Edge bead removal, clamping |
| **scratch** | Linear defect across wafer | Mechanical handling damage |
| **donut** | Ring-shaped pattern | Temperature gradient, gas flow |

## Manufacturing Metrics Explained

**Standard ML Metrics:**
- `accuracy`: Overall classification accuracy
- `f1_macro`: Unweighted average F1 across classes
- `f1_weighted`: Class-frequency weighted F1
- `roc_auc_ovr`: Multi-class ROC-AUC (one-vs-rest)
- `pr_auc_macro`: Average precision-recall AUC

**Manufacturing-Specific:**
- `pws` (Prediction Within Spec): % predictions exactly correct
- `estimated_loss`: Weighted misclassification cost (lower better)

## Dependency Handling

**PyTorch Available:**
- Uses SimpleCNN with convolutional layers
- GPU acceleration if available (auto-detected)
- Full CNN training with backpropagation

**PyTorch Unavailable (Graceful Fallback):**
- Uses sklearn RandomForest on flattened images
- CPU-only processing
- Still effective for pattern classification

Check availability:
```bash
python -c "import torch; print('PyTorch OK')" 2>/dev/null || echo "Using sklearn fallback"
```

## Synthetic Data Patterns

The synthetic generator creates realistic wafer maps:

```python
from pipeline import generate_synthetic_wafer_map

# Generate specific pattern
wafer = generate_synthetic_wafer_map(
    pattern='center',    # Pattern type
    size=64,            # Image size (64x64)
    noise_level=0.05    # Background noise
)
```

**Pattern characteristics:**
- `center`: Circular defect, radius ≈ size/6
- `edge`: Ring at periphery, width ≈ size/10
- `scratch`: Linear, random orientation, width ≈ size/32
- `donut`: Ring pattern, inner radius ≈ size/4, outer ≈ size/3
- `normal`: Just background noise

## Rapid Troubleshooting

**Low accuracy (<60%)**:
- Increase epochs: `--epochs 20`
- Check class balance in your data
- Verify image preprocessing is correct

**Training too slow:**
- Reduce batch size: `--batch-size 16`
- Use smaller dataset: `--dataset synthetic_small`
- Check if using CPU when GPU available

**Memory errors:**
- Reduce batch size: `--batch-size 8`
- Use smaller image size (modify DEFAULT_IMAGE_SIZE)
- Switch to sklearn fallback

**Model won't save/load:**
- Check write permissions
- Ensure consistent PyTorch versions
- Try `.joblib` extension

## Adding Real WM-811K Data

1. Download from Kaggle: [WM-811K Wafer Map](https://www.kaggle.com/datasets/qingyi/wm811k-wafer-map)
2. Extract to `datasets/wm811k/` directory
3. Implement custom loader in pipeline (placeholder exists)
4. Use `--dataset wm811k` flag

Expected structure:
```
datasets/wm811k/
  data/
    wafer_map/  # Arrays or images
    label/      # Metadata CSV
```

## Model Size & Performance

**SimpleCNN Architecture:**
- Input: 64×64×1 (single-channel)
- Conv layers: 16→32→64 filters
- Parameters: ~200K (lightweight)
- Training time: ~30s for 500 samples, 5 epochs (CPU)
- Memory: ~50MB model size

**sklearn Fallback:**
- Features: 4096 (64×64 flattened)
- RandomForest: 100 trees, max_depth=10
- Training time: ~5s for 500 samples
- Memory: ~10MB model size

## Integration Patterns

**Batch Processing:**
```python
# Load model once, predict many
pipeline = CNNDefectPipeline.load('model.joblib')
for wafer_batch in wafer_stream:
    predictions = pipeline.predict(wafer_batch)
    process_results(predictions)
```

**Real-time Inference:**
```python
# Single wafer prediction
result = pipeline.predict(single_wafer.reshape(1, 64, 64))
confidence = pipeline.predict_proba(single_wafer.reshape(1, 64, 64))
```

## Hyperparameter Tuning

**For better accuracy:**
- Increase epochs: `--epochs 50`
- Tune learning rate: `--learning-rate 0.0001`
- Experiment with batch size: `--batch-size 64`

**For faster training:**
- Smaller model: implement MiniCNN variant
- Use `--dataset synthetic_small`
- Lower epochs: `--epochs 3`

## Common Error Messages

**"PyTorch not available"**: Normal warning, sklearn fallback active
**"Unknown dataset"**: Check dataset name spelling
**"Model not fitted"**: Load model before predict/evaluate
**"RuntimeError: Nothing to save"**: Train model first
**"CUDA out of memory"**: Reduce batch size or use CPU

## Performance Benchmarks

**Target metrics on synthetic data:**
- Accuracy: >85%
- F1-macro: >80%
- PWS: >85%
- Training time: <60s (CPU)

**Real-world expectations:**
- Accuracy: 70-90% (depends on data quality)
- Class imbalance affects F1 scores
- Edge cases require expert review

## Next Steps (Future Modules)

- **6.3**: Advanced architectures (ResNet, Vision Transformer)
- **7.1**: Multi-scale analysis (die + wafer level)
- **7.2**: Temporal modeling (wafer sequences)
- **8.1**: Explainability (Grad-CAM, LIME)
- **9.1**: Production deployment (FastAPI, containerization)

## Python API Usage (Minimal)

```python
from pipeline import CNNDefectPipeline, generate_synthetic_dataset

# Generate data
X, y, class_names = generate_synthetic_dataset(n_samples=200)

# Train model
pipeline = CNNDefectPipeline(epochs=10)
pipeline.fit(X, y, class_names)

# Evaluate
metrics = pipeline.evaluate(X, y)
print(f"Accuracy: {metrics['accuracy']:.2f}")

# Save/load
pipeline.save(Path('my_model.joblib'))
loaded = CNNDefectPipeline.load(Path('my_model.joblib'))
```

## When to Use CNN vs Traditional ML

**Use CNN when:**
- Spatial patterns are important (defect signatures)
- Large dataset available (>1K samples)
- Need end-to-end learning
- Complex, non-linear patterns

**Use Traditional ML when:**
- Small dataset (<500 samples)
- Need interpretable features
- Fast inference required
- Limited computational resources

## Debugging Training Issues

**Loss not decreasing:**
- Check learning rate (try 0.01, 0.001, 0.0001)
- Verify data preprocessing
- Ensure labels are correct

**Overfitting (train >> test accuracy):**
- Add more dropout
- Use data augmentation
- Reduce model complexity
- Get more training data

**Underfitting (both accuracies low):**
- Increase model capacity
- Reduce regularization
- Train longer
- Check data quality