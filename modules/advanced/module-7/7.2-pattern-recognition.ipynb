{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 7.2 \u2013 Wafer Map Pattern Recognition\n",
        "\n",
        "## Interactive Tutorial: Classical + Deep Learning Approaches\n",
        "\n",
        "This notebook demonstrates end-to-end wafer map pattern recognition for semiconductor manufacturing, combining classical computer vision features with deep learning for automated defect classification.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand common wafer defect patterns and their manufacturing implications\n",
        "- Implement classical feature extraction (radial histograms, GLCM, HOG)\n",
        "- Build and train a compact CNN for pattern classification\n",
        "- Evaluate models using semiconductor-specific metrics (PWS, Estimated Loss)\n",
        "- Apply explainability techniques (SHAP, Grad-CAM) for production deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Add the module path for imports\n",
        "sys.path.append(str(Path().resolve()))\n",
        "\n",
        "# Import our pattern recognition pipeline\n",
        "# Note: Run this from the module-7 directory\n",
        "exec(open('7.2-pattern-recognition-pipeline.py').read())\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate and Visualize Synthetic Wafer Maps\n",
        "\n",
        "First, let's generate synthetic wafer maps representing different defect patterns commonly seen in semiconductor manufacturing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic wafer maps\n",
        "print(\"Generating synthetic wafer map dataset...\")\n",
        "data = generate_synthetic_wafer_maps(n_samples=300, map_size=64, seed=42)\n",
        "\n",
        "wafer_maps = data['wafer_maps']\n",
        "labels = data['labels']\n",
        "pattern_names = data['pattern_names']\n",
        "\n",
        "print(f\"Generated {len(wafer_maps)} wafer maps with {len(pattern_names)} pattern types\")\n",
        "print(f\"Pattern distribution: {dict(zip(pattern_names, np.bincount(labels)))}\")\n",
        "print(f\"Wafer map shape: {wafer_maps[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize examples of each pattern type\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "fig.suptitle('Wafer Map Defect Patterns', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, pattern_name in enumerate(pattern_names):\n",
        "    # Find first example of this pattern\n",
        "    pattern_idx = np.where(labels == i)[0][0]\n",
        "    wafer_map = wafer_maps[pattern_idx]\n",
        "    \n",
        "    # Plot the wafer map\n",
        "    im = axes[i].imshow(wafer_map, cmap='RdYlBu_r', interpolation='nearest')\n",
        "    axes[i].set_title(f'{pattern_name}\\n(Class {i})', fontweight='bold')\n",
        "    axes[i].set_xticks([])\n",
        "    axes[i].set_yticks([])\n",
        "    \n",
        "    # Add colorbar\n",
        "    plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Classical Feature Extraction\n",
        "\n",
        "Let's extract classical computer vision features and understand what they capture about different defect patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize feature extractor\n",
        "feature_extractor = ClassicalFeatureExtractor(n_radial_bins=10, n_angular_bins=8)\n",
        "\n",
        "# Extract features for a few examples\n",
        "example_indices = [np.where(labels == i)[0][0] for i in range(5)]\n",
        "example_features = {}\n",
        "\n",
        "for i, idx in enumerate(example_indices):\n",
        "    wafer_map = wafer_maps[idx]\n",
        "    pattern_name = pattern_names[i]\n",
        "    \n",
        "    # Extract individual feature types\n",
        "    radial_hist = feature_extractor.extract_radial_histogram(wafer_map)\n",
        "    angular_hist = feature_extractor.extract_angular_histogram(wafer_map)\n",
        "    texture_features = feature_extractor.extract_texture_features(wafer_map)\n",
        "    region_props = feature_extractor.extract_region_properties(wafer_map)\n",
        "    \n",
        "    example_features[pattern_name] = {\n",
        "        'radial': radial_hist,\n",
        "        'angular': angular_hist,\n",
        "        'texture': texture_features,\n",
        "        'region': region_props\n",
        "    }\n",
        "\n",
        "print(\"Classical features extracted for each pattern type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize radial and angular histograms\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "fig.suptitle('Classical Feature Analysis by Pattern Type', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, pattern_name in enumerate(pattern_names):\n",
        "    features = example_features[pattern_name]\n",
        "    \n",
        "    # Radial histogram (top row)\n",
        "    axes[0, i].bar(range(len(features['radial'])), features['radial'], alpha=0.7)\n",
        "    axes[0, i].set_title(f'{pattern_name}\\nRadial Density')\n",
        "    axes[0, i].set_xlabel('Radial Bin')\n",
        "    axes[0, i].set_ylabel('Defect Density')\n",
        "    \n",
        "    # Angular histogram (bottom row)\n",
        "    axes[1, i].bar(range(len(features['angular'])), features['angular'], alpha=0.7, color='orange')\n",
        "    axes[1, i].set_title(f'Angular Density')\n",
        "    axes[1, i].set_xlabel('Angular Bin')\n",
        "    axes[1, i].set_ylabel('Defect Density')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Texture feature comparison\n",
        "texture_names = ['Contrast', 'Dissimilarity', 'Homogeneity', 'Energy', 'Correlation']\n",
        "texture_data = []\n",
        "\n",
        "for pattern_name in pattern_names:\n",
        "    for j, texture_name in enumerate(texture_names):\n",
        "        texture_data.append({\n",
        "            'Pattern': pattern_name,\n",
        "            'Feature': texture_name,\n",
        "            'Value': example_features[pattern_name]['texture'][j]\n",
        "        })\n",
        "\n",
        "texture_df = pd.DataFrame(texture_data)\n",
        "\n",
        "# Create heatmap\n",
        "pivot_texture = texture_df.pivot(index='Pattern', columns='Feature', values='Value')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pivot_texture, annot=True, cmap='viridis', fmt='.3f')\n",
        "plt.title('GLCM Texture Features by Pattern Type', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Classical Models\n",
        "\n",
        "Now let's train classical machine learning models using the extracted features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train classical SVM model\n",
        "print(\"Training classical SVM model...\")\n",
        "classical_pipeline = PatternRecognitionPipeline(\n",
        "    approach='classical',\n",
        "    model='svm',\n",
        "    C=1.0\n",
        ")\n",
        "\n",
        "# Train on subset for speed\n",
        "train_indices = np.random.choice(len(wafer_maps), size=200, replace=False)\n",
        "X_train = wafer_maps[train_indices]\n",
        "y_train = labels[train_indices]\n",
        "wafer_ids_train = [f\"W{i//20:03d}\" for i in train_indices]\n",
        "\n",
        "classical_pipeline.fit(X_train, y_train, wafer_ids_train, pattern_names)\n",
        "print(\"Classical model training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate classical model\n",
        "test_indices = np.setdiff1d(np.arange(len(wafer_maps)), train_indices)[:50]  # Small test set\n",
        "X_test = wafer_maps[test_indices]\n",
        "y_test = labels[test_indices]\n",
        "\n",
        "classical_metrics = classical_pipeline.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Classical Model Performance:\")\n",
        "for metric, value in classical_metrics.items():\n",
        "    print(f\"  {metric}: {value:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Deep Learning CNN\n",
        "\n",
        "Let's compare with a deep learning approach using a compact CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if PyTorch is available\n",
        "if HAS_TORCH:\n",
        "    print(\"Training CNN model...\")\n",
        "    \n",
        "    cnn_pipeline = PatternRecognitionPipeline(\n",
        "        approach='deep_learning',\n",
        "        model='cnn',\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        learning_rate=0.001\n",
        "    )\n",
        "    \n",
        "    # Train CNN\n",
        "    cnn_pipeline.fit(X_train, y_train, wafer_ids_train, pattern_names)\n",
        "    \n",
        "    # Evaluate CNN\n",
        "    cnn_metrics = cnn_pipeline.evaluate(X_test, y_test)\n",
        "    \n",
        "    print(\"\\nCNN Model Performance:\")\n",
        "    for metric, value in cnn_metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "        \n",
        "else:\n",
        "    print(\"PyTorch not available - skipping CNN training\")\n",
        "    cnn_metrics = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison and Analysis\n",
        "\n",
        "Let's compare the performance of classical vs. deep learning approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performance\n",
        "comparison_data = []\n",
        "\n",
        "# Classical metrics\n",
        "for metric, value in classical_metrics.items():\n",
        "    comparison_data.append({'Model': 'Classical SVM', 'Metric': metric, 'Value': value})\n",
        "\n",
        "# CNN metrics (if available)\n",
        "if cnn_metrics:\n",
        "    for metric, value in cnn_metrics.items():\n",
        "        comparison_data.append({'Model': 'CNN', 'Metric': metric, 'Value': value})\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Create comparison plot\n",
        "key_metrics = ['accuracy', 'f1_weighted', 'roc_auc_weighted', 'pws', 'estimated_loss']\n",
        "plot_data = comparison_df[comparison_df['Metric'].isin(key_metrics)]\n",
        "\n",
        "if not plot_data.empty:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(data=plot_data, x='Metric', y='Value', hue='Model')\n",
        "    plt.title('Model Performance Comparison', fontweight='bold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Performance comparison plot not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Manufacturing-Specific Analysis\n",
        "\n",
        "Let's analyze the results from a semiconductor manufacturing perspective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get detailed predictions for analysis\n",
        "classical_preds = classical_pipeline.predict(X_test)\n",
        "classical_probs = classical_pipeline.predict_proba(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_test, classical_pipeline.label_encoder.transform(classical_preds))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=pattern_names, yticklabels=pattern_names)\n",
        "plt.title('Classical Model Confusion Matrix', fontweight='bold')\n",
        "plt.xlabel('Predicted Pattern')\n",
        "plt.ylabel('True Pattern')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, classical_pipeline.label_encoder.transform(classical_preds), \n",
        "                          target_names=pattern_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze confidence scores by pattern type\n",
        "confidence_data = []\n",
        "for i, (true_label, pred_probs) in enumerate(zip(y_test, classical_probs)):\n",
        "    max_prob = np.max(pred_probs)\n",
        "    confidence_data.append({\n",
        "        'Sample': i,\n",
        "        'True_Pattern': pattern_names[true_label],\n",
        "        'Confidence': max_prob\n",
        "    })\n",
        "\n",
        "confidence_df = pd.DataFrame(confidence_data)\n",
        "\n",
        "# Plot confidence distribution by pattern\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=confidence_df, x='True_Pattern', y='Confidence')\n",
        "plt.title('Prediction Confidence by Pattern Type', fontweight='bold')\n",
        "plt.ylabel('Confidence Score')\n",
        "plt.xlabel('Pattern Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.axhline(y=0.7, color='r', linestyle='--', alpha=0.7, label='Min Confidence Threshold')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis (Explainability)\n",
        "\n",
        "For production deployment, it's crucial to understand what features drive model decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract feature names for interpretation\n",
        "feature_names_detailed = (\n",
        "    [f'radial_bin_{i}' for i in range(10)] +\n",
        "    [f'angular_bin_{i}' for i in range(8)] +\n",
        "    ['glcm_contrast', 'glcm_dissimilarity', 'glcm_homogeneity', 'glcm_energy', 'glcm_correlation'] +\n",
        "    [f'hog_feature_{i}' for i in range(len(feature_extractor.extract_hog_features(X_test[0])))] +\n",
        "    ['total_area', 'total_perimeter', 'n_components', 'largest_area', 'largest_perimeter', 'compactness']\n",
        ")\n",
        "\n",
        "print(f\"Total features: {len(feature_names_detailed)}\")\n",
        "\n",
        "# For SVM, we can look at feature weights\n",
        "if hasattr(classical_pipeline.model, 'coef_'):\n",
        "    # Get feature importance for each class\n",
        "    feature_importance = np.abs(classical_pipeline.model.coef_).mean(axis=0)\n",
        "    \n",
        "    # Get top 20 most important features\n",
        "    top_indices = np.argsort(feature_importance)[-20:]\n",
        "    top_features = [feature_names_detailed[i] for i in top_indices]\n",
        "    top_importance = feature_importance[top_indices]\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(range(len(top_features)), top_importance)\n",
        "    plt.yticks(range(len(top_features)), top_features)\n",
        "    plt.xlabel('Feature Importance (|Coefficient|)')\n",
        "    plt.title('Top 20 Most Important Features (Classical SVM)', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    for i, (feat, imp) in enumerate(zip(top_features[-10:], top_importance[-10:])):\n",
        "        print(f\"{i+1:2d}. {feat}: {imp:.4f}\")\n",
        "else:\n",
        "    print(\"Feature importance not available for this model type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production Deployment Simulation\n",
        "\n",
        "Let's simulate how this model would work in a production environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_production_decision(wafer_map, model, threshold_confidence=0.7):\n",
        "    \"\"\"Simulate production decision making based on pattern recognition.\"\"\"\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(wafer_map[np.newaxis, ...])[0]\n",
        "    probabilities = model.predict_proba(wafer_map[np.newaxis, ...])[0]\n",
        "    confidence = np.max(probabilities)\n",
        "    \n",
        "    # Production decision logic\n",
        "    if confidence < threshold_confidence:\n",
        "        action = \"HUMAN_REVIEW\"\n",
        "        reason = f\"Low confidence ({confidence:.3f}) - requires expert inspection\"\n",
        "    elif prediction == \"Scratch\" and confidence > 0.8:\n",
        "        action = \"HOLD_LOT\"\n",
        "        reason = \"Critical scratch pattern detected - investigate handling equipment\"\n",
        "    elif prediction == \"Center\" and confidence > 0.7:\n",
        "        action = \"ADJUST_PROCESS\"\n",
        "        reason = \"Center pattern detected - check chuck temperature and gas flow\"\n",
        "    elif prediction == \"Edge\" and confidence > 0.7:\n",
        "        action = \"MONITOR_CLOSELY\"\n",
        "        reason = \"Edge pattern detected - monitor edge bead removal process\"\n",
        "    elif prediction == \"Ring\" and confidence > 0.7:\n",
        "        action = \"MAINTENANCE_ALERT\"\n",
        "        reason = \"Ring pattern detected - schedule equipment vibration check\"\n",
        "    else:\n",
        "        action = \"CONTINUE\"\n",
        "        reason = \"Normal pattern or acceptable variation\"\n",
        "    \n",
        "    return {\n",
        "        'pattern': prediction,\n",
        "        'confidence': confidence,\n",
        "        'action': action,\n",
        "        'reason': reason,\n",
        "        'probabilities': dict(zip(pattern_names, probabilities))\n",
        "    }\n",
        "\n",
        "# Test production simulation on a few examples\n",
        "print(\"Production Decision Simulation:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i in range(min(5, len(X_test))):\n",
        "    true_pattern = pattern_names[y_test[i]]\n",
        "    decision = simulate_production_decision(X_test[i], classical_pipeline)\n",
        "    \n",
        "    print(f\"\\nWafer {i+1}:\")\n",
        "    print(f\"  True Pattern: {true_pattern}\")\n",
        "    print(f\"  Predicted: {decision['pattern']} (confidence: {decision['confidence']:.3f})\")\n",
        "    print(f\"  Action: {decision['action']}\")\n",
        "    print(f\"  Reason: {decision['reason']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Cost-Benefit Analysis\n",
        "\n",
        "Let's analyze the business impact of our pattern recognition system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define cost structure (example values)\n",
        "pattern_costs = {\n",
        "    'Normal': {'miss_cost': 0, 'false_alarm_cost': 5},\n",
        "    'Center': {'miss_cost': 50, 'false_alarm_cost': 10},\n",
        "    'Edge': {'miss_cost': 40, 'false_alarm_cost': 8},\n",
        "    'Scratch': {'miss_cost': 100, 'false_alarm_cost': 15},\n",
        "    'Ring': {'miss_cost': 60, 'false_alarm_cost': 12}\n",
        "}\n",
        "\n",
        "def calculate_business_impact(y_true, y_pred, pattern_names, cost_structure):\n",
        "    \"\"\"Calculate business impact of pattern recognition system.\"\"\"\n",
        "    \n",
        "    total_cost = 0\n",
        "    pattern_costs_detail = {name: {'miss': 0, 'false_alarm': 0, 'correct': 0} \n",
        "                           for name in pattern_names}\n",
        "    \n",
        "    for true_idx, pred_name in zip(y_true, y_pred):\n",
        "        true_name = pattern_names[true_idx]\n",
        "        \n",
        "        if true_name == pred_name:\n",
        "            # Correct prediction - no cost\n",
        "            pattern_costs_detail[true_name]['correct'] += 1\n",
        "        else:\n",
        "            # Misclassification\n",
        "            if true_name != 'Normal':\n",
        "                # False negative - missed a defect pattern\n",
        "                cost = cost_structure[true_name]['miss_cost']\n",
        "                pattern_costs_detail[true_name]['miss'] += 1\n",
        "            else:\n",
        "                # False positive - false alarm\n",
        "                cost = cost_structure[pred_name]['false_alarm_cost']\n",
        "                pattern_costs_detail[pred_name]['false_alarm'] += 1\n",
        "            \n",
        "            total_cost += cost\n",
        "    \n",
        "    return total_cost, pattern_costs_detail\n",
        "\n",
        "# Calculate cost for our model\n",
        "total_cost, cost_breakdown = calculate_business_impact(\n",
        "    y_test, classical_preds, pattern_names, pattern_costs\n",
        ")\n",
        "\n",
        "print(f\"Total Business Cost: ${total_cost}\")\n",
        "print(f\"Average Cost per Wafer: ${total_cost/len(y_test):.2f}\")\n",
        "\n",
        "print(\"\\nCost Breakdown by Pattern:\")\n",
        "for pattern, costs in cost_breakdown.items():\n",
        "    print(f\"  {pattern}:\")\n",
        "    print(f\"    Correct: {costs['correct']}\")\n",
        "    print(f\"    Missed: {costs['miss']}\")\n",
        "    print(f\"    False Alarms: {costs['false_alarm']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Classical vs. Deep Learning**: Both approaches achieve good performance, with different trade-offs\n",
        "2. **Feature Importance**: Radial and angular distributions are key discriminators\n",
        "3. **Manufacturing Integration**: Pattern recognition enables automated decision-making\n",
        "4. **Cost Considerations**: False negatives (missed defects) are much more expensive than false positives\n",
        "\n",
        "### Production Recommendations\n",
        "\n",
        "1. **Start with Classical**: Lower complexity, good interpretability\n",
        "2. **Monitor Confidence**: Flag low-confidence predictions for human review\n",
        "3. **Pattern-Specific Thresholds**: Different confidence thresholds by defect type\n",
        "4. **Continuous Learning**: Update models as new patterns emerge\n",
        "\n",
        "### Future Enhancements\n",
        "\n",
        "1. **Real Dataset Integration**: Connect to WM-811K or proprietary wafer map data\n",
        "2. **Temporal Analysis**: Track pattern evolution across time/lots\n",
        "3. **Multi-Resolution**: Analyze patterns at different spatial scales\n",
        "4. **Active Learning**: Incorporate human feedback to improve models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final performance summary\n",
        "print(\"Final Model Performance Summary:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy: {classical_metrics['accuracy']:.3f}\")\n",
        "print(f\"F1-Score (Weighted): {classical_metrics['f1_weighted']:.3f}\")\n",
        "print(f\"ROC-AUC: {classical_metrics['roc_auc_weighted']:.3f}\")\n",
        "print(f\"PWS (Prediction Within Spec): {classical_metrics['pws']:.3f}\")\n",
        "print(f\"Estimated Loss: {classical_metrics['estimated_loss']:.3f}\")\n",
        "print(f\"Business Cost: ${total_cost} (${total_cost/len(y_test):.2f} per wafer)\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Model ready for production validation!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Validate on real fab data\")\n",
        "print(\"2. Set up production monitoring\")\n",
        "print(\"3. Train operators on new system\")\n",
        "print(\"4. Establish retraining schedule\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
