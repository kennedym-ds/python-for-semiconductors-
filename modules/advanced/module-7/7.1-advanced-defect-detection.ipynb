{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7.1: Advanced Defect Detection\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand object detection for semiconductor defect identification\n",
    "- Compare classical computer vision vs deep learning approaches\n",
    "- Implement YOLO, Faster R-CNN, and classical detection pipelines\n",
    "- Evaluate models using manufacturing-specific metrics\n",
    "- Deploy production-ready defect detection systems\n",
    "\n",
    "**Prerequisites:** Basic computer vision, neural networks, Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Data directory path resolution\n",
    "DATA_DIR = Path('../../../datasets').resolve()\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Defect Detection\n",
    "\n",
    "Defect detection in semiconductor manufacturing involves identifying and localizing anomalies on wafer surfaces. Common defects include:\n",
    "\n",
    "- **Scratches**: Linear defects from mechanical damage\n",
    "- **Particles**: Foreign matter contamination  \n",
    "- **Cracks**: Fracture patterns in the material\n",
    "\n",
    "Let's start by importing our pipeline components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our detection pipeline\n",
    "try:\n",
    "    from src.advanced_defect_detection_pipeline import (\n",
    "        AdvancedDefectDetectionPipeline,\n",
    "        generate_synthetic_wafer_defects,\n",
    "        ClassicalDetector,\n",
    "        calculate_iou,\n",
    "        evaluate_detection_metrics\n",
    "    )\n",
    "except ImportError:\n",
    "    # Direct import from pipeline file\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\n",
    "        \"pipeline\", \"7.1-advanced-defect-detection-pipeline.py\"\n",
    "    )\n",
    "    sys.modules[\"pipeline\"] = importlib.util.module_from_spec(spec)\n",
    "    pipeline = sys.modules[\"pipeline\"]\n",
    "    spec.loader.exec_module(pipeline)\n",
    "    \n",
    "    AdvancedDefectDetectionPipeline = pipeline.AdvancedDefectDetectionPipeline\n",
    "    generate_synthetic_wafer_defects = pipeline.generate_synthetic_wafer_defects\n",
    "    ClassicalDetector = pipeline.ClassicalDetector\n",
    "    calculate_iou = pipeline.calculate_iou\n",
    "    evaluate_detection_metrics = pipeline.evaluate_detection_metrics\n",
    "\n",
    "print(\"Pipeline imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Wafer Data Generation\n",
    "\n",
    "Since real wafer defect data is proprietary, we'll generate synthetic wafer images with known defects for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic wafer data\n",
    "print(\"Generating synthetic wafer defect data...\")\n",
    "images, annotations = generate_synthetic_wafer_defects(\n",
    "    n_images=10,\n",
    "    image_size=(400, 400),\n",
    "    n_defects_range=(2, 5),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(images)} images with annotations\")\n",
    "print(f\"Image shape: {images[0].shape}\")\n",
    "print(f\"Example annotations: {len(annotations[0])} defects in first image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic wafer images with defects\n",
    "def draw_bounding_boxes(image, annotations, title=\"Defect Detection\"):\n",
    "    \"\"\"Draw bounding boxes on image.\"\"\"\n",
    "    img_with_boxes = image.copy()\n",
    "    colors = {'scratch': (0, 255, 0), 'particle': (255, 0, 0), 'crack': (0, 0, 255)}\n",
    "    \n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']\n",
    "        class_name = ann['class']\n",
    "        color = colors.get(class_name, (255, 255, 255))\n",
    "        \n",
    "        cv2.rectangle(img_with_boxes, \n",
    "                     (int(bbox[0]), int(bbox[1])), \n",
    "                     (int(bbox[2]), int(bbox[3])), \n",
    "                     color, 2)\n",
    "        \n",
    "        # Add label\n",
    "        cv2.putText(img_with_boxes, class_name, \n",
    "                   (int(bbox[0]), int(bbox[1]-5)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    \n",
    "    return img_with_boxes\n",
    "\n",
    "# Display first few images with annotations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(6):\n",
    "    img_with_boxes = draw_bounding_boxes(images[i], annotations[i])\n",
    "    axes[i].imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "    axes[i].set_title(f\"Wafer {i+1}: {len(annotations[i])} defects\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Synthetic Wafer Images with Ground Truth Defects\", y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Print defect statistics\n",
    "defect_counts = {'scratch': 0, 'particle': 0, 'crack': 0}\n",
    "for ann_list in annotations:\n",
    "    for ann in ann_list:\n",
    "        defect_counts[ann['class']] += 1\n",
    "\n",
    "print(\"\\nDefect Distribution:\")\n",
    "for defect_type, count in defect_counts.items():\n",
    "    print(f\"{defect_type.capitalize()}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical Computer Vision Approach\n",
    "\n",
    "Let's start with a classical approach using OpenCV for defect detection. This method uses traditional image processing techniques without requiring training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classical detector\n",
    "classical_pipeline = AdvancedDefectDetectionPipeline(\n",
    "    backend='classical',\n",
    "    blur_kernel=5,\n",
    "    threshold_value=50\n",
    ")\n",
    "\n",
    "print(f\"Classical pipeline backend: {classical_pipeline.backend}\")\n",
    "print(f\"Detector type: {type(classical_pipeline.detector).__name__}\")\n",
    "\n",
    "# Fit the model (no-op for classical)\n",
    "classical_pipeline.fit(images, annotations)\n",
    "\n",
    "print(\"\\nClassical pipeline trained (no training required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with classical method\n",
    "print(\"Making predictions with classical detector...\")\n",
    "start_time = time.time()\n",
    "classical_predictions = classical_pipeline.predict(images[:5])\n",
    "classical_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Classical inference time: {classical_inference_time:.3f}s for 5 images\")\n",
    "print(f\"Average time per image: {classical_inference_time/5:.3f}s\")\n",
    "\n",
    "# Visualize classical predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for i in range(5):\n",
    "    # Ground truth\n",
    "    gt_img = draw_bounding_boxes(images[i], annotations[i])\n",
    "    axes[0, i].imshow(cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, i].set_title(f\"Ground Truth ({len(annotations[i])} defects)\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    pred_img = draw_bounding_boxes(images[i], classical_predictions[i])\n",
    "    axes[1, i].imshow(cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, i].set_title(f\"Classical Predictions ({len(classical_predictions[i])} defects)\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Classical Detection Results\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classical performance\n",
    "classical_metrics = classical_pipeline.evaluate(\n",
    "    images[:5], \n",
    "    annotations[:5], \n",
    "    iou_threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"Classical Detection Performance:\")\n",
    "print(f\"Precision: {classical_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {classical_metrics['recall']:.3f}\")\n",
    "print(f\"F1-Score: {classical_metrics['f1_score']:.3f}\")\n",
    "print(f\"mAP@0.5: {classical_metrics['map_50']:.3f}\")\n",
    "print(f\"PWS: {classical_metrics['pws_percent']:.1f}%\")\n",
    "print(f\"Estimated Loss: ${classical_metrics['estimated_loss_usd']:.0f}\")\n",
    "print(f\"True Positives: {classical_metrics['true_positives']}\")\n",
    "print(f\"False Positives: {classical_metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {classical_metrics['false_negatives']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Approaches\n",
    "\n",
    "Now let's explore deep learning approaches. We'll check for available backends and use them if possible, otherwise fall back gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available backends\n",
    "backend_availability = {}\n",
    "\n",
    "# Check YOLO (ultralytics)\n",
    "try:\n",
    "    import ultralytics\n",
    "    backend_availability['yolo'] = True\n",
    "    print(\"✓ YOLO (ultralytics) available\")\n",
    "except ImportError:\n",
    "    backend_availability['yolo'] = False\n",
    "    print(\"✗ YOLO (ultralytics) not available\")\n",
    "\n",
    "# Check Faster R-CNN (torchvision)\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    backend_availability['fasterrcnn'] = True\n",
    "    print(\"✓ Faster R-CNN (torchvision) available\")\n",
    "except ImportError:\n",
    "    backend_availability['fasterrcnn'] = False\n",
    "    print(\"✗ Faster R-CNN (torchvision) not available\")\n",
    "\n",
    "backend_availability['classical'] = True\n",
    "print(\"✓ Classical (OpenCV) always available\")\n",
    "\n",
    "print(f\"\\nAvailable backends: {[k for k, v in backend_availability.items() if v]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different backends\n",
    "backend_results = {}\n",
    "inference_times = {}\n",
    "\n",
    "# Test each available backend\n",
    "for backend_name in ['classical', 'yolo', 'fasterrcnn']:\n",
    "    if backend_name == 'classical' or backend_availability.get(backend_name, False):\n",
    "        print(f\"\\nTesting {backend_name.upper()} backend...\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize pipeline\n",
    "            pipeline = AdvancedDefectDetectionPipeline(backend=backend_name)\n",
    "            print(f\"Actual backend used: {pipeline.backend}\")\n",
    "            \n",
    "            # Train (quick for demo)\n",
    "            pipeline.fit(images[:3], annotations[:3], epochs=1)\n",
    "            \n",
    "            # Evaluate\n",
    "            start_time = time.time()\n",
    "            metrics = pipeline.evaluate(images[3:6], annotations[3:6])\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            backend_results[pipeline.backend] = metrics\n",
    "            inference_times[pipeline.backend] = inference_time\n",
    "            \n",
    "            print(f\"✓ {pipeline.backend} completed successfully\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "            print(f\"  mAP@0.5: {metrics['map_50']:.3f}\")\n",
    "            print(f\"  Inference time: {inference_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {backend_name} failed: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping {backend_name.upper()} backend (not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize backend comparison\n",
    "if len(backend_results) > 1:\n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame(backend_results).T\n",
    "    comparison_df['inference_time'] = pd.Series(inference_times)\n",
    "    \n",
    "    print(\"\\nBackend Comparison:\")\n",
    "    display_cols = ['precision', 'recall', 'f1_score', 'map_50', 'pws_percent', 'inference_time']\n",
    "    print(comparison_df[display_cols].round(3))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy metrics\n",
    "    metrics_to_plot = ['precision', 'recall', 'f1_score', 'map_50']\n",
    "    x_pos = np.arange(len(comparison_df))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        axes[0].bar(x_pos + i*width, comparison_df[metric], width, \n",
    "                   label=metric.replace('_', ' ').title())\n",
    "    \n",
    "    axes[0].set_xlabel('Backend')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('Detection Accuracy Metrics')\n",
    "    axes[0].set_xticks(x_pos + width * 1.5)\n",
    "    axes[0].set_xticklabels(comparison_df.index)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # PWS percentage\n",
    "    axes[1].bar(comparison_df.index, comparison_df['pws_percent'])\n",
    "    axes[1].set_ylabel('PWS (%)')\n",
    "    axes[1].set_title('Prediction Within Spec')\n",
    "    axes[1].set_ylim(0, 100)\n",
    "    \n",
    "    # Inference time\n",
    "    axes[2].bar(comparison_df.index, comparison_df['inference_time'])\n",
    "    axes[2].set_ylabel('Time (seconds)')\n",
    "    axes[2].set_title('Inference Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nOnly one backend available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Analysis\n",
    "\n",
    "Let's dive deeper into model evaluation with confusion matrices and detailed error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed error analysis\n",
    "def analyze_detection_errors(predictions, ground_truth, iou_threshold=0.5):\n",
    "    \"\"\"Analyze detection errors in detail.\"\"\"\n",
    "    total_gt = sum(len(gt) for gt in ground_truth)\n",
    "    total_pred = sum(len(pred) for pred in predictions)\n",
    "    \n",
    "    class_stats = {'scratch': {'tp': 0, 'fp': 0, 'fn': 0},\n",
    "                  'particle': {'tp': 0, 'fp': 0, 'fn': 0},\n",
    "                  'crack': {'tp': 0, 'fp': 0, 'fn': 0}}\n",
    "    \n",
    "    for pred_list, gt_list in zip(predictions, ground_truth):\n",
    "        gt_matched = [False] * len(gt_list)\n",
    "        \n",
    "        # Match predictions to ground truth\n",
    "        for pred in pred_list:\n",
    "            pred_bbox = pred['bbox']\n",
    "            pred_class = pred['class']\n",
    "            best_iou = 0.0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, gt in enumerate(gt_list):\n",
    "                if gt_matched[gt_idx]:\n",
    "                    continue\n",
    "                \n",
    "                gt_bbox = gt['bbox']\n",
    "                gt_class = gt['class']\n",
    "                iou = calculate_iou(pred_bbox, gt_bbox)\n",
    "                \n",
    "                if iou > best_iou and pred_class == gt_class:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = gt_idx\n",
    "            \n",
    "            if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "                class_stats[pred_class]['tp'] += 1\n",
    "                gt_matched[best_gt_idx] = True\n",
    "            else:\n",
    "                class_stats[pred_class]['fp'] += 1\n",
    "        \n",
    "        # Count missed detections\n",
    "        for gt_idx, gt in enumerate(gt_list):\n",
    "            if not gt_matched[gt_idx]:\n",
    "                class_stats[gt['class']]['fn'] += 1\n",
    "    \n",
    "    return class_stats, total_gt, total_pred\n",
    "\n",
    "# Analyze classical detector errors\n",
    "class_stats, total_gt, total_pred = analyze_detection_errors(\n",
    "    classical_predictions, annotations[:5]\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Performance Analysis:\")\n",
    "print(f\"{'Class':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for class_name, stats in class_stats.items():\n",
    "    tp, fp, fn = stats['tp'], stats['fp'], stats['fn']\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    print(f\"{class_name:<10} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f}\")\n",
    "\n",
    "print(f\"\\nTotal Ground Truth Objects: {total_gt}\")\n",
    "print(f\"Total Predicted Objects: {total_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU threshold sensitivity analysis\n",
    "iou_thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "threshold_results = []\n",
    "\n",
    "print(\"IoU Threshold Sensitivity Analysis:\")\n",
    "print(f\"{'IoU Threshold':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for threshold in iou_thresholds:\n",
    "    metrics = evaluate_detection_metrics(\n",
    "        classical_predictions, annotations[:5], iou_threshold=threshold\n",
    "    )\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1_score': metrics['f1_score']\n",
    "    })\n",
    "    \n",
    "    print(f\"{threshold:<15} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} {metrics['f1_score']:<10.3f}\")\n",
    "\n",
    "# Plot threshold sensitivity\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(threshold_df['threshold'], threshold_df['precision'], 'o-', label='Precision', linewidth=2)\n",
    "plt.plot(threshold_df['threshold'], threshold_df['recall'], 's-', label='Recall', linewidth=2)\n",
    "plt.plot(threshold_df['threshold'], threshold_df['f1_score'], '^-', label='F1-Score', linewidth=2)\n",
    "\n",
    "plt.xlabel('IoU Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Detection Performance vs IoU Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost-Sensitive Analysis\n",
    "\n",
    "In semiconductor manufacturing, different types of errors have different costs. Let's analyze the economic impact of our detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost sensitivity analysis\n",
    "def cost_analysis(metrics, defect_costs, false_alarm_costs):\n",
    "    \"\"\"Analyze costs under different cost structures.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for defect_cost in defect_costs:\n",
    "        for false_alarm_cost in false_alarm_costs:\n",
    "            estimated_loss = (\n",
    "                metrics['false_negatives'] * defect_cost + \n",
    "                metrics['false_positives'] * false_alarm_cost\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'defect_cost': defect_cost,\n",
    "                'false_alarm_cost': false_alarm_cost,\n",
    "                'total_loss': estimated_loss,\n",
    "                'fn_loss': metrics['false_negatives'] * defect_cost,\n",
    "                'fp_loss': metrics['false_positives'] * false_alarm_cost\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Different cost scenarios\n",
    "defect_costs = [500, 1000, 2000, 5000]  # Cost per missed defect\n",
    "false_alarm_costs = [50, 100, 200, 500]  # Cost per false alarm\n",
    "\n",
    "cost_results = cost_analysis(classical_metrics, defect_costs, false_alarm_costs)\n",
    "cost_df = pd.DataFrame(cost_results)\n",
    "\n",
    "# Create cost heatmap\n",
    "cost_pivot = cost_df.pivot(index='defect_cost', columns='false_alarm_cost', values='total_loss')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create heatmap manually since seaborn might not be available\n",
    "im = plt.imshow(cost_pivot.values, cmap='Reds', aspect='auto')\n",
    "\n",
    "# Add labels\n",
    "plt.xticks(range(len(cost_pivot.columns)), cost_pivot.columns)\n",
    "plt.yticks(range(len(cost_pivot.index)), cost_pivot.index)\n",
    "plt.xlabel('False Alarm Cost ($)')\n",
    "plt.ylabel('Defect Miss Cost ($)')\n",
    "plt.title('Total Estimated Loss Under Different Cost Structures')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(cost_pivot.index)):\n",
    "    for j in range(len(cost_pivot.columns)):\n",
    "        text = plt.text(j, i, f'${cost_pivot.iloc[i, j]:.0f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\" if cost_pivot.iloc[i, j] < cost_pivot.values.max()/2 else \"white\")\n",
    "\n",
    "plt.colorbar(im, label='Total Loss ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCost Analysis Summary:\")\n",
    "print(f\"Current False Negatives: {classical_metrics['false_negatives']}\")\n",
    "print(f\"Current False Positives: {classical_metrics['false_positives']}\")\n",
    "print(f\"\\nMinimum Total Loss: ${cost_df['total_loss'].min():.0f}\")\n",
    "print(f\"Maximum Total Loss: ${cost_df['total_loss'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Deployment Considerations\n",
    "\n",
    "Let's explore practical considerations for deploying defect detection in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "def benchmark_inference_speed(pipeline, test_images, n_runs=5):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        start_time = time.time()\n",
    "        predictions = pipeline.predict(test_images)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'images_per_second': len(test_images) / np.mean(times),\n",
    "        'ms_per_image': (np.mean(times) / len(test_images)) * 1000\n",
    "    }\n",
    "\n",
    "# Benchmark classical detector\n",
    "test_images = images[:3]\n",
    "speed_results = benchmark_inference_speed(classical_pipeline, test_images)\n",
    "\n",
    "print(\"Production Speed Benchmark:\")\n",
    "print(f\"Mean inference time: {speed_results['mean_time']:.3f} ± {speed_results['std_time']:.3f} seconds\")\n",
    "print(f\"Images per second: {speed_results['images_per_second']:.1f}\")\n",
    "print(f\"Milliseconds per image: {speed_results['ms_per_image']:.1f} ms\")\n",
    "\n",
    "# Production throughput requirements\n",
    "wafers_per_hour = [100, 500, 1000, 2000]\n",
    "images_per_wafer = 10  # Assume 10 images per wafer\n",
    "\n",
    "print(\"\\nProduction Throughput Analysis:\")\n",
    "print(f\"{'Wafers/Hour':<12} {'Images/Hour':<12} {'Required ms/Image':<18} {'Can Handle?':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for wph in wafers_per_hour:\n",
    "    images_per_hour = wph * images_per_wafer\n",
    "    required_ms_per_image = (3600 * 1000) / images_per_hour  # ms per image\n",
    "    can_handle = \"✓\" if speed_results['ms_per_image'] <= required_ms_per_image else \"✗\"\n",
    "    \n",
    "    print(f\"{wph:<12} {images_per_hour:<12} {required_ms_per_image:<18.1f} {can_handle:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence and deployment\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Save and reload model\n",
    "with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as f:\n",
    "    model_path = f.name\n",
    "\n",
    "try:\n",
    "    # Save model\n",
    "    classical_pipeline.save(Path(model_path))\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Model file size: {os.path.getsize(model_path)} bytes\")\n",
    "    \n",
    "    # Load model\n",
    "    loaded_pipeline = AdvancedDefectDetectionPipeline.load(Path(model_path))\n",
    "    print(f\"Model loaded successfully\")\n",
    "    print(f\"Loaded backend: {loaded_pipeline.backend}\")\n",
    "    \n",
    "    # Verify loaded model works\n",
    "    test_predictions = loaded_pipeline.predict(test_images[:1])\n",
    "    print(f\"Loaded model predictions: {len(test_predictions[0])} detections\")\n",
    "    \n",
    "    # Check metadata\n",
    "    if loaded_pipeline.metadata:\n",
    "        print(f\"\\nModel Metadata:\")\n",
    "        print(f\"Trained at: {loaded_pipeline.metadata.trained_at}\")\n",
    "        print(f\"Backend: {loaded_pipeline.metadata.backend}\")\n",
    "        print(f\"Class names: {loaded_pipeline.metadata.class_names}\")\n",
    "\nfinally:\n",
    "    # Clean up\n",
    "    if os.path.exists(model_path):\n",
    "        os.unlink(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've learned and discuss next steps for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"Module 7.1 Summary: Advanced Defect Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Synthetic Data Generation:\")\n",
    "print(f\"   - Generated {len(images)} synthetic wafer images\")\n",
    "print(f\"   - Three defect types: scratches, particles, cracks\")\n",
    "print(f\"   - Realistic wafer appearance with bounding box annotations\")\n",
    "\n",
    "print(\"\\n2. Classical Detection Performance:\")\n",
    "print(f\"   - Precision: {classical_metrics['precision']:.3f}\")\n",
    "print(f\"   - Recall: {classical_metrics['recall']:.3f}\")\n",
    "print(f\"   - mAP@0.5: {classical_metrics['map_50']:.3f}\")\n",
    "print(f\"   - PWS: {classical_metrics['pws_percent']:.1f}%\")\n",
    "print(f\"   - Inference speed: {speed_results['ms_per_image']:.1f} ms/image\")\n",
    "\n",
    "print(\"\\n3. Backend Availability:\")\n",
    "for backend, available in backend_availability.items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    print(f\"   {status} {backend.upper()}\")\n",
    "\n",
    "print(\"\\n4. Key Insights:\")\n",
    "print(\"   - Classical methods provide fast, CPU-friendly baseline\")\n",
    "print(\"   - Deep learning offers higher accuracy but requires training data\")\n",
    "print(\"   - Cost-sensitive metrics essential for manufacturing context\")\n",
    "print(\"   - Model persistence enables production deployment\")\n",
    "\n",
    "print(\"\\n5. Next Steps for Production:\")\n",
    "print(\"   - Collect real wafer defect data for training\")\n",
    "print(\"   - Implement A/B testing framework\")\n",
    "print(\"   - Set up continuous monitoring and retraining\")\n",
    "print(\"   - Optimize models for target hardware (CPU/GPU/Edge)\")\n",
    "print(\"   - Integrate with manufacturing execution systems\")\n",
    "\n",
    "print(\"\\n6. Additional Resources:\")\n",
    "print(\"   - Fundamentals: 7.1-advanced-defect-detection-fundamentals.md\")\n",
    "print(\"   - Quick Reference: 7.1-advanced-defect-detection-quick-ref.md\")\n",
    "print(\"   - Production Pipeline: 7.1-advanced-defect-detection-pipeline.py\")\n",
    "print(\"   - Tests: test_advanced_detection_pipeline.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hands-On Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Parameter Tuning\n",
    "Experiment with different classical detection parameters:\n",
    "- Try `blur_kernel` values: 3, 5, 7, 9\n",
    "- Try `threshold_value` values: 30, 50, 70, 90\n",
    "- Find the combination that maximizes F1-score\n",
    "\n",
    "### Exercise 2: Cost Optimization\n",
    "Given your facility's actual costs:\n",
    "- Missed defect cost: $2000\n",
    "- False alarm cost: $150\n",
    "- Determine optimal confidence threshold\n",
    "\n",
    "### Exercise 3: Real Data Integration\n",
    "If you have access to real wafer images:\n",
    "- Replace synthetic data with real images\n",
    "- Compare performance across different wafer types\n",
    "- Analyze failure modes and edge cases\n",
    "\n",
    "### Exercise 4: Production Deployment\n",
    "Design a production system:\n",
    "- Define SLA requirements (speed, accuracy)\n",
    "- Choose appropriate backend\n",
    "- Plan monitoring and alerting strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise workspace - modify and run your experiments here\n",
    "\n",
    "# Exercise 1: Parameter tuning example\n",
    "print(\"Exercise 1: Parameter Tuning\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = {}\n",
    "\n",
    "for blur_kernel in [3, 5, 7]:\n",
    "    for threshold_value in [30, 50, 70]:\n",
    "        # Create pipeline with specific parameters\n",
    "        test_pipeline = AdvancedDefectDetectionPipeline(\n",
    "            backend='classical',\n",
    "            blur_kernel=blur_kernel,\n",
    "            threshold_value=threshold_value\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate\n",
    "        test_pipeline.fit(images[:3], annotations[:3])\n",
    "        metrics = test_pipeline.evaluate(images[3:6], annotations[3:6])\n",
    "        \n",
    "        print(f\"Blur: {blur_kernel}, Threshold: {threshold_value} => F1: {metrics['f1_score']:.3f}\")\n",
    "        \n",
    "        if metrics['f1_score'] > best_f1:\n",
    "            best_f1 = metrics['f1_score']\n",
    "            best_params = {'blur_kernel': blur_kernel, 'threshold_value': threshold_value}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best F1-score: {best_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}