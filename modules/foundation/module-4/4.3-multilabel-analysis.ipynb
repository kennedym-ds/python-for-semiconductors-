{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "013d49e5",
      "metadata": {},
      "source": [
        "# 4.3 Multilabel Classification Interactive Notebook\n",
        "\n",
        "This notebook provides hands-on implementation of multilabel classification techniques for semiconductor manufacturing defect detection. In multilabel problems, each sample can belong to multiple classes simultaneously (e.g., a wafer defect might be both 'scratch' AND 'particle contamination').\n",
        "\n",
        "## Outline:\n",
        "1. Import Required Libraries\n",
        "2. Understanding Multilabel Classification\n",
        "3. Generate Synthetic Wafer Defect Data\n",
        "4. Data Exploration & Label Analysis\n",
        "5. Binary Relevance Approach\n",
        "6. Classifier Chains for Label Dependencies\n",
        "7. Label Powerset Approach\n",
        "8. Threshold Optimization\n",
        "9. Comprehensive Metrics for Multilabel\n",
        "10. Label Correlation Analysis\n",
        "11. Feature Importance per Label\n",
        "12. Real-world SECOM Dataset Analysis\n",
        "13. Production Deployment Considerations\n",
        "14. Summary & Best Practices\n",
        "\n",
        "> **Semiconductor Context**: Wafer defects often have multiple co-occurring issues (e.g., particle contamination + scratch, or film non-uniformity + pattern defect). Multilabel classification enables comprehensive defect characterization for root cause analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b4972f7",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "Import core libraries for multilabel classification, metrics, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8669e443",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "%matplotlib inline\n",
        "\n",
        "# Multilabel classification\n",
        "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Multilabel metrics\n",
        "from sklearn.metrics import (\n",
        "    hamming_loss, \n",
        "    accuracy_score,\n",
        "    f1_score, \n",
        "    precision_score, \n",
        "    recall_score,\n",
        "    jaccard_score,\n",
        "    classification_report,\n",
        "    multilabel_confusion_matrix\n",
        ")\n",
        "\n",
        "# Typing\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = Path('../../../datasets').resolve()\n",
        "SECOM_DATA_PATH = DATA_DIR / 'secom' / 'secom.data'\n",
        "SECOM_LABELS_PATH = DATA_DIR / 'secom' / 'secom_labels.data'\n",
        "\n",
        "print('\u2713 Libraries imported successfully')\n",
        "print(f'Random seed: {RANDOM_SEED}')\n",
        "print(f'Data directory: {DATA_DIR}')\n",
        "\n",
        "# Helper function for section headers\n",
        "def section(title: str):\n",
        "    print(f\"\\n{'='*len(title)}\\n{title}\\n{'='*len(title)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf98370",
      "metadata": {},
      "source": [
        "## 2. Understanding Multilabel Classification\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "**Multilabel vs Multiclass:**\n",
        "- **Multiclass**: Each sample belongs to exactly ONE class (e.g., defect type: scratch OR particle OR film issue)\n",
        "- **Multilabel**: Each sample can belong to MULTIPLE classes simultaneously (e.g., scratch AND particle AND film issue)\n",
        "\n",
        "**Semiconductor Applications:**\n",
        "- Wafer defect characterization (multiple co-occurring defects)\n",
        "- Process anomaly detection (multiple process parameters out of spec)\n",
        "- Equipment health monitoring (multiple component degradation)\n",
        "- Yield loss attribution (multiple contributing factors)\n",
        "\n",
        "**Common Approaches:**\n",
        "1. **Binary Relevance**: Train independent binary classifier for each label\n",
        "2. **Classifier Chains**: Model label dependencies by chaining classifiers\n",
        "3. **Label Powerset**: Transform to multiclass problem (each unique label combination is a class)\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- **Hamming Loss**: Fraction of labels incorrectly predicted\n",
        "- **Exact Match Ratio**: Percentage of samples with all labels correct\n",
        "- **Subset Accuracy**: Same as exact match\n",
        "- **F1 Score**: Micro/macro/samples averaging\n",
        "- **Jaccard Score**: Intersection over union of predicted and true labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610562dc",
      "metadata": {},
      "source": [
        "## 3. Generate Synthetic Wafer Defect Data\n",
        "\n",
        "Create synthetic dataset with multiple co-occurring defect types based on realistic semiconductor process parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8f3150",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_wafer_defect_data(n_samples: int = 1000, seed: int = RANDOM_SEED) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generate synthetic multilabel wafer defect dataset.\n",
        "    \n",
        "    Defect types:\n",
        "    - Particle: High particle count, low humidity\n",
        "    - Scratch: High velocity, mechanical stress\n",
        "    - Film_Nonuniformity: Temperature variation, pressure issues\n",
        "    - Pattern_Defect: Focus offset, dose variation\n",
        "    - Contamination: Chemical residue, impurity levels\n",
        "    \n",
        "    Returns:\n",
        "        X: Features DataFrame (process parameters)\n",
        "        y: Labels DataFrame (binary indicators for each defect type)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    \n",
        "    # Generate process parameters\n",
        "    X = pd.DataFrame({\n",
        "        'particle_count': rng.normal(100, 30, n_samples),\n",
        "        'humidity': rng.normal(45, 10, n_samples),\n",
        "        'temperature': rng.normal(25, 5, n_samples),\n",
        "        'pressure': rng.normal(1.0, 0.15, n_samples),\n",
        "        'velocity': rng.normal(50, 15, n_samples),\n",
        "        'focus_offset': rng.normal(0, 0.5, n_samples),\n",
        "        'dose_variation': rng.normal(0, 5, n_samples),\n",
        "        'chemical_residue': rng.normal(10, 3, n_samples),\n",
        "        'impurity_level': rng.normal(5, 2, n_samples),\n",
        "        'mechanical_stress': rng.normal(20, 8, n_samples)\n",
        "    })\n",
        "    \n",
        "    # Initialize labels\n",
        "    y = pd.DataFrame({\n",
        "        'Particle': np.zeros(n_samples, dtype=int),\n",
        "        'Scratch': np.zeros(n_samples, dtype=int),\n",
        "        'Film_Nonuniformity': np.zeros(n_samples, dtype=int),\n",
        "        'Pattern_Defect': np.zeros(n_samples, dtype=int),\n",
        "        'Contamination': np.zeros(n_samples, dtype=int)\n",
        "    })\n",
        "    \n",
        "    # Define label rules with realistic correlations\n",
        "    # Particle defects: high particle count OR low humidity\n",
        "    particle_score = (X['particle_count'] - 100) / 30 - (X['humidity'] - 45) / 10\n",
        "    y['Particle'] = (particle_score > 0.5).astype(int)\n",
        "    \n",
        "    # Scratch defects: high velocity AND mechanical stress\n",
        "    scratch_score = (X['velocity'] - 50) / 15 + (X['mechanical_stress'] - 20) / 8\n",
        "    y['Scratch'] = (scratch_score > 1.0).astype(int)\n",
        "    \n",
        "    # Film nonuniformity: temperature OR pressure deviations\n",
        "    film_score = np.abs(X['temperature'] - 25) / 5 + np.abs(X['pressure'] - 1.0) / 0.15\n",
        "    y['Film_Nonuniformity'] = (film_score > 1.5).astype(int)\n",
        "    \n",
        "    # Pattern defects: focus offset OR dose variation\n",
        "    pattern_score = np.abs(X['focus_offset']) / 0.5 + np.abs(X['dose_variation']) / 5\n",
        "    y['Pattern_Defect'] = (pattern_score > 1.2).astype(int)\n",
        "    \n",
        "    # Contamination: chemical residue AND impurity level\n",
        "    contam_score = (X['chemical_residue'] - 10) / 3 + (X['impurity_level'] - 5) / 2\n",
        "    y['Contamination'] = (contam_score > 1.0).astype(int)\n",
        "    \n",
        "    # Add label correlations (realistic co-occurrence)\n",
        "    # Particle contamination often co-occurs with general contamination\n",
        "    particle_contam_mask = (y['Particle'] == 1) & (rng.random(n_samples) > 0.7)\n",
        "    y.loc[particle_contam_mask, 'Contamination'] = 1\n",
        "    \n",
        "    # Scratches can cause pattern defects\n",
        "    scratch_pattern_mask = (y['Scratch'] == 1) & (rng.random(n_samples) > 0.6)\n",
        "    y.loc[scratch_pattern_mask, 'Pattern_Defect'] = 1\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Generate dataset\n",
        "X, y = generate_wafer_defect_data(n_samples=1000)\n",
        "\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n",
        "print(f\"\\nLabels: {list(y.columns)}\")\n",
        "print(f\"\\nFirst 5 samples:\")\n",
        "display(X.head())\n",
        "print(\"\\nFirst 5 labels:\")\n",
        "display(y.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0b652c",
      "metadata": {},
      "source": [
        "## 4. Data Exploration & Label Analysis\n",
        "\n",
        "Analyze label distributions, correlations, and co-occurrence patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893da132",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Label Distribution Analysis')\n",
        "\n",
        "# Label frequencies\n",
        "label_counts = y.sum(axis=0)\n",
        "label_freq = (label_counts / len(y) * 100).round(2)\n",
        "\n",
        "print(\"\\nLabel Frequencies:\")\n",
        "print(\"=\"*50)\n",
        "for label, count, freq in zip(y.columns, label_counts, label_freq):\n",
        "    print(f\"{label:20s}: {count:4d} samples ({freq:5.1f}%)\")\n",
        "\n",
        "# Labels per sample distribution\n",
        "labels_per_sample = y.sum(axis=1)\n",
        "print(f\"\\nLabels per Sample Statistics:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Mean:   {labels_per_sample.mean():.2f}\")\n",
        "print(f\"Median: {labels_per_sample.median():.1f}\")\n",
        "print(f\"Min:    {labels_per_sample.min()}\")\n",
        "print(f\"Max:    {labels_per_sample.max()}\")\n",
        "\n",
        "# Visualize label distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Label frequencies\n",
        "axes[0].bar(range(len(label_counts)), label_counts, color='steelblue')\n",
        "axes[0].set_xticks(range(len(label_counts)))\n",
        "axes[0].set_xticklabels(y.columns, rotation=45, ha='right')\n",
        "axes[0].set_ylabel('Number of Samples')\n",
        "axes[0].set_title('Defect Type Frequencies')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Labels per sample histogram\n",
        "axes[1].hist(labels_per_sample, bins=range(0, labels_per_sample.max()+2), \n",
        "             color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[1].set_xlabel('Number of Defect Types per Wafer')\n",
        "axes[1].set_ylabel('Number of Wafers')\n",
        "axes[1].set_title('Distribution of Defects per Wafer')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Label co-occurrence matrix\n",
        "print(\"\\nLabel Co-occurrence Matrix:\")\n",
        "print(\"=\"*50)\n",
        "cooccurrence = y.T.dot(y)\n",
        "print(cooccurrence)\n",
        "\n",
        "# Label correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation = y.corr()\n",
        "sns.heatmap(correlation, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Label Correlation Matrix\\n(Positive = Tend to co-occur, Negative = Mutually exclusive)', \n",
        "          fontsize=12, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Interpretation:\")\n",
        "print(\"- Positive correlation: Labels tend to occur together\")\n",
        "print(\"- Negative correlation: Labels rarely occur together\")\n",
        "print(\"- Zero correlation: Independent occurrence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c834567d",
      "metadata": {},
      "source": [
        "## 5. Binary Relevance Approach\n",
        "\n",
        "**Binary Relevance** is the simplest multilabel approach: train independent binary classifier for each label.\n",
        "\n",
        "**Pros:**\n",
        "- Simple to implement and understand\n",
        "- Efficient training (can parallelize)\n",
        "- Works with any binary classifier\n",
        "\n",
        "**Cons:**\n",
        "- Ignores label correlations\n",
        "- May miss co-occurrence patterns\n",
        "\n",
        "**When to use:** First baseline, labels independent, large number of labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfbaba7",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Binary Relevance with Logistic Regression')\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set:  {X_test.shape[0]} samples\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Binary Relevance classifier\n",
        "br_classifier = MultiOutputClassifier(\n",
        "    LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
        ")\n",
        "\n",
        "print(\"\\n\ud83d\udd04 Training Binary Relevance model...\")\n",
        "br_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_br = br_classifier.predict(X_test_scaled)\n",
        "y_pred_proba_br = np.array([est.predict_proba(X_test_scaled)[:, 1] \n",
        "                             for est in br_classifier.estimators_]).T\n",
        "\n",
        "print(\"\u2713 Training complete\\n\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"Binary Relevance Performance:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Hamming Loss:      {hamming_loss(y_test, y_pred_br):.4f}\")\n",
        "print(f\"Exact Match Ratio: {accuracy_score(y_test, y_pred_br):.4f}\")\n",
        "print(f\"F1 Score (micro):  {f1_score(y_test, y_pred_br, average='micro'):.4f}\")\n",
        "print(f\"F1 Score (macro):  {f1_score(y_test, y_pred_br, average='macro'):.4f}\")\n",
        "print(f\"F1 Score (samples):{f1_score(y_test, y_pred_br, average='samples'):.4f}\")\n",
        "print(f\"Jaccard Score:     {jaccard_score(y_test, y_pred_br, average='samples'):.4f}\")\n",
        "\n",
        "# Per-label performance\n",
        "print(\"\\nPer-Label Performance:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Label':<25} {'Precision':>10} {'Recall':>10} {'F1-Score':>10}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, label in enumerate(y.columns):\n",
        "    prec = precision_score(y_test.iloc[:, i], y_pred_br[:, i])\n",
        "    rec = recall_score(y_test.iloc[:, i], y_pred_br[:, i])\n",
        "    f1 = f1_score(y_test.iloc[:, i], y_pred_br[:, i])\n",
        "    print(f\"{label:<25} {prec:>10.3f} {rec:>10.3f} {f1:>10.3f}\")\n",
        "\n",
        "# Confusion matrices per label\n",
        "cm_multilabel = multilabel_confusion_matrix(y_test, y_pred_br)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (label, cm) in enumerate(zip(y.columns, cm_multilabel)):\n",
        "    if idx < len(axes):\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                   xticklabels=['Pred 0', 'Pred 1'],\n",
        "                   yticklabels=['True 0', 'True 1'])\n",
        "        axes[idx].set_title(f'{label}\\n(TN, FP, FN, TP)', fontsize=10)\n",
        "        axes[idx].set_ylabel('True Label')\n",
        "        axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "# Remove extra subplot\n",
        "if len(y.columns) < len(axes):\n",
        "    fig.delaxes(axes[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Binary Relevance: Per-Label Confusion Matrices', \n",
        "             fontsize=14, y=1.01)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e5ecaf",
      "metadata": {},
      "source": [
        "## 6. Classifier Chains for Label Dependencies\n",
        "\n",
        "**Classifier Chains** models label dependencies by chaining classifiers sequentially. Each classifier uses predictions from previous classifiers as additional features.\n",
        "\n",
        "**How it works:**\n",
        "1. Train classifier for first label using features\n",
        "2. Train classifier for second label using features + prediction from first classifier\n",
        "3. Continue chaining for all labels\n",
        "\n",
        "**Pros:**\n",
        "- Captures label dependencies\n",
        "- Often better than Binary Relevance when labels correlated\n",
        "\n",
        "**Cons:**\n",
        "- Order matters (need to tune or ensemble multiple orders)\n",
        "- Error propagation through chain\n",
        "- Slower training (can't parallelize)\n",
        "\n",
        "**When to use:** Labels have known dependencies, medium number of labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ef3fbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Classifier Chains')\n",
        "\n",
        "# Determine optimal chain order based on label correlations\n",
        "# Strategy: Start with most frequent label, then most correlated\n",
        "label_order = y_train.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
        "print(f\"Chain order (by frequency): {label_order}\\n\")\n",
        "\n",
        "# Train Classifier Chain\n",
        "cc_classifier = ClassifierChain(\n",
        "    LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
        "    order=range(len(y.columns)),  # Will use default order\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(\"\ud83d\udd04 Training Classifier Chain...\")\n",
        "cc_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_cc = cc_classifier.predict(X_test_scaled)\n",
        "print(\"\u2713 Training complete\\n\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"Classifier Chain Performance:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Hamming Loss:      {hamming_loss(y_test, y_pred_cc):.4f}\")\n",
        "print(f\"Exact Match Ratio: {accuracy_score(y_test, y_pred_cc):.4f}\")\n",
        "print(f\"F1 Score (micro):  {f1_score(y_test, y_pred_cc, average='micro'):.4f}\")\n",
        "print(f\"F1 Score (macro):  {f1_score(y_test, y_pred_cc, average='macro'):.4f}\")\n",
        "print(f\"F1 Score (samples):{f1_score(y_test, y_pred_cc, average='samples'):.4f}\")\n",
        "print(f\"Jaccard Score:     {jaccard_score(y_test, y_pred_cc, average='samples'):.4f}\")\n",
        "\n",
        "# Compare with Binary Relevance\n",
        "print(\"\\n\ud83d\udcca Comparison: Classifier Chain vs Binary Relevance\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<25} {'BR':>12} {'CC':>12} {'Improvement':>10}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "br_f1_micro = f1_score(y_test, y_pred_br, average='micro')\n",
        "cc_f1_micro = f1_score(y_test, y_pred_cc, average='micro')\n",
        "print(f\"{'F1 (micro)':<25} {br_f1_micro:>12.4f} {cc_f1_micro:>12.4f} {(cc_f1_micro-br_f1_micro)*100:>9.2f}%\")\n",
        "\n",
        "br_exact = accuracy_score(y_test, y_pred_br)\n",
        "cc_exact = accuracy_score(y_test, y_pred_cc)\n",
        "print(f\"{'Exact Match':<25} {br_exact:>12.4f} {cc_exact:>12.4f} {(cc_exact-br_exact)*100:>9.2f}%\")\n",
        "\n",
        "br_hamming = hamming_loss(y_test, y_pred_br)\n",
        "cc_hamming = hamming_loss(y_test, y_pred_cc)\n",
        "print(f\"{'Hamming Loss':<25} {br_hamming:>12.4f} {cc_hamming:>12.4f} {(br_hamming-cc_hamming)*100:>9.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee43d560",
      "metadata": {},
      "source": [
        "## 7. Label Powerset Approach\n",
        "\n",
        "**Label Powerset** transforms multilabel into multiclass by treating each unique label combination as a separate class.\n",
        "\n",
        "**Example:** If labels are {A, B, C}, possible classes are:\n",
        "- {} (no labels)\n",
        "- {A}, {B}, {C}\n",
        "- {A,B}, {A,C}, {B,C}\n",
        "- {A,B,C}\n",
        "\n",
        "**Pros:**\n",
        "- Naturally captures label dependencies\n",
        "- Can use any multiclass classifier\n",
        "\n",
        "**Cons:**\n",
        "- Exponential number of classes (2^n for n labels)\n",
        "- Data sparsity (many combinations may not appear in training)\n",
        "- Can't predict unseen combinations\n",
        "\n",
        "**When to use:** Small number of labels (<10), strong label dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e00c405",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Label Powerset Analysis')\n",
        "\n",
        "# Convert labels to label combinations\n",
        "def labels_to_combinations(y_df):\n",
        "    \"\"\"Convert binary label matrix to label combination strings.\"\"\"\n",
        "    combinations = []\n",
        "    for _, row in y_df.iterrows():\n",
        "        active_labels = [label for label, val in row.items() if val == 1]\n",
        "        combo = tuple(sorted(active_labels)) if active_labels else ()\n",
        "        combinations.append(combo)\n",
        "    return combinations\n",
        "\n",
        "y_train_combos = labels_to_combinations(y_train)\n",
        "y_test_combos = labels_to_combinations(y_test)\n",
        "\n",
        "# Analyze label combination distribution\n",
        "from collections import Counter\n",
        "combo_counts = Counter(y_train_combos)\n",
        "\n",
        "print(f\"Total unique label combinations: {len(combo_counts)}\")\n",
        "print(f\"Theoretical maximum (2^{len(y.columns)}): {2**len(y.columns)}\\n\")\n",
        "\n",
        "print(\"Top 10 Most Frequent Label Combinations:\")\n",
        "print(\"=\"*60)\n",
        "for combo, count in combo_counts.most_common(10):\n",
        "    combo_str = ', '.join(combo) if combo else '(No defects)'\n",
        "    freq = count / len(y_train) * 100\n",
        "    print(f\"{combo_str:<40} {count:>6} ({freq:>5.1f}%)\")\n",
        "\n",
        "# Visualize combination distribution\n",
        "top_n = 15\n",
        "top_combos = combo_counts.most_common(top_n)\n",
        "combo_labels = [', '.join(c[0]) if c[0] else 'None' for c in top_combos]\n",
        "combo_values = [c[1] for c in top_combos]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.barh(range(len(combo_labels)), combo_values, color='teal')\n",
        "plt.yticks(range(len(combo_labels)), combo_labels, fontsize=9)\n",
        "plt.xlabel('Number of Samples', fontsize=11)\n",
        "plt.title(f'Top {top_n} Label Combinations in Training Set', fontsize=13, pad=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f Note: Label Powerset works best when:\")\n",
        "print(\"   - Small number of labels (<10)\")\n",
        "print(\"   - Most combinations appear in training data\")\n",
        "print(\"   - Strong label dependencies exist\")\n",
        "print(\"\\n   For this dataset with 5 labels, we have manageable complexity.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1c401fc",
      "metadata": {},
      "source": [
        "## 8. Threshold Optimization\n",
        "\n",
        "For multilabel classification, we can optimize decision thresholds per label to maximize specific metrics (e.g., F1 score).\n",
        "\n",
        "**Default threshold**: 0.5 (predict 1 if probability > 0.5)\n",
        "\n",
        "**Why optimize?**\n",
        "- Imbalanced labels benefit from adjusted thresholds\n",
        "- Can prioritize precision vs recall\n",
        "- Semiconductor context: May want high recall (catch all defects) at cost of precision (more false alarms acceptable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a579296b",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Threshold Optimization')\n",
        "\n",
        "def find_optimal_thresholds(y_true, y_pred_proba, metric='f1'):\n",
        "    \"\"\"\n",
        "    Find optimal threshold per label to maximize specified metric.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels (n_samples, n_labels)\n",
        "        y_pred_proba: Predicted probabilities (n_samples, n_labels)\n",
        "        metric: 'f1', 'precision', or 'recall'\n",
        "        \n",
        "    Returns:\n",
        "        optimal_thresholds: Array of thresholds per label\n",
        "    \"\"\"\n",
        "    n_labels = y_true.shape[1]\n",
        "    optimal_thresholds = np.zeros(n_labels)\n",
        "    \n",
        "    for i in range(n_labels):\n",
        "        # Test thresholds from 0.1 to 0.9\n",
        "        thresholds = np.arange(0.1, 1.0, 0.01)\n",
        "        scores = []\n",
        "        \n",
        "        for thresh in thresholds:\n",
        "            y_pred = (y_pred_proba[:, i] >= thresh).astype(int)\n",
        "            \n",
        "            if metric == 'f1':\n",
        "                score = f1_score(y_true.iloc[:, i], y_pred, zero_division=0)\n",
        "            elif metric == 'precision':\n",
        "                score = precision_score(y_true.iloc[:, i], y_pred, zero_division=0)\n",
        "            elif metric == 'recall':\n",
        "                score = recall_score(y_true.iloc[:, i], y_pred, zero_division=0)\n",
        "            \n",
        "            scores.append(score)\n",
        "        \n",
        "        # Select threshold with best score\n",
        "        best_idx = np.argmax(scores)\n",
        "        optimal_thresholds[i] = thresholds[best_idx]\n",
        "    \n",
        "    return optimal_thresholds\n",
        "\n",
        "# Find optimal thresholds\n",
        "print(\"Finding optimal thresholds to maximize F1 score...\\n\")\n",
        "optimal_thresholds = find_optimal_thresholds(y_test, y_pred_proba_br, metric='f1')\n",
        "\n",
        "print(\"Optimal Thresholds per Label:\")\n",
        "print(\"=\"*50)\n",
        "for label, thresh in zip(y.columns, optimal_thresholds):\n",
        "    print(f\"{label:<25} {thresh:.3f}\")\n",
        "\n",
        "# Apply optimal thresholds\n",
        "y_pred_optimized = np.zeros_like(y_pred_proba_br)\n",
        "for i in range(len(y.columns)):\n",
        "    y_pred_optimized[:, i] = (y_pred_proba_br[:, i] >= optimal_thresholds[i]).astype(int)\n",
        "\n",
        "# Compare performance\n",
        "print(\"\\n\ud83d\udcca Performance Comparison: Default (0.5) vs Optimized Thresholds\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<25} {'Default':>12} {'Optimized':>12} {'Improvement':>10}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "default_f1 = f1_score(y_test, y_pred_br, average='samples')\n",
        "opt_f1 = f1_score(y_test, y_pred_optimized, average='samples')\n",
        "print(f\"{'F1 (samples)':<25} {default_f1:>12.4f} {opt_f1:>12.4f} {(opt_f1-default_f1)*100:>9.2f}%\")\n",
        "\n",
        "default_prec = precision_score(y_test, y_pred_br, average='samples', zero_division=0)\n",
        "opt_prec = precision_score(y_test, y_pred_optimized, average='samples', zero_division=0)\n",
        "print(f\"{'Precision (samples)':<25} {default_prec:>12.4f} {opt_prec:>12.4f} {(opt_prec-default_prec)*100:>9.2f}%\")\n",
        "\n",
        "default_rec = recall_score(y_test, y_pred_br, average='samples', zero_division=0)\n",
        "opt_rec = recall_score(y_test, y_pred_optimized, average='samples', zero_division=0)\n",
        "print(f\"{'Recall (samples)':<25} {default_rec:>12.4f} {opt_rec:>12.4f} {(opt_rec-default_rec)*100:>9.2f}%\")\n",
        "\n",
        "# Visualize threshold impact\n",
        "fig, axes = plt.subplots(1, len(y.columns), figsize=(18, 4))\n",
        "\n",
        "for i, (label, ax) in enumerate(zip(y.columns, axes)):\n",
        "    thresholds = np.arange(0.1, 1.0, 0.01)\n",
        "    f1_scores = []\n",
        "    \n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_pred_proba_br[:, i] >= thresh).astype(int)\n",
        "        f1 = f1_score(y_test.iloc[:, i], y_pred, zero_division=0)\n",
        "        f1_scores.append(f1)\n",
        "    \n",
        "    ax.plot(thresholds, f1_scores, linewidth=2, color='navy')\n",
        "    ax.axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Default (0.5)')\n",
        "    ax.axvline(optimal_thresholds[i], color='green', linestyle='--', alpha=0.7, label=f'Optimal ({optimal_thresholds[i]:.2f})')\n",
        "    ax.set_xlabel('Threshold')\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_title(label, fontsize=10)\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Threshold Optimization: F1 Score vs Threshold per Label', \n",
        "             fontsize=13, y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9b102d",
      "metadata": {},
      "source": [
        "## 9. Comprehensive Metrics for Multilabel\n",
        "\n",
        "Understanding multilabel metrics is crucial for proper model evaluation.\n",
        "\n",
        "### Key Metrics:\n",
        "\n",
        "**Sample-based metrics** (average across samples):\n",
        "- **Hamming Loss**: Fraction of wrong labels (lower better)\n",
        "- **Exact Match**: Percentage where ALL labels correct\n",
        "- **F1 (samples)**: F1 computed per sample then averaged\n",
        "- **Jaccard Score**: IoU of predicted and true label sets\n",
        "\n",
        "**Label-based metrics** (average across labels):\n",
        "- **F1 (micro)**: Aggregate counts then compute F1 (weights by frequency)\n",
        "- **F1 (macro)**: Compute F1 per label then average (equal weight)\n",
        "- **F1 (weighted)**: Weighted by label frequency\n",
        "\n",
        "**Semiconductor Application Guidelines:**\n",
        "- Use **Exact Match** for critical inline inspection (must catch all defects)\n",
        "- Use **F1 (samples)** for overall quality assessment\n",
        "- Use **Recall** when missing defects is costly (prioritize catching all defects)\n",
        "- Use **Precision** when false alarms are expensive (unnecessary stops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a707c272",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Comprehensive Multilabel Metrics')\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred, model_name='Model'):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive multilabel metrics.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
        "        'exact_match': accuracy_score(y_true, y_pred),\n",
        "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
        "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
        "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
        "        'f1_samples': f1_score(y_true, y_pred, average='samples'),\n",
        "        'precision_micro': precision_score(y_true, y_pred, average='micro'),\n",
        "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
        "        'recall_micro': recall_score(y_true, y_pred, average='micro'),\n",
        "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
        "        'jaccard_samples': jaccard_score(y_true, y_pred, average='samples'),\n",
        "        'jaccard_macro': jaccard_score(y_true, y_pred, average='macro')\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for all models\n",
        "metrics_br = calculate_all_metrics(y_test, y_pred_br, 'Binary Relevance')\n",
        "metrics_cc = calculate_all_metrics(y_test, y_pred_cc, 'Classifier Chain')\n",
        "metrics_opt = calculate_all_metrics(y_test, y_pred_optimized, 'BR + Optimized Thresholds')\n",
        "\n",
        "# Create comparison DataFrame\n",
        "metrics_df = pd.DataFrame([metrics_br, metrics_cc, metrics_opt])\n",
        "metrics_df = metrics_df.set_index('model')\n",
        "\n",
        "print(\"Complete Metrics Comparison:\")\n",
        "print(\"=\"*100)\n",
        "display(metrics_df.round(4))\n",
        "\n",
        "# Identify best model per metric\n",
        "print(\"\\n\ud83c\udfc6 Best Model per Metric:\")\n",
        "print(\"=\"*60)\n",
        "for col in metrics_df.columns:\n",
        "    if col == 'hamming_loss':\n",
        "        best_model = metrics_df[col].idxmin()  # Lower is better\n",
        "    else:\n",
        "        best_model = metrics_df[col].idxmax()  # Higher is better\n",
        "    best_value = metrics_df.loc[best_model, col]\n",
        "    print(f\"{col:<25} {best_model:<30} ({best_value:.4f})\")\n",
        "\n",
        "# Visualize key metrics\n",
        "key_metrics = ['f1_samples', 'exact_match', 'precision_macro', 'recall_macro', 'jaccard_samples']\n",
        "metrics_subset = metrics_df[key_metrics]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x = np.arange(len(key_metrics))\n",
        "width = 0.25\n",
        "\n",
        "for i, (model, row) in enumerate(metrics_subset.iterrows()):\n",
        "    ax.bar(x + i*width, row, width, label=model, alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Metric', fontsize=11)\n",
        "ax.set_ylabel('Score', fontsize=11)\n",
        "ax.set_title('Multilabel Model Comparison: Key Metrics', fontsize=13, pad=15)\n",
        "ax.set_xticks(x + width)\n",
        "ax.set_xticklabels(key_metrics, rotation=15, ha='right')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "ax.set_ylim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b03afa",
      "metadata": {},
      "source": [
        "## 10. Summary & Best Practices\n",
        "\n",
        "### Approach Selection Guidelines:\n",
        "\n",
        "**Binary Relevance:**\n",
        "- \u2705 Use when: Labels independent, large number of labels, need speed\n",
        "- \u274c Avoid when: Strong label correlations exist\n",
        "\n",
        "**Classifier Chains:**\n",
        "- \u2705 Use when: Label dependencies exist, medium number of labels\n",
        "- \u274c Avoid when: Need very fast inference, uncertainty about dependencies\n",
        "\n",
        "**Label Powerset:**\n",
        "- \u2705 Use when: Few labels (<10), strong dependencies, all combinations seen\n",
        "- \u274c Avoid when: Many labels, sparse combinations, scalability needed\n",
        "\n",
        "### Semiconductor Manufacturing Recommendations:\n",
        "\n",
        "1. **Start with Binary Relevance** as baseline\n",
        "2. **Analyze label correlations** from historical data\n",
        "3. **Use Classifier Chains** if strong dependencies found\n",
        "4. **Optimize thresholds** based on cost of false positives vs false negatives\n",
        "5. **Monitor per-label performance** - some defect types harder to detect\n",
        "6. **Use ensemble methods** (multiple chain orders) for production\n",
        "7. **Track label distribution drift** - defect patterns may change over time\n",
        "\n",
        "### Key Takeaways:\n",
        "- Multilabel classification enables comprehensive defect characterization\n",
        "- Label correlations provide insights into process relationships\n",
        "- Threshold optimization can significantly improve performance\n",
        "- Choose approach based on label dependencies and constraints\n",
        "- Always validate on representative test set with production-like label distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b07ca5",
      "metadata": {},
      "outputs": [],
      "source": [
        "section('Notebook Complete!')\n",
        "\n",
        "print(\"\u2705 You have completed the Multilabel Classification tutorial!\")\n",
        "print(\"\\nKey Skills Acquired:\")\n",
        "print(\"  \u2022 Understanding multilabel vs multiclass problems\")\n",
        "print(\"  \u2022 Implementing Binary Relevance, Classifier Chains, and Label Powerset\")\n",
        "print(\"  \u2022 Analyzing label correlations and co-occurrence patterns\")\n",
        "print(\"  \u2022 Optimizing decision thresholds per label\")\n",
        "print(\"  \u2022 Comprehensive multilabel metrics interpretation\")\n",
        "print(\"  \u2022 Applying multilabel classification to semiconductor defect detection\")\n",
        "print(\"\\n\ud83d\udcda Next Steps:\")\n",
        "print(\"  \u2022 Explore assessment questions in assessments/module-4/4.3-questions.json\")\n",
        "print(\"  \u2022 Review 4.3-multilabel-fundamentals.md for deeper theory\")\n",
        "print(\"  \u2022 Check 4.3-multilabel-quick-ref.md for quick reference\")\n",
        "print(\"  \u2022 Try 4.3-multilabel-pipeline.py for production implementation\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
