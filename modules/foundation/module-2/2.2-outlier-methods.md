# 2.2 Outlier Detection Methods for Semiconductor Manufacturing

## 📚 Overview

Outlier detection is critical in semiconductor manufacturing for identifying anomalous process conditions, equipment malfunctions, and data quality issues. Unlike generic outlier detection, semiconductor manufacturing requires domain-specific approaches that consider the physical constraints of processes, equipment behavior patterns, and the multi-variate nature of manufacturing data.

This comprehensive guide covers theoretical foundations, practical methodologies, and industry-specific implementations for outlier detection in semiconductor environments.

## 🎯 Learning Objectives

After studying this document, you will understand:

- Statistical and machine learning approaches to outlier detection
- Semiconductor-specific anomaly patterns and their causes
- Multi-variate outlier detection for correlated process parameters
- Time-series outlier detection for continuous process monitoring
- Real-time outlier detection pipeline design and implementation
- Domain-specific outlier rules and validation techniques

## 🔬 Types of Outliers in Semiconductor Manufacturing

### 1. Process Outliers

**Definition**: Data points that deviate significantly from normal process operating conditions.

**Characteristics**:

- Values outside typical operating ranges
- Sudden changes in process parameters
- Equipment-specific anomalous patterns

**Manufacturing Causes**:

- **Equipment Malfunction**: Heater failure, pump irregularities
- **Recipe Deviations**: Incorrect parameter settings
- **Environmental Factors**: Facility power fluctuations, vibrations
- **Material Variations**: Substrate quality, chemical purity

**Detection Challenges**:

- Multi-modal distributions due to different recipes
- Correlated parameters (temperature affects pressure)
- Non-stationary behavior over time

### 2. Measurement Outliers

**Definition**: Erroneous measurements that don't reflect actual process conditions.

**Characteristics**:

- Sensor reading errors
- Communication failures
- Calibration issues

**Common Patterns**:

- **Stuck Values**: Sensor reports same value repeatedly
- **Spike Errors**: Single abnormally high/low readings
- **Drift Errors**: Gradual deviation from true values
- **Quantization Errors**: Values clustered at specific levels

**Detection Methods**:

- Rate-of-change analysis
- Cross-sensor validation
- Statistical distribution analysis

### 3. Contextual Outliers

**Definition**: Values that are normal in one context but anomalous in another.

**Manufacturing Examples**:

- High temperature normal for oxidation, abnormal for lithography
- Different pressure ranges for different process chambers
- Recipe-dependent parameter expectations

**Detection Approach**:

- Context-aware modeling
- Recipe-specific baselines
- Multi-level hierarchical analysis

### 4. Collective Outliers

**Definition**: Groups of data points that together form an anomalous pattern.

**Pattern Examples**:

- **Tool Drift**: Gradual degradation over multiple lots
- **Batch Effects**: Systematic differences between production runs
- **Maintenance Cycles**: Patterns related to preventive maintenance

**Detection Techniques**:

- Pattern recognition algorithms
- Time series anomaly detection
- Multi-variate sequential analysis

## 📊 Statistical Outlier Detection Methods

### 1. Univariate Statistical Methods

#### Z-Score Method

**Theory**:

```
Z = (x - μ) / σ

Where:
- x = observed value
- μ = population mean
- σ = population standard deviation
```

**Decision Rule**: |Z| > threshold (typically 2.5 or 3.0)

**Assumptions**:

- Normal distribution
- Independent observations
- Stable mean and variance

**Manufacturing Implementation**:

```python
def zscore_outlier_detection(data, threshold=3.0):
    """
    Z-score based outlier detection for manufacturing data.
    
    Args:
        data: pandas Series or numpy array
        threshold: Z-score threshold for outlier detection
        
    Returns:
        Boolean array indicating outliers
    """
    z_scores = np.abs(stats.zscore(data, nan_policy='omit'))
    return z_scores > threshold
```

**Pros**:

- Simple and interpretable
- Fast computation
- Well-established threshold values

**Cons**:

- Assumes normal distribution
- Sensitive to extreme outliers
- Not suitable for skewed data

#### Modified Z-Score (Robust)

**Theory**:

```
Modified Z = 0.6745 × (x - median) / MAD

Where:
- MAD = median absolute deviation
- 0.6745 = normal distribution constant
```

**Advantages over Standard Z-Score**:

- Uses median instead of mean (robust to outliers)
- Uses MAD instead of standard deviation
- Better for non-normal distributions

**Manufacturing Application**:

```python
def modified_zscore_detection(data, threshold=3.5):
    """
    Modified Z-score for robust outlier detection.
    Particularly useful for semiconductor data with non-normal distributions.
    """
    median = np.median(data)
    mad = np.median(np.abs(data - median))
    modified_z_scores = 0.6745 * (data - median) / mad
    return np.abs(modified_z_scores) > threshold
```

#### Interquartile Range (IQR) Method

**Theory**:

```
IQR = Q3 - Q1
Lower Bound = Q1 - k × IQR
Upper Bound = Q3 + k × IQR

Where k is typically 1.5 or 3.0
```

**Implementation**:

```python
def iqr_outlier_detection(data, k=1.5):
    """
    IQR-based outlier detection.
    k=1.5: mild outliers
    k=3.0: extreme outliers
    """
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - k * IQR
    upper_bound = Q3 + k * IQR
    
    return (data < lower_bound) | (data > upper_bound)
```

**Manufacturing Advantages**:

- Non-parametric (no distribution assumptions)
- Robust to extreme outliers
- Intuitive interpretation

### 2. Multivariate Statistical Methods

#### Mahalanobis Distance

**Theory**:

```
D² = (x - μ)ᵀ Σ⁻¹ (x - μ)

Where:
- x = observation vector
- μ = mean vector
- Σ = covariance matrix
```

**Significance**: Accounts for correlation structure in multivariate data.

**Manufacturing Application**:

```python
from scipy.spatial.distance import mahalanobis

def mahalanobis_outlier_detection(data, threshold_percentile=95):
    """
    Multivariate outlier detection using Mahalanobis distance.
    Ideal for correlated semiconductor process parameters.
    """
    # Remove missing values
    clean_data = data.dropna()
    
    # Calculate mean and covariance
    mean = clean_data.mean().values
    cov = clean_data.cov().values
    inv_cov = np.linalg.pinv(cov)  # Use pseudo-inverse for stability
    
    # Calculate Mahalanobis distances
    distances = []
    for _, row in clean_data.iterrows():
        dist = mahalanobis(row.values, mean, inv_cov)
        distances.append(dist)
    
    # Determine threshold
    threshold = np.percentile(distances, threshold_percentile)
    
    return np.array(distances) > threshold
```

**Pros**:

- Handles correlated variables
- Scale-invariant
- Statistically principled

**Cons**:

- Requires invertible covariance matrix
- Assumes multivariate normality
- Computationally expensive for large datasets

#### Hotelling's T² Test

**Theory**: Extension of Student's t-test to multivariate case.

**Application**: Detect when process mean vector shifts from target.

**Implementation**:

```python
def hotellings_t2_test(data, reference_data=None, alpha=0.05):
    """
    Hotelling's T² test for multivariate process monitoring.
    """
    if reference_data is None:
        # Use historical data as reference
        n_samples = len(data)
        split_point = int(n_samples * 0.8)
        reference_data = data.iloc[:split_point]
        test_data = data.iloc[split_point:]
    else:
        test_data = data
    
    # Calculate statistics
    n1, p = reference_data.shape
    n2 = len(test_data)
    
    mean1 = reference_data.mean()
    mean2 = test_data.mean()
    
    # Pooled covariance
    cov1 = reference_data.cov()
    cov2 = test_data.cov()
    pooled_cov = ((n1-1)*cov1 + (n2-1)*cov2) / (n1+n2-2)
    
    # T² statistic
    diff_mean = mean2 - mean1
    t2_stat = (n1*n2)/(n1+n2) * diff_mean.T @ np.linalg.inv(pooled_cov) @ diff_mean
    
    # Critical value
    f_critical = stats.f.ppf(1-alpha, p, n1+n2-p-1)
    critical_value = (p*(n1+n2-2)*f_critical) / (n1+n2-p-1)
    
    return t2_stat > critical_value, t2_stat, critical_value
```

## 🤖 Machine Learning-Based Outlier Detection

### 1. Isolation Forest

**Theory**: Isolates anomalies by randomly selecting features and split values.

**Key Insight**: Anomalies are easier to isolate than normal points.

**Algorithm**:

1. Randomly select a feature
2. Randomly select a split value between min and max
3. Recursively partition data
4. Anomalies require fewer splits to isolate

**Manufacturing Implementation**:

```python
from sklearn.ensemble import IsolationForest

def isolation_forest_detection(data, contamination=0.1, random_state=42):
    """
    Isolation Forest for semiconductor anomaly detection.
    
    Args:
        data: DataFrame with process parameters
        contamination: Expected proportion of outliers
        random_state: For reproducible results
    """
    # Handle missing values
    imputer = SimpleImputer(strategy='median')
    data_imputed = pd.DataFrame(
        imputer.fit_transform(data),
        columns=data.columns,
        index=data.index
    )
    
    # Fit Isolation Forest
    iso_forest = IsolationForest(
        contamination=contamination,
        random_state=random_state,
        n_estimators=100
    )
    
    # Predict outliers (-1 = outlier, 1 = normal)
    predictions = iso_forest.fit_predict(data_imputed)
    scores = iso_forest.decision_function(data_imputed)
    
    return predictions == -1, scores
```

**Advantages**:

- No assumptions about data distribution
- Efficient for large datasets
- Handles high-dimensional data well
- Linear time complexity

**Disadvantages**:

- Hyperparameter tuning required
- Performance varies with contamination parameter
- Less interpretable than statistical methods

### 2. One-Class SVM

**Theory**: Learns boundary around normal data in feature space.

**Approach**:

- Maps data to high-dimensional space using kernel
- Finds hyperplane that separates normal data from origin
- New points on "normal" side are classified as inliers

**Implementation**:

```python
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler

def oneclass_svm_detection(data, nu=0.1, kernel='rbf', gamma='scale'):
    """
    One-Class SVM for anomaly detection in process data.
    
    Args:
        data: Process parameter DataFrame
        nu: Upper bound on fraction of outliers
        kernel: Kernel function ('rbf', 'linear', 'poly')
        gamma: Kernel coefficient
    """
    # Preprocessing
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data.fillna(data.median()))
    
    # Fit One-Class SVM
    oc_svm = OneClassSVM(nu=nu, kernel=kernel, gamma=gamma)
    predictions = oc_svm.fit_predict(data_scaled)
    scores = oc_svm.decision_function(data_scaled)
    
    return predictions == -1, scores
```

**Best Use Cases**:

- Complex, non-linear decision boundaries
- High-dimensional feature spaces
- When normal data forms compact clusters

### 3. Local Outlier Factor (LOF)

**Theory**: Measures local density deviation of data points.

**Key Concept**: Outliers have lower density than their neighbors.

**Algorithm**:

1. For each point, find k nearest neighbors
2. Calculate local reachability density
3. Compare with neighbors' densities
4. LOF > 1 indicates potential outlier

**Implementation**:

```python
from sklearn.neighbors import LocalOutlierFactor

def lof_detection(data, n_neighbors=20, contamination=0.1):
    """
    Local Outlier Factor for density-based anomaly detection.
    Excellent for data with varying cluster densities.
    """
    # Preprocess data
    data_clean = data.fillna(data.median())
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_clean)
    
    # Apply LOF
    lof = LocalOutlierFactor(
        n_neighbors=n_neighbors,
        contamination=contamination
    )
    
    predictions = lof.fit_predict(data_scaled)
    scores = lof.negative_outlier_factor_
    
    return predictions == -1, -scores  # Convert to positive scores
```

**Advantages**:

- Adapts to local data density
- Good for clusters with varying densities
- Intuitive interpretation

**Disadvantages**:

- Sensitive to n_neighbors parameter
- Computationally expensive for large datasets
- Struggles with high-dimensional data

## ⏱️ Time-Series Outlier Detection

### 1. Statistical Process Control Methods

#### EWMA (Exponentially Weighted Moving Average)

**Theory**:

```
EWMA_t = λ × X_t + (1-λ) × EWMA_{t-1}

Control Limits:
UCL = μ + L × σ × √(λ/(2-λ) × (1-(1-λ)^{2t}))
LCL = μ - L × σ × √(λ/(2-λ) × (1-(1-λ)^{2t}))
```

**Manufacturing Implementation**:

```python
def ewma_control_chart(data, lambda_param=0.2, L=3.0):
    """
    EWMA control chart for time-series outlier detection.
    Sensitive to small shifts in process mean.
    """
    n = len(data)
    mean_data = np.mean(data)
    std_data = np.std(data)
    
    # Initialize EWMA
    ewma = np.zeros(n)
    ewma[0] = data[0]
    
    # Calculate EWMA values
    for t in range(1, n):
        ewma[t] = lambda_param * data[t] + (1 - lambda_param) * ewma[t-1]
    
    # Calculate control limits
    variance_factor = lambda_param / (2 - lambda_param)
    ucl = np.zeros(n)
    lcl = np.zeros(n)
    
    for t in range(n):
        variance_multiplier = variance_factor * (1 - (1 - lambda_param)**(2*(t+1)))
        ucl[t] = mean_data + L * std_data * np.sqrt(variance_multiplier)
        lcl[t] = mean_data - L * std_data * np.sqrt(variance_multiplier)
    
    # Detect outliers
    outliers = (ewma > ucl) | (ewma < lcl)
    
    return outliers, ewma, ucl, lcl
```

#### CUSUM (Cumulative Sum Control Chart)

**Theory**: Detects small shifts in process mean by accumulating deviations.

**Implementation**:

```python
def cusum_control_chart(data, target_mean=None, std_dev=None, h=5, k=0.5):
    """
    CUSUM control chart for detecting process shifts.
    
    Args:
        data: Time series data
        target_mean: Target process mean
        std_dev: Process standard deviation  
        h: Decision interval (typically 4-5)
        k: Reference value (typically 0.5)
    """
    if target_mean is None:
        target_mean = np.mean(data)
    if std_dev is None:
        std_dev = np.std(data)
    
    n = len(data)
    c_plus = np.zeros(n)
    c_minus = np.zeros(n)
    
    # Standardize data
    standardized = (data - target_mean) / std_dev
    
    # Calculate CUSUM statistics
    for i in range(1, n):
        c_plus[i] = max(0, c_plus[i-1] + standardized[i] - k)
        c_minus[i] = min(0, c_minus[i-1] + standardized[i] + k)
    
    # Detect outliers
    outliers = (c_plus > h) | (c_minus < -h)
    
    return outliers, c_plus, c_minus
```

### 2. Change Point Detection

#### PELT (Pruned Exact Linear Time)

**Theory**: Efficiently detects multiple change points in time series.

**Application**: Identify when process parameters change regime.

**Implementation**:

```python
import ruptures as rpt

def change_point_detection(data, model='rbf', penalty=10):
    """
    Change point detection for identifying process regime changes.
    """
    # Apply change point detection
    algo = rpt.Pelt(model=model).fit(data)
    change_points = algo.predict(pen=penalty)
    
    # Create outlier mask for points near change points
    outliers = np.zeros(len(data), dtype=bool)
    
    # Mark points around change points as potential outliers
    window = 5  # Points around change point to flag
    for cp in change_points[:-1]:  # Exclude last point (end of series)
        start = max(0, cp - window)
        end = min(len(data), cp + window)
        outliers[start:end] = True
    
    return outliers, change_points
```

## 🏭 Semiconductor-Specific Outlier Detection

### 1. Recipe-Aware Outlier Detection

**Challenge**: Different recipes have different normal operating ranges.

**Solution**: Context-aware modeling based on recipe parameters.

```python
def recipe_aware_outlier_detection(data, recipe_column, method='isolation_forest'):
    """
    Detect outliers within recipe contexts.
    
    Args:
        data: DataFrame with process parameters
        recipe_column: Column identifying recipe/process type
        method: Detection method to use
    """
    outliers = np.zeros(len(data), dtype=bool)
    
    for recipe in data[recipe_column].unique():
        if pd.isna(recipe):
            continue
            
        # Get data for this recipe
        recipe_mask = data[recipe_column] == recipe
        recipe_data = data[recipe_mask].drop(columns=[recipe_column])
        
        # Apply outlier detection method
        if method == 'isolation_forest':
            recipe_outliers, _ = isolation_forest_detection(recipe_data)
        elif method == 'mahalanobis':
            recipe_outliers = mahalanobis_outlier_detection(recipe_data)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Update global outlier mask
        outliers[recipe_mask] = recipe_outliers
    
    return outliers
```

### 2. Tool-Specific Baseline Models

**Approach**: Each tool/chamber has its own baseline model.

```python
def tool_specific_outlier_detection(data, tool_column, baseline_period_days=30):
    """
    Tool-specific outlier detection with adaptive baselines.
    """
    outliers = np.zeros(len(data), dtype=bool)
    
    for tool in data[tool_column].unique():
        tool_data = data[data[tool_column] == tool].copy()
        
        if len(tool_data) < 50:  # Minimum data requirement
            continue
        
        # Use rolling baseline for adaptation
        window_size = min(baseline_period_days * 24, len(tool_data) // 2)
        
        for i in range(window_size, len(tool_data)):
            # Get baseline data
            baseline_data = tool_data.iloc[i-window_size:i]
            current_point = tool_data.iloc[i:i+1]
            
            # Apply detection method (using historical data as baseline)
            baseline_mean = baseline_data.drop(columns=[tool_column]).mean()
            baseline_std = baseline_data.drop(columns=[tool_column]).std()
            
            # Z-score based detection
            current_values = current_point.drop(columns=[tool_column]).iloc[0]
            z_scores = np.abs((current_values - baseline_mean) / baseline_std)
            
            # Mark as outlier if any parameter exceeds threshold
            if (z_scores > 3.0).any():
                original_index = tool_data.index[i]
                outliers[data.index == original_index] = True
    
    return outliers
```

### 3. Physics-Based Validation

**Principle**: Use process physics to validate outlier detections.

```python
def physics_based_validation(data, outlier_mask):
    """
    Validate outliers using semiconductor process physics constraints.
    """
    validated_outliers = outlier_mask.copy()
    
    # Example physics-based rules
    physics_rules = {
        'temperature_pressure': {
            'rule': 'High temperature should correlate with high pressure in plasma processes',
            'validation': lambda df: (df['temperature'] > df['temperature'].quantile(0.9)) & 
                                   (df['pressure'] < df['pressure'].quantile(0.1))
        },
        'flow_rate_pressure': {
            'rule': 'Higher flow rates should increase chamber pressure',
            'validation': lambda df: (df['flow_rate'] > df['flow_rate'].quantile(0.9)) & 
                                   (df['pressure'] < df['pressure'].quantile(0.1))
        },
        'power_temperature': {
            'rule': 'RF power should correlate with substrate temperature',
            'validation': lambda df: (df['rf_power'] > df['rf_power'].quantile(0.9)) & 
                                   (df['substrate_temp'] < df['substrate_temp'].quantile(0.1))
        }
    }
    
    # Apply physics validation
    for rule_name, rule_info in physics_rules.items():
        try:
            physics_outliers = rule_info['validation'](data)
            # Mark as high-confidence outliers if both statistical and physics agree
            validated_outliers = validated_outliers | physics_outliers
        except KeyError:
            # Skip rule if required columns not present
            continue
    
    return validated_outliers
```

## 📊 Evaluation and Validation

### 1. Outlier Detection Metrics

```python
def evaluate_outlier_detection(y_true, y_pred, y_scores=None):
    """
    Comprehensive evaluation of outlier detection performance.
    
    Args:
        y_true: True outlier labels (1=outlier, 0=normal)
        y_pred: Predicted outlier labels
        y_scores: Anomaly scores (optional)
    
    Returns:
        Dictionary of evaluation metrics
    """
    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
    
    metrics = {}
    
    # Basic classification metrics
    metrics['precision'] = precision_score(y_true, y_pred)
    metrics['recall'] = recall_score(y_true, y_pred)
    metrics['f1_score'] = f1_score(y_true, y_pred)
    
    # Confusion matrix components
    TP = np.sum((y_true == 1) & (y_pred == 1))
    TN = np.sum((y_true == 0) & (y_pred == 0))
    FP = np.sum((y_true == 0) & (y_pred == 1))
    FN = np.sum((y_true == 1) & (y_pred == 0))
    
    metrics['true_positive_rate'] = TP / (TP + FN) if (TP + FN) > 0 else 0
    metrics['false_positive_rate'] = FP / (FP + TN) if (FP + TN) > 0 else 0
    metrics['true_negative_rate'] = TN / (TN + FP) if (TN + FP) > 0 else 0
    
    # ROC AUC if scores provided
    if y_scores is not None:
        metrics['roc_auc'] = roc_auc_score(y_true, y_scores)
    
    return metrics
```

### 2. Cross-Validation for Outlier Detection

```python
def cross_validate_outlier_detection(data, outlier_labels, method='isolation_forest', cv_folds=5):
    """
    Cross-validation for outlier detection methods.
    """
    from sklearn.model_selection import StratifiedKFold
    
    results = []
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    for train_idx, test_idx in skf.split(data, outlier_labels):
        train_data = data.iloc[train_idx]
        test_data = data.iloc[test_idx]
        test_labels = outlier_labels[test_idx]
        
        # Train outlier detection model
        if method == 'isolation_forest':
            pred_outliers, scores = isolation_forest_detection(train_data)
            # Apply to test data
            # Note: In practice, you'd retrain on train_data and predict on test_data
            test_pred, test_scores = isolation_forest_detection(test_data)
        
        # Evaluate
        metrics = evaluate_outlier_detection(test_labels, test_pred, test_scores)
        results.append(metrics)
    
    # Aggregate results
    avg_metrics = {}
    for metric in results[0].keys():
        avg_metrics[metric] = np.mean([r[metric] for r in results])
        avg_metrics[f"{metric}_std"] = np.std([r[metric] for r in results])
    
    return avg_metrics
```

## 🚨 Real-Time Outlier Detection Pipeline

### 1. Streaming Data Processing

```python
class RealTimeOutlierDetector:
    """
    Real-time outlier detection for semiconductor manufacturing streams.
    """
    
    def __init__(self, methods=['zscore', 'isolation_forest'], window_size=1000):
        self.methods = methods
        self.window_size = window_size
        self.data_buffer = []
        self.models = {}
        self.baseline_stats = {}
        
    def update_baseline(self, new_data):
        """Update baseline statistics with new data."""
        self.data_buffer.extend(new_data.values.tolist())
        
        # Maintain rolling window
        if len(self.data_buffer) > self.window_size:
            self.data_buffer = self.data_buffer[-self.window_size:]
        
        # Update statistics
        if len(self.data_buffer) >= 100:  # Minimum for stable statistics
            buffer_df = pd.DataFrame(self.data_buffer, columns=new_data.columns)
            self.baseline_stats = {
                'mean': buffer_df.mean(),
                'std': buffer_df.std(),
                'median': buffer_df.median(),
                'mad': buffer_df.mad()
            }
            
            # Retrain models
            self._retrain_models(buffer_df)
    
    def _retrain_models(self, data):
        """Retrain outlier detection models."""
        if 'isolation_forest' in self.methods:
            self.models['isolation_forest'] = IsolationForest(
                contamination=0.1,
                random_state=42
            ).fit(data.fillna(data.median()))
    
    def detect_outliers(self, current_data):
        """Detect outliers in current data point."""
        outlier_flags = {}
        
        if len(self.baseline_stats) == 0:
            return {'no_baseline': True}
        
        # Z-score detection
        if 'zscore' in self.methods:
            z_scores = np.abs((current_data - self.baseline_stats['mean']) / 
                            self.baseline_stats['std'])
            outlier_flags['zscore'] = (z_scores > 3.0).any()
        
        # Modified Z-score detection
        if 'modified_zscore' in self.methods:
            mod_z_scores = 0.6745 * np.abs((current_data - self.baseline_stats['median']) / 
                                         self.baseline_stats['mad'])
            outlier_flags['modified_zscore'] = (mod_z_scores > 3.5).any()
        
        # Isolation Forest detection
        if 'isolation_forest' in self.methods and 'isolation_forest' in self.models:
            prediction = self.models['isolation_forest'].predict(
                current_data.fillna(self.baseline_stats['median']).values.reshape(1, -1)
            )
            outlier_flags['isolation_forest'] = prediction[0] == -1
        
        return outlier_flags
```

### 2. Alert System Integration

```python
class OutlierAlertSystem:
    """
    Alert system for outlier detection in manufacturing.
    """
    
    def __init__(self, alert_thresholds=None):
        self.alert_thresholds = alert_thresholds or {
            'critical': 0.8,  # 80% of methods agree
            'warning': 0.5    # 50% of methods agree
        }
        self.alert_history = []
    
    def process_outlier_detection(self, outlier_results, timestamp, context=None):
        """Process outlier detection results and generate alerts."""
        
        # Calculate consensus score
        method_flags = [v for k, v in outlier_results.items() 
                       if isinstance(v, bool)]
        
        if not method_flags:
            return None
        
        consensus_score = sum(method_flags) / len(method_flags)
        
        # Determine alert level
        alert_level = None
        if consensus_score >= self.alert_thresholds['critical']:
            alert_level = 'CRITICAL'
        elif consensus_score >= self.alert_thresholds['warning']:
            alert_level = 'WARNING'
        
        if alert_level:
            alert = {
                'timestamp': timestamp,
                'level': alert_level,
                'consensus_score': consensus_score,
                'methods_agreeing': method_flags,
                'context': context or {},
                'recommendation': self._generate_recommendation(alert_level, outlier_results)
            }
            
            self.alert_history.append(alert)
            return alert
        
        return None
    
    def _generate_recommendation(self, alert_level, outlier_results):
        """Generate action recommendations based on alert."""
        if alert_level == 'CRITICAL':
            return {
                'action': 'IMMEDIATE_INVESTIGATION',
                'description': 'Multiple detection methods indicate anomaly. Stop production and investigate.',
                'priority': 1
            }
        elif alert_level == 'WARNING':
            return {
                'action': 'MONITOR_CLOSELY',
                'description': 'Potential anomaly detected. Increase monitoring frequency.',
                'priority': 2
            }
        
        return None
```

## 🔧 Best Practices and Guidelines

### 1. Method Selection Guidelines

**For Different Data Characteristics**:

- **Normal Distribution, Low Correlation**: Z-score, IQR methods
- **Non-Normal Distribution**: Modified Z-score, IQR methods
- **High Correlation, Multivariate**: Mahalanobis distance, PCA-based methods
- **High Dimensionality**: Isolation Forest, One-Class SVM
- **Time Series**: EWMA, CUSUM, change point detection
- **Mixed Data Types**: Isolation Forest, LOF

**For Different Manufacturing Contexts**:

- **Real-Time Monitoring**: Statistical methods (fast computation)
- **Batch Analysis**: Machine learning methods (higher accuracy)
- **Equipment Health**: Time-series methods with physics validation
- **Quality Control**: Recipe-aware methods with domain rules

### 2. Implementation Checklist

**Data Preparation**:

- [ ] Handle missing values appropriately
- [ ] Scale/normalize features if required
- [ ] Remove obvious data quality issues
- [ ] Validate data integrity

**Method Selection**:

- [ ] Consider data characteristics and assumptions
- [ ] Account for manufacturing context
- [ ] Plan for computational requirements
- [ ] Design validation strategy

**Model Training**:

- [ ] Use representative training data
- [ ] Set appropriate hyperparameters
- [ ] Validate on hold-out test set
- [ ] Document model limitations

**Deployment**:

- [ ] Implement real-time processing capability
- [ ] Design alert and notification system
- [ ] Plan for model updates and retraining
- [ ] Create monitoring and logging

**Validation**:

- [ ] Test on diverse scenarios
- [ ] Validate with domain experts
- [ ] Monitor false positive/negative rates
- [ ] Continuously improve based on feedback

## 📚 Further Reading

1. **"Outlier Detection for Temporal Data"** by Gupta et al.
2. **"Anomaly Detection: A Tutorial"** by Chandola et al.
3. **"Statistical Quality Control"** by Montgomery
4. **"Semiconductor Manufacturing: An Introduction"** by Hong Xiao
5. **"Machine Learning for Time Series Forecasting"** by Brownlee

## 💡 Key Takeaways

1. **Context Matters**: Semiconductor outlier detection requires domain knowledge
2. **Multi-Method Approach**: Combine statistical and ML methods for robustness
3. **Real-Time Capability**: Manufacturing requires immediate anomaly detection
4. **Physics Validation**: Use process knowledge to validate statistical findings
5. **Continuous Learning**: Models must adapt to changing process conditions
6. **False Positive Management**: Balance sensitivity with operational practicality
7. **Documentation is Critical**: Maintain detailed records for troubleshooting

## 🎯 Next Steps

In the next section (2.3), we'll explore **Advanced Statistical Analysis with ANOVA**, building on the outlier detection foundation to perform sophisticated multi-factor process analysis and designed experiments in semiconductor manufacturing.
