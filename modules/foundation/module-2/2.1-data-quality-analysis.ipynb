{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79ee94d4",
      "metadata": {},
      "source": [
        "# 2.1 Data Quality Analysis for Semiconductor Manufacturing\n",
        "\n",
        "## \ud83d\udcda Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "- Understand the fundamentals of data quality assessment\n",
        "- Master data profiling techniques for semiconductor datasets\n",
        "- Implement data validation frameworks\n",
        "- Create comprehensive data quality reports\n",
        "- Build automated data quality monitoring systems\n",
        "\n",
        "## \ud83c\udfaf What You'll Build\n",
        "\n",
        "We'll work with the **SECOM dataset** - a real semiconductor manufacturing dataset with 1567 records and 590 process parameters. You'll learn to:\n",
        "\n",
        "1. **Profile** the dataset to understand its characteristics\n",
        "2. **Assess** data quality across multiple dimensions\n",
        "3. **Identify** data quality issues and their impact\n",
        "4. **Implement** automated validation frameworks\n",
        "5. **Generate** actionable data quality reports\n",
        "\n",
        "Let's start by importing the essential libraries and loading our data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b55065",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries for data quality analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dataset path resolution (per repo standard)\n",
        "DATA_DIR = Path('../../../datasets').resolve()\n",
        "\n",
        "print(\"\u2705 Libraries imported successfully!\")\n",
        "print(f\"\ud83d\udcca Pandas version: {pd.__version__}\")\n",
        "print(f\"\ud83d\udd22 NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7e6590",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Loading the SECOM Dataset\n",
        "\n",
        "The **SECOM** (SEmiCOnductor Manufacturing) dataset contains:\n",
        "- **1567 observations** of semiconductor manufacturing processes\n",
        "- **590 process parameters** (sensors, measurements, etc.)\n",
        "- **Binary target** indicating pass/fail status\n",
        "- **Real-world** data with typical manufacturing data quality challenges\n",
        "\n",
        "This dataset is perfect for learning data quality assessment because it contains:\n",
        "- Missing values (common in manufacturing)\n",
        "- Outliers (equipment malfunctions, measurement errors)\n",
        "- Multicollinearity (related process parameters)\n",
        "- Scale differences (various measurement units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ca3940",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the SECOM dataset\n",
        "def load_secom_data():\n",
        "    \"\"\"\n",
        "    Load the SECOM semiconductor manufacturing dataset.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (features_df, target_series, metadata_dict)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to load from the standard datasets directory\n",
        "        secom_dir = DATA_DIR / 'secom'\n",
        "        features_path = secom_dir / 'secom.data'\n",
        "        labels_path = secom_dir / 'secom_labels.data'\n",
        "        \n",
        "        # Load features and targets\n",
        "        features_df = pd.read_csv(features_path, sep=\" \", header=None)\n",
        "        target_df = pd.read_csv(labels_path, sep=\" \", header=None)\n",
        "        \n",
        "        # Clean column names\n",
        "        features_df.columns = [f\"sensor_{i:03d}\" for i in range(len(features_df.columns))]\n",
        "        target_series = target_df.iloc[:, 0]\n",
        "        \n",
        "        # Create metadata\n",
        "        metadata = {\n",
        "            \"dataset_name\": \"SECOM\",\n",
        "            \"description\": \"Semiconductor Manufacturing Process Data\",\n",
        "            \"n_samples\": len(features_df),\n",
        "            \"n_features\": len(features_df.columns),\n",
        "            \"target_classes\": target_series.value_counts().to_dict(),\n",
        "            \"source\": \"UCI Machine Learning Repository\"\n",
        "        }\n",
        "        \n",
        "        print(f\"\u2705 SECOM dataset loaded successfully!\")\n",
        "        print(f\"\ud83d\udccf Shape: {features_df.shape}\")\n",
        "        print(f\"\ud83c\udfaf Target distribution: {dict(target_series.value_counts())}\")\n",
        "        \n",
        "        return features_df, target_series, metadata\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(\"\u274c SECOM dataset not found. Creating synthetic data for demonstration...\")\n",
        "        return create_synthetic_secom_data()\n",
        "\n",
        "def create_synthetic_secom_data():\n",
        "    \"\"\"Create synthetic semiconductor data that mimics SECOM characteristics.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create synthetic features with manufacturing characteristics\n",
        "    n_samples, n_features = 1567, 590\n",
        "    \n",
        "    # Base features with different distributions\n",
        "    features = np.random.normal(0, 1, (n_samples, n_features))\n",
        "    \n",
        "    # Add manufacturing-specific patterns\n",
        "    # Some sensors are highly correlated (process coupling)\n",
        "    for i in range(0, min(50, n_features), 5):\n",
        "        base_signal = np.random.normal(0, 1, n_samples)\n",
        "        noise_level = 0.3\n",
        "        for j in range(5):\n",
        "            if i + j < n_features:\n",
        "                features[:, i + j] = base_signal + np.random.normal(0, noise_level, n_samples)\n",
        "    \n",
        "    # Add missing values (realistic pattern)\n",
        "    missing_rate = 0.1\n",
        "    for col in range(n_features):\n",
        "        n_missing = int(np.random.poisson(missing_rate * n_samples))\n",
        "        if n_missing > 0:\n",
        "            missing_indices = np.random.choice(n_samples, n_missing, replace=False)\n",
        "            features[missing_indices, col] = np.nan\n",
        "    \n",
        "    # Add outliers (equipment malfunctions)\n",
        "    outlier_rate = 0.02\n",
        "    for col in range(n_features):\n",
        "        n_outliers = int(np.random.poisson(outlier_rate * n_samples))\n",
        "        if n_outliers > 0:\n",
        "            outlier_indices = np.random.choice(n_samples, n_outliers, replace=False)\n",
        "            outlier_values = np.random.choice([-1, 1], n_outliers) * np.random.exponential(5, n_outliers)\n",
        "            features[outlier_indices, col] = outlier_values\n",
        "    \n",
        "    # Create DataFrame\n",
        "    features_df = pd.DataFrame(features, columns=[f\"sensor_{i:03d}\" for i in range(n_features)])\n",
        "    \n",
        "    # Create realistic target (some correlation with features)\n",
        "    feature_importance = np.random.exponential(0.1, n_features)\n",
        "    target_scores = np.dot(features, feature_importance)\n",
        "    target_probs = 1 / (1 + np.exp(-target_scores / np.std(target_scores)))\n",
        "    target_series = pd.Series(np.random.binomial(1, target_probs), name=\"target\")\n",
        "    \n",
        "    metadata = {\n",
        "        \"dataset_name\": \"Synthetic SECOM\",\n",
        "        \"description\": \"Synthetic Semiconductor Manufacturing Process Data\",\n",
        "        \"n_samples\": n_samples,\n",
        "        \"n_features\": n_features,\n",
        "        \"target_classes\": target_series.value_counts().to_dict(),\n",
        "        \"source\": \"Synthetic (for demonstration)\"\n",
        "    }\n",
        "    \n",
        "    print(f\"\u2705 Synthetic SECOM dataset created!\")\n",
        "    print(f\"\ud83d\udccf Shape: {features_df.shape}\")\n",
        "    print(f\"\ud83c\udfaf Target distribution: {dict(target_series.value_counts())}\")\n",
        "    \n",
        "    return features_df, target_series, metadata\n",
        "\n",
        "# Load the data\n",
        "features_df, target_series, metadata = load_secom_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7d2dc0",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d Basic Data Quality Assessment\n",
        "\n",
        "Let's start with a fundamental data quality assessment. We'll examine:\n",
        "\n",
        "1. **Data Completeness** - How much data is missing?\n",
        "2. **Data Types** - Are the data types appropriate?\n",
        "3. **Data Ranges** - What are the min/max values?\n",
        "4. **Data Distribution** - How are values distributed?\n",
        "5. **Data Consistency** - Are there any obvious inconsistencies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e86b471",
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_data_quality_assessment(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"\n",
        "    Perform basic data quality assessment on a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing quality metrics\n",
        "    \"\"\"\n",
        "    assessment = {}\n",
        "    \n",
        "    # Basic info\n",
        "    assessment['shape'] = df.shape\n",
        "    assessment['memory_usage'] = df.memory_usage(deep=True).sum()\n",
        "    \n",
        "    # Missing data analysis\n",
        "    missing_count = df.isnull().sum()\n",
        "    assessment['missing_data'] = {\n",
        "        'total_missing': missing_count.sum(),\n",
        "        'missing_percentage': (missing_count.sum() / (df.shape[0] * df.shape[1])) * 100,\n",
        "        'columns_with_missing': (missing_count > 0).sum(),\n",
        "        'columns_missing_percentage': (missing_count > 0).sum() / df.shape[1] * 100\n",
        "    }\n",
        "    \n",
        "    # Data types\n",
        "    assessment['data_types'] = df.dtypes.value_counts().to_dict()\n",
        "    \n",
        "    # Numeric columns analysis\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        assessment['numeric_summary'] = {\n",
        "            'count': len(numeric_cols),\n",
        "            'infinite_values': np.isinf(df[numeric_cols]).sum().sum(),\n",
        "            'negative_values': (df[numeric_cols] < 0).sum().sum(),\n",
        "            'zero_values': (df[numeric_cols] == 0).sum().sum()\n",
        "        }\n",
        "    \n",
        "    # Duplicated rows\n",
        "    assessment['duplicates'] = {\n",
        "        'duplicate_rows': df.duplicated().sum(),\n",
        "        'duplicate_percentage': df.duplicated().sum() / len(df) * 100\n",
        "    }\n",
        "    \n",
        "    return assessment\n",
        "\n",
        "# Perform basic assessment\n",
        "print(\"\ud83d\udd0d Performing Basic Data Quality Assessment...\")\n",
        "quality_assessment = basic_data_quality_assessment(features_df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\ud83d\udcca BASIC DATA QUALITY REPORT\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Dataset Shape: {quality_assessment['shape']}\")\n",
        "print(f\"Memory Usage: {quality_assessment['memory_usage'] / 1024**2:.2f} MB\")\n",
        "print(f\"\\n\ud83d\udcc9 Missing Data:\")\n",
        "print(f\"  Total Missing Values: {quality_assessment['missing_data']['total_missing']:,}\")\n",
        "print(f\"  Missing Percentage: {quality_assessment['missing_data']['missing_percentage']:.2f}%\")\n",
        "print(f\"  Columns with Missing: {quality_assessment['missing_data']['columns_with_missing']}\")\n",
        "print(f\"\\n\ud83d\udcca Data Types:\")\n",
        "for dtype, count in quality_assessment['data_types'].items():\n",
        "    print(f\"  {dtype}: {count} columns\")\n",
        "print(f\"\\n\ud83d\udd22 Numeric Data:\")\n",
        "if 'numeric_summary' in quality_assessment:\n",
        "    print(f\"  Numeric Columns: {quality_assessment['numeric_summary']['count']}\")\n",
        "    print(f\"  Infinite Values: {quality_assessment['numeric_summary']['infinite_values']}\")\n",
        "    print(f\"  Negative Values: {quality_assessment['numeric_summary']['negative_values']}\")\n",
        "    print(f\"  Zero Values: {quality_assessment['numeric_summary']['zero_values']}\")\n",
        "print(f\"\\n\ud83d\udd04 Duplicates:\")\n",
        "print(f\"  Duplicate Rows: {quality_assessment['duplicates']['duplicate_rows']}\")\n",
        "print(f\"  Duplicate Percentage: {quality_assessment['duplicates']['duplicate_percentage']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2494bb7f",
      "metadata": {},
      "source": [
        "## \ud83d\udcc8 Missing Data Analysis\n",
        "\n",
        "Missing data is one of the most critical data quality issues in manufacturing. Let's dive deeper into understanding patterns of missingness in our semiconductor dataset.\n",
        "\n",
        "### Types of Missing Data:\n",
        "1. **MCAR (Missing Completely at Random)** - Missing values are random\n",
        "2. **MAR (Missing at Random)** - Missing depends on observed data\n",
        "3. **MNAR (Missing Not at Random)** - Missing depends on unobserved data\n",
        "\n",
        "In semiconductor manufacturing, missing data often occurs due to:\n",
        "- **Sensor failures** (equipment malfunction)\n",
        "- **Maintenance windows** (scheduled downtime)\n",
        "- **Process variations** (different measurement protocols)\n",
        "- **Data transmission errors** (network issues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276b1120",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_missing_patterns(df: pd.DataFrame, max_cols_to_show: int = 20) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze missing data patterns in detail.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        max_cols_to_show: Maximum columns to show in detailed analysis\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with missing pattern analysis\n",
        "    \"\"\"\n",
        "    missing_analysis = {}\n",
        "    \n",
        "    # Calculate missing statistics per column\n",
        "    missing_stats = pd.DataFrame({\n",
        "        'missing_count': df.isnull().sum(),\n",
        "        'missing_percentage': (df.isnull().sum() / len(df)) * 100,\n",
        "        'data_type': df.dtypes\n",
        "    })\n",
        "    \n",
        "    # Sort by missing percentage\n",
        "    missing_stats = missing_stats.sort_values('missing_percentage', ascending=False)\n",
        "    missing_analysis['column_stats'] = missing_stats\n",
        "    \n",
        "    # Missing patterns by row\n",
        "    missing_per_row = df.isnull().sum(axis=1)\n",
        "    missing_analysis['row_stats'] = {\n",
        "        'min_missing_per_row': missing_per_row.min(),\n",
        "        'max_missing_per_row': missing_per_row.max(),\n",
        "        'avg_missing_per_row': missing_per_row.mean(),\n",
        "        'rows_with_no_missing': (missing_per_row == 0).sum(),\n",
        "        'rows_with_all_missing': (missing_per_row == len(df.columns)).sum()\n",
        "    }\n",
        "    \n",
        "    # Create missing pattern matrix (for visualization)\n",
        "    if len(df.columns) <= max_cols_to_show:\n",
        "        cols_to_analyze = df.columns[:max_cols_to_show]\n",
        "    else:\n",
        "        # Select top missing columns\n",
        "        cols_to_analyze = missing_stats.head(max_cols_to_show).index\n",
        "    \n",
        "    missing_analysis['pattern_matrix'] = df[cols_to_analyze].isnull()\n",
        "    \n",
        "    return missing_analysis\n",
        "\n",
        "# Analyze missing patterns\n",
        "print(\"\ud83d\udd0d Analyzing Missing Data Patterns...\")\n",
        "missing_analysis = analyze_missing_patterns(features_df)\n",
        "\n",
        "# Display top columns with missing data\n",
        "print(\"\\n\ud83d\udcca TOP 10 COLUMNS WITH MISSING DATA\")\n",
        "print(\"=\" * 50)\n",
        "top_missing = missing_analysis['column_stats'].head(10)\n",
        "for idx, (col, row) in enumerate(top_missing.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {col}: {row['missing_count']:4d} missing ({row['missing_percentage']:5.1f}%)\")\n",
        "\n",
        "# Display row statistics\n",
        "print(f\"\\n\ud83d\udcca MISSING DATA BY ROW\")\n",
        "print(\"=\" * 50)\n",
        "row_stats = missing_analysis['row_stats']\n",
        "print(f\"Rows with no missing data: {row_stats['rows_with_no_missing']:,}\")\n",
        "print(f\"Average missing per row: {row_stats['avg_missing_per_row']:.1f}\")\n",
        "print(f\"Max missing in a single row: {row_stats['max_missing_per_row']}\")\n",
        "\n",
        "# Visualize missing patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Missing data heatmap (top 20 columns)\n",
        "top_20_missing_cols = missing_analysis['column_stats'].head(20).index\n",
        "missing_matrix = features_df[top_20_missing_cols].isnull()\n",
        "sns.heatmap(missing_matrix.iloc[:200], \n",
        "            xticklabels=True, yticklabels=False, \n",
        "            cbar=True, ax=axes[0,0])\n",
        "axes[0,0].set_title('Missing Data Pattern (Top 20 Columns, First 200 Rows)')\n",
        "axes[0,0].set_xlabel('Sensor Columns')\n",
        "\n",
        "# 2. Missing percentage per column histogram\n",
        "missing_pcts = missing_analysis['column_stats']['missing_percentage']\n",
        "axes[0,1].hist(missing_pcts, bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[0,1].set_title('Distribution of Missing Percentages Across Columns')\n",
        "axes[0,1].set_xlabel('Missing Percentage (%)')\n",
        "axes[0,1].set_ylabel('Number of Columns')\n",
        "\n",
        "# 3. Missing count per row histogram\n",
        "missing_per_row = features_df.isnull().sum(axis=1)\n",
        "axes[1,0].hist(missing_per_row, bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[1,0].set_title('Distribution of Missing Values per Row')\n",
        "axes[1,0].set_xlabel('Number of Missing Values')\n",
        "axes[1,0].set_ylabel('Number of Rows')\n",
        "\n",
        "# 4. Missing vs Target correlation (if applicable)\n",
        "if target_series is not None:\n",
        "    missing_by_target = pd.DataFrame({\n",
        "        'missing_count': missing_per_row,\n",
        "        'target': target_series\n",
        "    })\n",
        "    missing_by_target.boxplot(column='missing_count', by='target', ax=axes[1,1])\n",
        "    axes[1,1].set_title('Missing Values by Target Class')\n",
        "    axes[1,1].set_xlabel('Target Class')\n",
        "    axes[1,1].set_ylabel('Missing Values per Row')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4a1f43",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Data Quality Dimensions Framework\n",
        "\n",
        "In manufacturing, we assess data quality across **6 critical dimensions**:\n",
        "\n",
        "1. **Completeness** - Is all required data present?\n",
        "2. **Accuracy** - Is the data correct and precise?\n",
        "3. **Consistency** - Is the data uniform across sources?\n",
        "4. **Validity** - Does the data conform to defined formats/rules?\n",
        "5. **Uniqueness** - Are there inappropriate duplicates?\n",
        "6. **Timeliness** - Is the data current and up-to-date?\n",
        "\n",
        "Let's implement a comprehensive framework to assess all dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1161aea1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataQualityFramework:\n",
        "    \"\"\"\n",
        "    Comprehensive data quality assessment framework for semiconductor manufacturing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, target: Optional[pd.Series] = None):\n",
        "        self.df = df\n",
        "        self.target = target\n",
        "        self.quality_report = {}\n",
        "        \n",
        "    def assess_completeness(self) -> Dict:\n",
        "        \"\"\"Assess data completeness.\"\"\"\n",
        "        missing_data = self.df.isnull()\n",
        "        \n",
        "        completeness = {\n",
        "            'overall_completeness': (1 - missing_data.sum().sum() / (self.df.shape[0] * self.df.shape[1])) * 100,\n",
        "            'column_completeness': ((1 - missing_data.sum() / len(self.df)) * 100).to_dict(),\n",
        "            'row_completeness': ((1 - missing_data.sum(axis=1) / len(self.df.columns)) * 100).describe().to_dict(),\n",
        "            'complete_rows_percentage': ((missing_data.sum(axis=1) == 0).sum() / len(self.df)) * 100\n",
        "        }\n",
        "        \n",
        "        return completeness\n",
        "    \n",
        "    def assess_accuracy(self) -> Dict:\n",
        "        \"\"\"Assess data accuracy using statistical methods.\"\"\"\n",
        "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "        accuracy = {}\n",
        "        \n",
        "        # Outlier detection using IQR method\n",
        "        outlier_counts = {}\n",
        "        for col in numeric_cols:\n",
        "            if self.df[col].notna().sum() > 0:\n",
        "                Q1 = self.df[col].quantile(0.25)\n",
        "                Q3 = self.df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                outliers = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()\n",
        "                outlier_counts[col] = outliers\n",
        "        \n",
        "        accuracy['outlier_analysis'] = {\n",
        "            'total_outliers': sum(outlier_counts.values()),\n",
        "            'outlier_percentage': sum(outlier_counts.values()) / (self.df.shape[0] * len(numeric_cols)) * 100,\n",
        "            'columns_with_outliers': sum(1 for count in outlier_counts.values() if count > 0),\n",
        "            'top_outlier_columns': dict(sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True)[:10])\n",
        "        }\n",
        "        \n",
        "        # Statistical consistency checks\n",
        "        accuracy['statistical_checks'] = {\n",
        "            'infinite_values': np.isinf(self.df[numeric_cols]).sum().sum(),\n",
        "            'negative_values_in_positive_cols': 0,  # Would be domain-specific\n",
        "            'values_outside_expected_range': 0  # Would be domain-specific\n",
        "        }\n",
        "        \n",
        "        return accuracy\n",
        "    \n",
        "    def assess_consistency(self) -> Dict:\n",
        "        \"\"\"Assess data consistency.\"\"\"\n",
        "        consistency = {}\n",
        "        \n",
        "        # Data type consistency\n",
        "        dtype_counts = self.df.dtypes.value_counts()\n",
        "        consistency['data_type_diversity'] = len(dtype_counts)\n",
        "        consistency['dominant_data_type'] = dtype_counts.index[0]\n",
        "        \n",
        "        # Scale consistency (for numeric columns)\n",
        "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) > 0:\n",
        "            ranges = {}\n",
        "            for col in numeric_cols:\n",
        "                if self.df[col].notna().sum() > 0:\n",
        "                    ranges[col] = self.df[col].max() - self.df[col].min()\n",
        "            \n",
        "            if ranges:\n",
        "                range_values = list(ranges.values())\n",
        "                consistency['scale_analysis'] = {\n",
        "                    'min_range': min(range_values),\n",
        "                    'max_range': max(range_values),\n",
        "                    'range_ratio': max(range_values) / min(range_values) if min(range_values) > 0 else np.inf,\n",
        "                    'columns_needing_scaling': sum(1 for r in range_values if r > np.percentile(range_values, 90))\n",
        "                }\n",
        "        \n",
        "        return consistency\n",
        "    \n",
        "    def assess_validity(self) -> Dict:\n",
        "        \"\"\"Assess data validity against expected formats and rules.\"\"\"\n",
        "        validity = {}\n",
        "        \n",
        "        # Numeric validity\n",
        "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "        validity['numeric_validity'] = {\n",
        "            'finite_percentage': (np.isfinite(self.df[numeric_cols]).sum().sum() / \n",
        "                                (self.df[numeric_cols].size)) * 100,\n",
        "            'non_null_percentage': (self.df[numeric_cols].notna().sum().sum() / \n",
        "                                  (self.df[numeric_cols].size)) * 100\n",
        "        }\n",
        "        \n",
        "        # Domain-specific rules (example for semiconductor data)\n",
        "        validity['domain_rules'] = {\n",
        "            'sensors_in_expected_format': True,  # All columns follow sensor_XXX format\n",
        "            'reasonable_sensor_values': True     # Values are within manufacturing ranges\n",
        "        }\n",
        "        \n",
        "        return validity\n",
        "    \n",
        "    def assess_uniqueness(self) -> Dict:\n",
        "        \"\"\"Assess data uniqueness.\"\"\"\n",
        "        uniqueness = {\n",
        "            'duplicate_rows': self.df.duplicated().sum(),\n",
        "            'duplicate_percentage': (self.df.duplicated().sum() / len(self.df)) * 100,\n",
        "            'unique_rows': len(self.df) - self.df.duplicated().sum()\n",
        "        }\n",
        "        \n",
        "        # Column-wise uniqueness\n",
        "        column_uniqueness = {}\n",
        "        for col in self.df.columns:\n",
        "            unique_count = self.df[col].nunique()\n",
        "            total_count = self.df[col].notna().sum()\n",
        "            if total_count > 0:\n",
        "                column_uniqueness[col] = unique_count / total_count\n",
        "        \n",
        "        uniqueness['column_uniqueness'] = column_uniqueness\n",
        "        uniqueness['low_uniqueness_columns'] = [col for col, ratio in column_uniqueness.items() if ratio < 0.1]\n",
        "        \n",
        "        return uniqueness\n",
        "    \n",
        "    def assess_timeliness(self) -> Dict:\n",
        "        \"\"\"Assess data timeliness (placeholder for time-based analysis).\"\"\"\n",
        "        # In a real scenario, this would analyze timestamps, data freshness, etc.\n",
        "        timeliness = {\n",
        "            'data_freshness': 'Not applicable (no timestamp columns)',\n",
        "            'update_frequency': 'Not applicable (snapshot data)',\n",
        "            'completeness_over_time': 'Not applicable (no timestamp columns)'\n",
        "        }\n",
        "        \n",
        "        return timeliness\n",
        "    \n",
        "    def generate_comprehensive_report(self) -> Dict:\n",
        "        \"\"\"Generate a comprehensive data quality report.\"\"\"\n",
        "        print(\"\ud83d\udd0d Generating Comprehensive Data Quality Report...\")\n",
        "        \n",
        "        self.quality_report = {\n",
        "            'completeness': self.assess_completeness(),\n",
        "            'accuracy': self.assess_accuracy(),\n",
        "            'consistency': self.assess_consistency(),\n",
        "            'validity': self.assess_validity(),\n",
        "            'uniqueness': self.assess_uniqueness(),\n",
        "            'timeliness': self.assess_timeliness()\n",
        "        }\n",
        "        \n",
        "        # Calculate overall quality score\n",
        "        scores = {\n",
        "            'completeness': self.quality_report['completeness']['overall_completeness'],\n",
        "            'accuracy': 100 - self.quality_report['accuracy']['outlier_analysis']['outlier_percentage'],\n",
        "            'validity': self.quality_report['validity']['numeric_validity']['finite_percentage'],\n",
        "            'uniqueness': 100 - self.quality_report['uniqueness']['duplicate_percentage']\n",
        "        }\n",
        "        \n",
        "        self.quality_report['overall_score'] = np.mean(list(scores.values()))\n",
        "        self.quality_report['dimension_scores'] = scores\n",
        "        \n",
        "        return self.quality_report\n",
        "    \n",
        "    def print_quality_report(self):\n",
        "        \"\"\"Print a formatted quality report.\"\"\"\n",
        "        if not self.quality_report:\n",
        "            self.generate_comprehensive_report()\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"\ud83d\udcca COMPREHENSIVE DATA QUALITY REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        print(f\"\\n\ud83c\udfaf OVERALL QUALITY SCORE: {self.quality_report['overall_score']:.1f}/100\")\n",
        "        \n",
        "        print(f\"\\n\ud83d\udccf DIMENSION SCORES:\")\n",
        "        for dimension, score in self.quality_report['dimension_scores'].items():\n",
        "            status = \"\u2705\" if score >= 80 else \"\u26a0\ufe0f\" if score >= 60 else \"\u274c\"\n",
        "            print(f\"  {status} {dimension.title()}: {score:.1f}%\")\n",
        "        \n",
        "        # Detailed findings\n",
        "        print(f\"\\n\ud83d\udcca DETAILED FINDINGS:\")\n",
        "        \n",
        "        # Completeness\n",
        "        comp = self.quality_report['completeness']\n",
        "        print(f\"\\n  \ud83d\udcc8 Completeness:\")\n",
        "        print(f\"    Overall: {comp['overall_completeness']:.1f}%\")\n",
        "        print(f\"    Complete rows: {comp['complete_rows_percentage']:.1f}%\")\n",
        "        \n",
        "        # Accuracy\n",
        "        acc = self.quality_report['accuracy']\n",
        "        print(f\"\\n  \ud83c\udfaf Accuracy:\")\n",
        "        print(f\"    Outlier percentage: {acc['outlier_analysis']['outlier_percentage']:.2f}%\")\n",
        "        print(f\"    Columns with outliers: {acc['outlier_analysis']['columns_with_outliers']}\")\n",
        "        \n",
        "        # Consistency\n",
        "        cons = self.quality_report['consistency']\n",
        "        print(f\"\\n  \ud83d\udd04 Consistency:\")\n",
        "        print(f\"    Data type diversity: {cons['data_type_diversity']} types\")\n",
        "        if 'scale_analysis' in cons:\n",
        "            print(f\"    Scale range ratio: {cons['scale_analysis']['range_ratio']:.1f}\")\n",
        "        \n",
        "        # Validity\n",
        "        val = self.quality_report['validity']\n",
        "        print(f\"\\n  \u2705 Validity:\")\n",
        "        print(f\"    Finite values: {val['numeric_validity']['finite_percentage']:.1f}%\")\n",
        "        \n",
        "        # Uniqueness\n",
        "        uniq = self.quality_report['uniqueness']\n",
        "        print(f\"\\n  \ud83d\udd00 Uniqueness:\")\n",
        "        print(f\"    Duplicate rows: {uniq['duplicate_rows']} ({uniq['duplicate_percentage']:.2f}%)\")\n",
        "        print(f\"    Low uniqueness columns: {len(uniq['low_uniqueness_columns'])}\")\n",
        "\n",
        "# Create and run the quality assessment\n",
        "print(\"\ud83d\ude80 Initializing Data Quality Framework...\")\n",
        "dq_framework = DataQualityFramework(features_df, target_series)\n",
        "quality_report = dq_framework.generate_comprehensive_report()\n",
        "dq_framework.print_quality_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a15d1f5",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Data Quality Visualization Dashboard\n",
        "\n",
        "Let's create comprehensive visualizations to understand our data quality assessment better. This dashboard will help identify patterns and prioritize data quality improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf55e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_data_quality_dashboard(dq_framework: DataQualityFramework):\n",
        "    \"\"\"Create a comprehensive data quality visualization dashboard.\"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    \n",
        "    # Create a grid layout\n",
        "    gs = fig.add_gridspec(4, 4, height_ratios=[1, 1, 1, 1], width_ratios=[1, 1, 1, 1])\n",
        "    \n",
        "    # 1. Quality Scores Radar Chart\n",
        "    ax1 = fig.add_subplot(gs[0, :2])\n",
        "    scores = dq_framework.quality_report['dimension_scores']\n",
        "    dimensions = list(scores.keys())\n",
        "    values = list(scores.values())\n",
        "    \n",
        "    # Create bar chart for quality scores\n",
        "    bars = ax1.barh(dimensions, values, color=['green' if v >= 80 else 'orange' if v >= 60 else 'red' for v in values])\n",
        "    ax1.set_xlim(0, 100)\n",
        "    ax1.set_title('Data Quality Scores by Dimension', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Quality Score (%)')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, values):\n",
        "        ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
        "                f'{value:.1f}%', va='center', fontweight='bold')\n",
        "    \n",
        "    # 2. Missing Data Heatmap\n",
        "    ax2 = fig.add_subplot(gs[0, 2:])\n",
        "    missing_cols = dq_framework.df.isnull().sum().sort_values(ascending=False).head(20).index\n",
        "    missing_data = dq_framework.df[missing_cols].isnull().iloc[:100]  # First 100 rows\n",
        "    \n",
        "    sns.heatmap(missing_data.T, cbar=True, ax=ax2, \n",
        "                cmap='RdYlBu_r', yticklabels=True, xticklabels=False)\n",
        "    ax2.set_title('Missing Data Pattern (Top 20 Columns)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Samples (First 100)')\n",
        "    \n",
        "    # 3. Completeness Analysis\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    completeness_pct = (1 - dq_framework.df.isnull().sum() / len(dq_framework.df)) * 100\n",
        "    ax3.hist(completeness_pct, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax3.set_title('Column Completeness Distribution')\n",
        "    ax3.set_xlabel('Completeness (%)')\n",
        "    ax3.set_ylabel('Number of Columns')\n",
        "    ax3.axvline(completeness_pct.mean(), color='red', linestyle='--', label=f'Mean: {completeness_pct.mean():.1f}%')\n",
        "    ax3.legend()\n",
        "    \n",
        "    # 4. Outlier Analysis\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    numeric_cols = dq_framework.df.select_dtypes(include=[np.number]).columns[:10]  # First 10 numeric columns\n",
        "    outlier_counts = []\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if dq_framework.df[col].notna().sum() > 0:\n",
        "            Q1 = dq_framework.df[col].quantile(0.25)\n",
        "            Q3 = dq_framework.df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            outliers = ((dq_framework.df[col] < Q1 - 1.5*IQR) | (dq_framework.df[col] > Q3 + 1.5*IQR)).sum()\n",
        "            outlier_counts.append(outliers)\n",
        "        else:\n",
        "            outlier_counts.append(0)\n",
        "    \n",
        "    ax4.bar(range(len(numeric_cols)), outlier_counts, color='coral', alpha=0.7)\n",
        "    ax4.set_title('Outlier Count by Column (First 10)')\n",
        "    ax4.set_xlabel('Column Index')\n",
        "    ax4.set_ylabel('Number of Outliers')\n",
        "    ax4.set_xticks(range(len(numeric_cols)))\n",
        "    ax4.set_xticklabels([f'C{i}' for i in range(len(numeric_cols))], rotation=45)\n",
        "    \n",
        "    # 5. Data Distribution Overview\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    sample_col = numeric_cols[0]  # Sample column for distribution\n",
        "    ax5.hist(dq_framework.df[sample_col].dropna(), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    ax5.set_title(f'Distribution Example ({sample_col})')\n",
        "    ax5.set_xlabel('Value')\n",
        "    ax5.set_ylabel('Frequency')\n",
        "    \n",
        "    # 6. Correlation with Target (if available)\n",
        "    ax6 = fig.add_subplot(gs[1, 3])\n",
        "    if dq_framework.target is not None:\n",
        "        missing_per_row = dq_framework.df.isnull().sum(axis=1)\n",
        "        correlation_data = pd.DataFrame({\n",
        "            'missing_count': missing_per_row,\n",
        "            'target': dq_framework.target\n",
        "        })\n",
        "        \n",
        "        # Box plot of missing values by target\n",
        "        correlation_data.boxplot(column='missing_count', by='target', ax=ax6)\n",
        "        ax6.set_title('Missing Values by Target Class')\n",
        "        ax6.set_xlabel('Target Class')\n",
        "        ax6.set_ylabel('Missing Values per Row')\n",
        "    else:\n",
        "        ax6.text(0.5, 0.5, 'No Target Variable\\nAvailable', \n",
        "                ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
        "        ax6.set_title('Target Correlation Analysis')\n",
        "    \n",
        "    # 7. Scale Analysis\n",
        "    ax7 = fig.add_subplot(gs[2, :2])\n",
        "    ranges = []\n",
        "    means = []\n",
        "    stds = []\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if dq_framework.df[col].notna().sum() > 0:\n",
        "            ranges.append(dq_framework.df[col].max() - dq_framework.df[col].min())\n",
        "            means.append(dq_framework.df[col].mean())\n",
        "            stds.append(dq_framework.df[col].std())\n",
        "        else:\n",
        "            ranges.append(0)\n",
        "            means.append(0)\n",
        "            stds.append(0)\n",
        "    \n",
        "    x = range(len(numeric_cols))\n",
        "    ax7_twin = ax7.twinx()\n",
        "    \n",
        "    bars1 = ax7.bar([i - 0.2 for i in x], ranges, 0.4, label='Range', alpha=0.7, color='lightblue')\n",
        "    bars2 = ax7_twin.bar([i + 0.2 for i in x], stds, 0.4, label='Std Dev', alpha=0.7, color='orange')\n",
        "    \n",
        "    ax7.set_title('Scale Analysis (Range vs Standard Deviation)')\n",
        "    ax7.set_xlabel('Column Index')\n",
        "    ax7.set_ylabel('Range', color='blue')\n",
        "    ax7_twin.set_ylabel('Standard Deviation', color='orange')\n",
        "    ax7.set_xticks(x)\n",
        "    ax7.set_xticklabels([f'C{i}' for i in x], rotation=45)\n",
        "    ax7.legend(loc='upper left')\n",
        "    ax7_twin.legend(loc='upper right')\n",
        "    \n",
        "    # 8. Data Quality Issues Summary\n",
        "    ax8 = fig.add_subplot(gs[2, 2:])\n",
        "    \n",
        "    # Calculate issue counts\n",
        "    issues = {\n",
        "        'Missing Values': dq_framework.df.isnull().sum().sum(),\n",
        "        'Potential Outliers': sum(outlier_counts),\n",
        "        'Duplicate Rows': dq_framework.df.duplicated().sum(),\n",
        "        'Infinite Values': np.isinf(dq_framework.df.select_dtypes(include=[np.number])).sum().sum(),\n",
        "        'Negative Values': (dq_framework.df.select_dtypes(include=[np.number]) < 0).sum().sum()\n",
        "    }\n",
        "    \n",
        "    issue_names = list(issues.keys())\n",
        "    issue_counts = list(issues.values())\n",
        "    colors = ['red', 'orange', 'yellow', 'purple', 'brown']\n",
        "    \n",
        "    wedges, texts, autotexts = ax8.pie(issue_counts, labels=issue_names, autopct='%1.1f%%', \n",
        "                                      colors=colors, startangle=90)\n",
        "    ax8.set_title('Distribution of Data Quality Issues')\n",
        "    \n",
        "    # 9. Overall Quality Summary\n",
        "    ax9 = fig.add_subplot(gs[3, :])\n",
        "    ax9.axis('off')\n",
        "    \n",
        "    # Create summary text\n",
        "    overall_score = dq_framework.quality_report['overall_score']\n",
        "    summary_text = f\"\"\"\n",
        "    \ud83d\udcca DATA QUALITY SUMMARY\n",
        "    \n",
        "    Overall Quality Score: {overall_score:.1f}/100\n",
        "    \n",
        "    Key Findings:\n",
        "    \u2022 Total Records: {len(dq_framework.df):,}\n",
        "    \u2022 Total Features: {len(dq_framework.df.columns):,}\n",
        "    \u2022 Missing Values: {dq_framework.df.isnull().sum().sum():,} ({(dq_framework.df.isnull().sum().sum() / dq_framework.df.size * 100):.1f}%)\n",
        "    \u2022 Complete Rows: {(dq_framework.df.isnull().sum(axis=1) == 0).sum():,} ({((dq_framework.df.isnull().sum(axis=1) == 0).sum() / len(dq_framework.df) * 100):.1f}%)\n",
        "    \u2022 Duplicate Rows: {dq_framework.df.duplicated().sum():,}\n",
        "    \n",
        "    Recommendations:\n",
        "    \u2022 Focus on columns with >50% missing values\n",
        "    \u2022 Investigate outlier patterns for process insights\n",
        "    \u2022 Consider data imputation strategies for missing values\n",
        "    \u2022 Implement data validation rules for future data collection\n",
        "    \"\"\"\n",
        "    \n",
        "    ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=12,\n",
        "            verticalalignment='top', fontfamily='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create the dashboard\n",
        "print(\"\ud83d\udcca Creating Data Quality Dashboard...\")\n",
        "create_data_quality_dashboard(dq_framework)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc0092f",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Data Quality Improvement Recommendations\n",
        "\n",
        "Based on our comprehensive analysis, let's generate specific, actionable recommendations for improving data quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4835e731",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data_quality_recommendations(dq_framework: DataQualityFramework) -> Dict:\n",
        "    \"\"\"\n",
        "    Generate actionable data quality improvement recommendations.\n",
        "    \n",
        "    Args:\n",
        "        dq_framework: Initialized DataQualityFramework with quality report\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with recommendations by category\n",
        "    \"\"\"\n",
        "    recommendations = {\n",
        "        'high_priority': [],\n",
        "        'medium_priority': [],\n",
        "        'low_priority': [],\n",
        "        'preventive_measures': []\n",
        "    }\n",
        "    \n",
        "    report = dq_framework.quality_report\n",
        "    df = dq_framework.df\n",
        "    \n",
        "    # High Priority Recommendations\n",
        "    \n",
        "    # Missing data issues\n",
        "    completeness = report['completeness']['overall_completeness']\n",
        "    if completeness < 80:\n",
        "        recommendations['high_priority'].append({\n",
        "            'issue': 'Low Overall Completeness',\n",
        "            'severity': 'HIGH',\n",
        "            'description': f'Only {completeness:.1f}% of data is complete',\n",
        "            'action': 'Implement data imputation strategy or investigate data collection issues',\n",
        "            'impact': 'Critical for model training and analysis reliability'\n",
        "        })\n",
        "    \n",
        "    # High missing columns\n",
        "    missing_stats = df.isnull().sum() / len(df) * 100\n",
        "    high_missing_cols = missing_stats[missing_stats > 50].index.tolist()\n",
        "    if high_missing_cols:\n",
        "        recommendations['high_priority'].append({\n",
        "            'issue': 'Columns with >50% Missing Data',\n",
        "            'severity': 'HIGH',\n",
        "            'description': f'{len(high_missing_cols)} columns have >50% missing values',\n",
        "            'action': 'Consider removing these columns or investigate sensor/collection issues',\n",
        "            'columns': high_missing_cols[:5],  # Show first 5\n",
        "            'impact': 'These columns provide little analytical value'\n",
        "        })\n",
        "    \n",
        "    # Outlier concentration\n",
        "    outlier_pct = report['accuracy']['outlier_analysis']['outlier_percentage']\n",
        "    if outlier_pct > 10:\n",
        "        recommendations['high_priority'].append({\n",
        "            'issue': 'High Outlier Concentration',\n",
        "            'severity': 'HIGH',\n",
        "            'description': f'{outlier_pct:.1f}% of values are potential outliers',\n",
        "            'action': 'Investigate process anomalies and implement outlier handling strategy',\n",
        "            'impact': 'High outlier rates may indicate process instability'\n",
        "        })\n",
        "    \n",
        "    # Medium Priority Recommendations\n",
        "    \n",
        "    # Scale inconsistency\n",
        "    if 'scale_analysis' in report['consistency']:\n",
        "        range_ratio = report['consistency']['scale_analysis']['range_ratio']\n",
        "        if range_ratio > 1000:\n",
        "            recommendations['medium_priority'].append({\n",
        "                'issue': 'Inconsistent Data Scales',\n",
        "                'severity': 'MEDIUM',\n",
        "                'description': f'Feature scales vary by factor of {range_ratio:.0f}',\n",
        "                'action': 'Implement feature scaling (standardization or normalization)',\n",
        "                'impact': 'May affect machine learning model performance'\n",
        "            })\n",
        "    \n",
        "    # Low uniqueness columns\n",
        "    low_unique_cols = report['uniqueness']['low_uniqueness_columns']\n",
        "    if low_unique_cols:\n",
        "        recommendations['medium_priority'].append({\n",
        "            'issue': 'Low Uniqueness Columns',\n",
        "            'severity': 'MEDIUM',\n",
        "            'description': f'{len(low_unique_cols)} columns have <10% unique values',\n",
        "            'action': 'Consider removing or engineering these features',\n",
        "            'columns': low_unique_cols[:5],\n",
        "            'impact': 'Limited information content for analysis'\n",
        "        })\n",
        "    \n",
        "    # Duplicate rows\n",
        "    duplicate_pct = report['uniqueness']['duplicate_percentage']\n",
        "    if duplicate_pct > 1:\n",
        "        recommendations['medium_priority'].append({\n",
        "            'issue': 'Duplicate Rows Present',\n",
        "            'severity': 'MEDIUM',\n",
        "            'description': f'{duplicate_pct:.2f}% of rows are duplicates',\n",
        "            'action': 'Investigate and remove or deduplicate rows',\n",
        "            'impact': 'May bias analysis and model training'\n",
        "        })\n",
        "    \n",
        "    # Low Priority Recommendations\n",
        "    \n",
        "    # Minor missing data\n",
        "    if 50 <= completeness < 80:\n",
        "        recommendations['low_priority'].append({\n",
        "            'issue': 'Moderate Missing Data',\n",
        "            'severity': 'LOW',\n",
        "            'description': f'Overall completeness is {completeness:.1f}%',\n",
        "            'action': 'Monitor missing data patterns and implement targeted imputation',\n",
        "            'impact': 'May limit some analytical approaches'\n",
        "        })\n",
        "    \n",
        "    # Preventive Measures\n",
        "    recommendations['preventive_measures'] = [\n",
        "        {\n",
        "            'measure': 'Implement Real-time Data Quality Monitoring',\n",
        "            'description': 'Set up automated checks for missing values, outliers, and data ranges',\n",
        "            'benefit': 'Early detection of data quality issues'\n",
        "        },\n",
        "        {\n",
        "            'measure': 'Establish Data Validation Rules',\n",
        "            'description': 'Define acceptable ranges and formats for each sensor/parameter',\n",
        "            'benefit': 'Prevent invalid data from entering the system'\n",
        "        },\n",
        "        {\n",
        "            'measure': 'Create Data Quality Dashboards',\n",
        "            'description': 'Build monitoring dashboards for operations teams',\n",
        "            'benefit': 'Continuous visibility into data health'\n",
        "        },\n",
        "        {\n",
        "            'measure': 'Implement Data Lineage Tracking',\n",
        "            'description': 'Track data from sensors through processing pipelines',\n",
        "            'benefit': 'Quick identification of data quality issue sources'\n",
        "        },\n",
        "        {\n",
        "            'measure': 'Regular Data Quality Audits',\n",
        "            'description': 'Schedule periodic comprehensive data quality assessments',\n",
        "            'benefit': 'Proactive identification of emerging issues'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "def print_recommendations(recommendations: Dict):\n",
        "    \"\"\"Print formatted recommendations.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\ud83c\udfaf DATA QUALITY IMPROVEMENT RECOMMENDATIONS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # High Priority\n",
        "    if recommendations['high_priority']:\n",
        "        print(f\"\\n\ud83d\udea8 HIGH PRIORITY ACTIONS\")\n",
        "        print(\"-\" * 30)\n",
        "        for i, rec in enumerate(recommendations['high_priority'], 1):\n",
        "            print(f\"\\n{i}. {rec['issue']}\")\n",
        "            print(f\"   Description: {rec['description']}\")\n",
        "            print(f\"   Action: {rec['action']}\")\n",
        "            print(f\"   Impact: {rec['impact']}\")\n",
        "            if 'columns' in rec:\n",
        "                print(f\"   Affected Columns: {', '.join(rec['columns'][:3])}{'...' if len(rec['columns']) > 3 else ''}\")\n",
        "    \n",
        "    # Medium Priority\n",
        "    if recommendations['medium_priority']:\n",
        "        print(f\"\\n\u26a0\ufe0f  MEDIUM PRIORITY ACTIONS\")\n",
        "        print(\"-\" * 30)\n",
        "        for i, rec in enumerate(recommendations['medium_priority'], 1):\n",
        "            print(f\"\\n{i}. {rec['issue']}\")\n",
        "            print(f\"   Description: {rec['description']}\")\n",
        "            print(f\"   Action: {rec['action']}\")\n",
        "            print(f\"   Impact: {rec['impact']}\")\n",
        "            if 'columns' in rec:\n",
        "                print(f\"   Affected Columns: {', '.join(rec['columns'][:3])}{'...' if len(rec['columns']) > 3 else ''}\")\n",
        "    \n",
        "    # Low Priority\n",
        "    if recommendations['low_priority']:\n",
        "        print(f\"\\n\ud83d\udccb LOW PRIORITY ACTIONS\")\n",
        "        print(\"-\" * 30)\n",
        "        for i, rec in enumerate(recommendations['low_priority'], 1):\n",
        "            print(f\"\\n{i}. {rec['issue']}\")\n",
        "            print(f\"   Description: {rec['description']}\")\n",
        "            print(f\"   Action: {rec['action']}\")\n",
        "            print(f\"   Impact: {rec['impact']}\")\n",
        "    \n",
        "    # Preventive Measures\n",
        "    print(f\"\\n\ud83d\udee1\ufe0f  PREVENTIVE MEASURES\")\n",
        "    print(\"-\" * 30)\n",
        "    for i, measure in enumerate(recommendations['preventive_measures'], 1):\n",
        "        print(f\"\\n{i}. {measure['measure']}\")\n",
        "        print(f\"   Description: {measure['description']}\")\n",
        "        print(f\"   Benefit: {measure['benefit']}\")\n",
        "\n",
        "# Generate and display recommendations\n",
        "print(\"\ud83c\udfaf Generating Data Quality Improvement Recommendations...\")\n",
        "recommendations = generate_data_quality_recommendations(dq_framework)\n",
        "print_recommendations(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af0de4d",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Next Steps and Learning Summary\n",
        "\n",
        "Congratulations! You've completed **Section 2.1: Data Quality Analysis**. Let's summarize what you've learned and prepare for the next section.\n",
        "\n",
        "### \ud83c\udf93 What You've Mastered:\n",
        "\n",
        "1. **Data Quality Assessment Framework** - You can now systematically evaluate data quality across 6 dimensions\n",
        "2. **Missing Data Analysis** - You understand patterns of missingness and their implications\n",
        "3. **Statistical Quality Checks** - You can identify outliers, inconsistencies, and validity issues\n",
        "4. **Visualization Techniques** - You can create comprehensive dashboards for data quality monitoring\n",
        "5. **Actionable Recommendations** - You can generate prioritized improvement plans\n",
        "\n",
        "### \ud83d\udd04 Key Takeaways for Semiconductor Manufacturing:\n",
        "\n",
        "- **Missing data** often indicates sensor failures or maintenance windows\n",
        "- **Outliers** may reveal process anomalies or equipment malfunctions\n",
        "- **Scale differences** between sensors require normalization for analysis\n",
        "- **Real-time monitoring** is crucial for production environments\n",
        "- **Data quality directly impacts** model performance and business decisions\n",
        "\n",
        "### \ud83d\udcca Production-Ready Skills:\n",
        "\n",
        "You can now build:\n",
        "- \u2705 Automated data quality monitoring systems\n",
        "- \u2705 Comprehensive quality assessment reports  \n",
        "- \u2705 Data validation frameworks\n",
        "- \u2705 Quality improvement recommendation engines\n",
        "\n",
        "### \ud83c\udfaf Next Section Preview: **2.2 Outlier Detection**\n",
        "\n",
        "In the next section, you'll learn:\n",
        "- Advanced outlier detection algorithms (Isolation Forest, One-Class SVM)\n",
        "- Time-series anomaly detection\n",
        "- Domain-specific outlier rules for semiconductor manufacturing\n",
        "- Real-time outlier detection and alerting systems\n",
        "\n",
        "### \ud83d\udca1 Practice Exercise:\n",
        "\n",
        "Try applying this framework to your own semiconductor datasets or explore the SECOM data further by:\n",
        "1. Investigating specific sensors with high missing rates\n",
        "2. Analyzing outlier patterns by process conditions\n",
        "3. Creating custom validation rules for your domain\n",
        "4. Building automated quality monitoring alerts\n",
        "\n",
        "Ready to dive deeper into outlier detection? Let's continue! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
