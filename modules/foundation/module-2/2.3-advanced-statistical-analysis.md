# 2.3 Advanced Statistical Analysis with ANOVA for Semiconductor Manufacturing

## üìö Overview

Analysis of Variance (ANOVA) is a fundamental statistical technique for understanding sources of variation in semiconductor manufacturing processes. Unlike simple outlier detection, ANOVA allows engineers to systematically decompose process variation, compare multiple groups or conditions, and optimize manufacturing parameters through designed experiments.

This comprehensive guide covers theoretical foundations, practical implementations, and semiconductor-specific applications of ANOVA, factorial designs, and Design of Experiments (DOE) methodologies.

## üéØ Learning Objectives

After studying this document, you will understand:

- ANOVA fundamentals and assumptions for semiconductor applications
- One-way, two-way, and multi-factor ANOVA techniques
- Design of Experiments (DOE) principles and factorial designs
- Response Surface Methodology (RSM) for process optimization
- Mixed-effects models for hierarchical manufacturing data
- Multiple comparison procedures and effect size estimation
- Statistical power analysis and sample size planning

## üî¨ ANOVA Fundamentals for Manufacturing

### What is ANOVA?

**Analysis of Variance (ANOVA)** partitions total variation in a response variable into components attributable to different sources:

```
Total Variation = Between-Group Variation + Within-Group Variation
```

**Key Insight**: If between-group variation is significantly larger than within-group variation, the groups likely represent different populations (different process conditions, tools, recipes, etc.).

### Semiconductor Manufacturing Context

In semiconductor manufacturing, ANOVA helps answer critical questions:

- **Tool Comparison**: Do different etch chambers produce different results?
- **Recipe Optimization**: Which parameter settings minimize defect density?
- **Batch Effects**: Is there significant lot-to-lot variation?
- **Interaction Effects**: Do temperature and pressure interact to affect yield?
- **Supplier Qualification**: Are materials from different vendors equivalent?

### ANOVA Assumptions

1. **Independence**: Observations are independent
2. **Normality**: Residuals follow normal distribution
3. **Homoscedasticity**: Equal variances across groups
4. **Linearity**: (for factorial designs) Response is linear in factors

**Semiconductor Considerations**:

- **Spatial Correlation**: Wafers from same lot may be correlated
- **Temporal Trends**: Tool drift over time violates independence
- **Non-Normal Responses**: Defect counts, yield percentages
- **Heteroscedasticity**: Variance may depend on process mean

## üìä One-Way ANOVA

### Theory and Formulation

**Research Question**: Does the mean response differ across k groups?

**Hypotheses**:

- H‚ÇÄ: Œº‚ÇÅ = Œº‚ÇÇ = ... = Œº‚Çñ (all group means equal)
- H‚ÇÅ: At least one mean differs

**Model**:

```
y·µ¢‚±º = Œº + Œ±·µ¢ + Œµ·µ¢‚±º

Where:
- y·µ¢‚±º = observation j in group i
- Œº = overall mean
- Œ±·µ¢ = effect of group i
- Œµ·µ¢‚±º ~ N(0, œÉ¬≤) = random error
```

**Sum of Squares Decomposition**:

```
SST = SSB + SSW

Where:
- SST = Total Sum of Squares = Œ£·µ¢‚±º(y·µ¢‚±º - »≥)¬≤
- SSB = Between Sum of Squares = Œ£·µ¢n·µ¢(»≥·µ¢ - »≥)¬≤
- SSW = Within Sum of Squares = Œ£·µ¢‚±º(y·µ¢‚±º - »≥·µ¢)¬≤
```

**F-Statistic**:

```
F = MSB/MSW = (SSB/(k-1))/(SSW/(N-k))

Where:
- k = number of groups
- N = total sample size
- Under H‚ÇÄ: F ~ F(k-1, N-k)
```

### Manufacturing Example: Tool Comparison

**Scenario**: Comparing etch rates across 4 different plasma etch tools.

```python
import pandas as pd
import numpy as np
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Example data: etch rates (nm/min) for 4 tools
data = pd.DataFrame({
    'tool': ['A']*10 + ['B']*10 + ['C']*10 + ['D']*10,
    'etch_rate': [
        # Tool A
        [245, 248, 242, 247, 249, 244, 246, 243, 248, 245],
        # Tool B  
        [252, 255, 251, 254, 256, 253, 255, 252, 254, 253],
        # Tool C
        [239, 241, 238, 240, 242, 239, 241, 238, 240, 239],
        # Tool D
        [248, 250, 247, 249, 251, 248, 250, 247, 249, 248]
    ].flatten()
})

# Perform one-way ANOVA
model = ols('etch_rate ~ C(tool)', data=data).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)
```

**Interpretation**:

- **F-statistic**: Measures ratio of between-tool to within-tool variation
- **p-value**: Probability of observing such differences if tools are equivalent
- **Effect Size (Œ∑¬≤)**: Proportion of variation explained by tool differences

### Post-Hoc Analysis

When ANOVA is significant, post-hoc tests identify which specific groups differ:

**Tukey's HSD (Honestly Significant Difference)**:

```python
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Tukey's post-hoc test
tukey = pairwise_tukeyhsd(data['etch_rate'], data['tool'])
print(tukey)
```

**Bonferroni Correction**:

```python
from statsmodels.stats.multicomp import MultiComparison

mc = MultiComparison(data['etch_rate'], data['tool'])
bonferroni_result = mc.allpairwise(stats.ttest_ind, method='bonf')
```

## üìà Two-Way ANOVA

### Theory and Applications

**Research Question**: How do two factors (and their interaction) affect the response?

**Model**:

```
y·µ¢‚±º‚Çñ = Œº + Œ±·µ¢ + Œ≤‚±º + (Œ±Œ≤)·µ¢‚±º + Œµ·µ¢‚±º‚Çñ

Where:
- Œ±·µ¢ = main effect of factor A (level i)
- Œ≤‚±º = main effect of factor B (level j)
- (Œ±Œ≤)·µ¢‚±º = interaction effect
- Œµ·µ¢‚±º‚Çñ ~ N(0, œÉ¬≤)
```

**Sum of Squares Decomposition**:

```
SST = SSA + SSB + SSAB + SSE

Where:
- SSA = variation due to factor A
- SSB = variation due to factor B
- SSAB = variation due to A√óB interaction
- SSE = error (residual) variation
```

### Manufacturing Example: Temperature √ó Pressure Effects

**Scenario**: Studying how temperature and pressure affect oxide thickness uniformity.

```python
# Factorial design: 3 temperatures √ó 3 pressures
design_data = pd.DataFrame({
    'temperature': [150]*9 + [175]*9 + [200]*9,
    'pressure': [1.0, 1.5, 2.0]*9,
    'uniformity': [
        # Temperature 150
        [2.1, 2.3, 2.8, 2.0, 2.2, 2.7, 1.9, 2.1, 2.6],
        # Temperature 175
        [1.8, 1.9, 2.1, 1.7, 1.8, 2.0, 1.6, 1.7, 1.9],
        # Temperature 200
        [2.5, 2.1, 1.8, 2.4, 2.0, 1.7, 2.3, 1.9, 1.6]
    ].flatten()
})

# Two-way ANOVA with interaction
model_2way = ols('uniformity ~ C(temperature) + C(pressure) + C(temperature):C(pressure)', 
                 data=design_data).fit()
anova_2way = sm.stats.anova_lm(model_2way, typ=2)
print(anova_2way)
```

### Interaction Effects

**Statistical Interaction**: When the effect of one factor depends on the level of another factor.

**Visual Detection**:

```python
import matplotlib.pyplot as plt

# Interaction plot
fig, ax = plt.subplots(figsize=(10, 6))
for temp in design_data['temperature'].unique():
    temp_data = design_data[design_data['temperature'] == temp]
    means = temp_data.groupby('pressure')['uniformity'].mean()
    ax.plot(means.index, means.values, marker='o', label=f'Temp {temp}¬∞C')

ax.set_xlabel('Pressure (Torr)')
ax.set_ylabel('Uniformity (%)')
ax.set_title('Temperature √ó Pressure Interaction')
ax.legend()
plt.show()
```

**Interpretation**:

- **Parallel lines**: No interaction
- **Non-parallel lines**: Interaction present
- **Crossing lines**: Strong interaction (disordinal)

## üî¨ Design of Experiments (DOE)

### Principles of Experimental Design

1. **Randomization**: Eliminates systematic bias
2. **Replication**: Enables error estimation
3. **Blocking**: Controls known sources of variation
4. **Factorial Structure**: Studies multiple factors simultaneously

### Factorial Designs

**2·µè Factorial Design**: k factors, each at 2 levels

**Advantages**:

- Efficient screening of many factors
- Estimates main effects and interactions
- Foundation for optimization studies

**Example**: 2¬≥ Design for CVD Process

**Factors**:

- A: Temperature (low/high)
- B: Pressure (low/high)  
- C: Flow Rate (low/high)

**Design Matrix**:

```
Run | A | B | C | Response
----|---|---|---|----------
1   |-1|-1|-1 |   y‚ÇÅ
2   |+1|-1|-1 |   y‚ÇÇ
3   |-1|+1|-1 |   y‚ÇÉ
4   |+1|+1|-1 |   y‚ÇÑ
5   |-1|-1|+1 |   y‚ÇÖ
6   |+1|-1|+1 |   y‚ÇÜ
7   |-1|+1|+1 |   y‚Çá
8   |+1|+1|+1 |   y‚Çà
```

**Effect Calculation**:

```
Main Effect A = (y‚ÇÇ + y‚ÇÑ + y‚ÇÜ + y‚Çà - y‚ÇÅ - y‚ÇÉ - y‚ÇÖ - y‚Çá)/4
AB Interaction = (y‚ÇÅ + y‚ÇÇ + y‚Çá + y‚Çà - y‚ÇÉ - y‚ÇÑ - y‚ÇÖ - y‚ÇÜ)/4
```

### Fractional Factorial Designs

When full factorial is too expensive, use carefully chosen subset:

**2·µè‚Åª·µñ Design**: 2·µè‚Åª·µñ runs instead of 2·µè

**Example**: 2‚Åµ‚Åª¬π design (16 runs instead of 32)

- Studies 5 factors in 16 experiments
- Some higher-order interactions confounded
- Assumes higher-order interactions negligible

```python
from pyDOE2 import fracfact

# Generate 2^(5-1) fractional factorial
design = fracfact("a b c d e")
design_df = pd.DataFrame(design, columns=['A', 'B', 'C', 'D', 'E'])
```

### Response Surface Methodology (RSM)

**Purpose**: Optimize response by modeling its relationship with factors

**Common Designs**:

- **Central Composite Design (CCD)**
- **Box-Behnken Design**
- **Optimal Designs**

**Second-Order Model**:

```
y = Œ≤‚ÇÄ + Œ£Œ≤·µ¢x·µ¢ + Œ£Œ≤·µ¢·µ¢x·µ¢¬≤ + Œ£Œ£Œ≤·µ¢‚±ºx·µ¢x‚±º + Œµ

Where:
- Linear terms: Œ≤·µ¢x·µ¢
- Quadratic terms: Œ≤·µ¢·µ¢x·µ¢¬≤
- Interaction terms: Œ≤·µ¢‚±ºx·µ¢x‚±º
```

**Central Composite Design Example**:

```python
from pyDOE2 import ccdesign

# CCD for 3 factors
ccd = ccdesign(3, center=(2,2), alpha='orthogonal')
ccd_df = pd.DataFrame(ccd, columns=['Temperature', 'Pressure', 'Flow'])
```

## üìä Advanced ANOVA Topics

### Mixed-Effects Models

**Application**: Hierarchical data structure common in manufacturing

**Example Structure**:

- **Fixed Effects**: Controllable factors (temperature, pressure)
- **Random Effects**: Uncontrollable factors (lot, operator, tool)

**Model**:

```
y·µ¢‚±º‚Çñ = Œº + Œ±·µ¢ + Œ≤‚±º + (Œ±Œ≤)·µ¢‚±º + Œ≥‚Çñ + Œµ·µ¢‚±º‚Çñ

Where:
- Œ±·µ¢ = fixed effect (treatment i)
- Œ≥‚Çñ ~ N(0, œÉ¬≤Œ≥) = random effect (block k)
```

**Implementation**:

```python
import statsmodels.formula.api as smf

# Mixed-effects model
mixed_model = smf.mixedlm("response ~ treatment", 
                         data=data, 
                         groups=data["lot"]).fit()
```

### Nested ANOVA

**Structure**: Groups nested within larger groups

**Manufacturing Example**:

- Wafers nested within lots
- Lots nested within suppliers
- Sites nested within wafers

**Model**:

```
y·µ¢‚±º‚Çñ = Œº + Œ±·µ¢ + Œ≤‚±º(·µ¢) + Œµ‚Çñ(·µ¢‚±º)

Where:
- Œ±·µ¢ = supplier effect
- Œ≤‚±º(·µ¢) = lot effect within supplier i
- Œµ‚Çñ(·µ¢‚±º) = wafer effect within lot j of supplier i
```

### Repeated Measures ANOVA

**Application**: Same experimental units measured multiple times

**Manufacturing Example**: Tool drift monitoring over time

**Model Considerations**:

- **Sphericity**: Assumption of equal correlations
- **Greenhouse-Geisser Correction**: Adjusts for sphericity violations
- **Compound Symmetry**: Specific correlation structure

## üîß ANOVA Diagnostics and Assumptions

### Checking Normality

**Methods**:

1. **Q-Q Plots**: Visual assessment
2. **Shapiro-Wilk Test**: Statistical test
3. **Anderson-Darling Test**: More powerful for tail deviations

```python
from scipy.stats import shapiro, anderson
import matplotlib.pyplot as plt

# Q-Q plot
from scipy.stats import probplot
fig, ax = plt.subplots()
probplot(residuals, dist="norm", plot=ax)
ax.set_title("Q-Q Plot of Residuals")

# Shapiro-Wilk test
stat, p_value = shapiro(residuals)
print(f"Shapiro-Wilk: statistic={stat:.4f}, p-value={p_value:.4f}")
```

### Checking Homoscedasticity

**Methods**:

1. **Residual Plots**: Plot residuals vs fitted values
2. **Levene's Test**: Tests equality of variances
3. **Bartlett's Test**: More powerful but assumes normality

```python
from scipy.stats import levene

# Levene's test
groups = [data[data['group']==g]['response'] for g in data['group'].unique()]
stat, p_value = levene(*groups)
print(f"Levene's test: statistic={stat:.4f}, p-value={p_value:.4f}")

# Residual plot
plt.scatter(fitted_values, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted Values')
```

### Checking Independence

**Methods**:

1. **Durbin-Watson Test**: Tests for autocorrelation
2. **Run Test**: Tests for randomness
3. **Residual Time Series Plot**: Visual inspection

```python
from statsmodels.stats.diagnostic import durbin_watson

# Durbin-Watson test
dw_stat = durbin_watson(residuals)
print(f"Durbin-Watson statistic: {dw_stat:.4f}")
# Values close to 2 indicate no autocorrelation
```

## üè≠ Semiconductor-Specific Applications

### Tool Matching Studies

**Objective**: Ensure equivalent performance across multiple tools

**Design**:

- **Balanced design**: Equal number of wafers per tool
- **Randomization**: Random assignment of wafers to tools
- **Blocking**: Control for lot effects

**Analysis**:

```python
# Tool matching ANOVA
tool_model = ols('thickness ~ C(tool) + C(lot)', data=tool_data).fit()
tool_anova = sm.stats.anova_lm(tool_model, typ=2)

# Calculate tool-to-tool variation
tool_variance = tool_anova.loc['C(tool)', 'sum_sq'] / tool_anova.loc['C(tool)', 'df']
error_variance = tool_anova.loc['Residual', 'mean_sq']
```

### Process Characterization

**Split-Plot Designs**: When some factors harder to change

**Example**: Temperature (hard to change) and gas flow (easy to change)

**Structure**:

- **Whole plots**: Different temperatures
- **Subplots**: Different gas flows within each temperature

```python
# Split-plot ANOVA
split_model = smf.mixedlm("response ~ C(temperature) + C(gas_flow) + C(temperature):C(gas_flow)",
                         data=split_data,
                         groups=split_data["whole_plot"]).fit()
```

### Gauge R&R Studies

**Purpose**: Quantify measurement system variation

**Components**:

- **Repeatability**: Same operator, same gauge
- **Reproducibility**: Different operators, same gauge
- **Part-to-part**: True variation

**ANOVA Model**:

```
y·µ¢‚±º‚Çñ = Œº + P·µ¢ + O‚±º + (PO)·µ¢‚±º + Œµ·µ¢‚±º‚Çñ

Where:
- P·µ¢ = part effect
- O‚±º = operator effect
- (PO)·µ¢‚±º = part √ó operator interaction
```

**Variance Components**:

```python
def calculate_grr_components(anova_table):
    # Extract variance components from ANOVA table
    var_part = (anova_table.loc['part', 'mean_sq'] - 
                anova_table.loc['part:operator', 'mean_sq']) / (n_operators * n_trials)
    var_operator = (anova_table.loc['operator', 'mean_sq'] - 
                   anova_table.loc['part:operator', 'mean_sq']) / (n_parts * n_trials)
    var_interaction = (anova_table.loc['part:operator', 'mean_sq'] - 
                      anova_table.loc['Residual', 'mean_sq']) / n_trials
    var_repeatability = anova_table.loc['Residual', 'mean_sq']
    
    return {
        'part': var_part,
        'operator': var_operator, 
        'interaction': var_interaction,
        'repeatability': var_repeatability
    }
```

## üìä Effect Size and Practical Significance

### Measures of Effect Size

**Eta-squared (Œ∑¬≤)**:

```
Œ∑¬≤ = SSeffect / SStotal
```

- Proportion of variance explained by the effect
- Range: 0 to 1

**Partial Eta-squared (Œ∑‚Çö¬≤)**:

```
Œ∑‚Çö¬≤ = SSeffect / (SSeffect + SSerror)
```

- Controls for other effects in the model

**Cohen's Guidelines**:

- Small effect: Œ∑¬≤ = 0.01
- Medium effect: Œ∑¬≤ = 0.06  
- Large effect: Œ∑¬≤ = 0.14

### Practical vs Statistical Significance

**Manufacturing Context**:

- Statistical significance ‚â† practical importance
- Consider engineering tolerance limits
- Cost-benefit analysis of process changes

**Example**:

```python
def practical_significance(effect_size, tolerance_limit, cost_improvement):
    """
    Assess practical significance of a statistically significant effect
    """
    if abs(effect_size) < tolerance_limit * 0.1:
        return "Statistically significant but practically negligible"
    elif cost_improvement < threshold:
        return "Significant but not cost-effective"
    else:
        return "Both statistically and practically significant"
```

## üîç Power Analysis and Sample Size Planning

### Statistical Power

**Definition**: Probability of detecting a true effect (1 - Œ≤)

**Factors Affecting Power**:

1. **Effect size**: Larger effects easier to detect
2. **Sample size**: More data increases power
3. **Significance level (Œ±)**: Lower Œ± reduces power
4. **Variance**: Lower variance increases power

### Sample Size Calculation

**For One-Way ANOVA**:

```python
import scipy.stats as stats
from statsmodels.stats.power import FTestAnovaPower

def sample_size_anova(effect_size, alpha=0.05, power=0.80, k_groups=3):
    """
    Calculate sample size for one-way ANOVA
    
    Parameters:
    effect_size: Cohen's f (small=0.1, medium=0.25, large=0.4)
    alpha: Type I error rate
    power: Desired statistical power
    k_groups: Number of groups
    """
    power_analysis = FTestAnovaPower()
    n_per_group = power_analysis.solve_power(
        effect_size=effect_size,
        nobs=None,
        alpha=alpha,
        power=power,
        k_groups=k_groups
    )
    return int(np.ceil(n_per_group))

# Example: Medium effect size, 4 tools
n_required = sample_size_anova(effect_size=0.25, k_groups=4)
print(f"Required sample size per group: {n_required}")
```

### Post-Hoc Power Analysis

```python
def post_hoc_power(observed_f, df_num, df_den, alpha=0.05):
    """
    Calculate achieved power from observed F-statistic
    """
    critical_f = stats.f.ppf(1-alpha, df_num, df_den)
    if observed_f <= critical_f:
        return 0.0  # Non-significant result
    
    # Calculate non-centrality parameter
    ncp = observed_f * df_num
    
    # Power = 1 - P(F < critical_f | H1 true)
    power = 1 - stats.ncf.cdf(critical_f, df_num, df_den, ncp)
    return power
```

## üõ†Ô∏è Implementation Best Practices

### Data Preparation Checklist

1. **Exploratory Data Analysis**
   - Check distributions
   - Identify outliers
   - Assess missing data patterns

2. **Design Validation**
   - Verify randomization
   - Check balance
   - Confirm replication

3. **Assumption Checking**
   - Normality tests
   - Homoscedasticity tests
   - Independence verification

### Reporting Standards

**ANOVA Table Format**:

```
Source      | SS      | df | MS      | F      | p-value | Œ∑¬≤
------------|---------|----|---------| -------|---------|-----
Factor A    | 125.43  | 2  | 62.72   | 15.68  | <0.001  | 0.34
Factor B    | 87.21   | 3  | 29.07   | 7.27   | 0.002   | 0.24
A √ó B       | 23.45   | 6  | 3.91    | 0.98   | 0.465   | 0.07
Error       | 95.67   | 24 | 3.99    |        |         |
Total       | 331.76  | 35 |         |        |         |
```

**Interpretation Template**:

1. **Significance**: State statistical conclusions
2. **Effect sizes**: Report practical importance
3. **Post-hoc results**: Identify specific differences
4. **Assumptions**: Note any violations and remedies
5. **Limitations**: Discuss scope and generalizability

## üìö Advanced Topics and Extensions

### Robust ANOVA

**When Assumptions Violated**:

1. **Welch's ANOVA**: Unequal variances
2. **Kruskal-Wallis**: Non-normal data
3. **Bootstrap procedures**: Empirical distributions
4. **Trimmed means**: Robust to outliers

```python
from scipy.stats import kruskal

# Kruskal-Wallis test (non-parametric alternative)
groups = [data[data['group']==g]['response'] for g in data['group'].unique()]
stat, p_value = kruskal(*groups)
print(f"Kruskal-Wallis: H={stat:.4f}, p={p_value:.4f}")
```

### Multivariate ANOVA (MANOVA)

**Multiple Response Variables**:

```python
from statsmodels.multivariate.manova import MANOVA

# MANOVA for multiple responses
manova = MANOVA.from_formula('thickness + uniformity ~ C(tool)', data=data)
print(manova.mv_test())
```

### Bayesian ANOVA

**Advantages**:

- Incorporates prior knowledge
- Provides probability statements
- Handles complex hierarchical structures

```python
import pymc3 as pm

# Bayesian one-way ANOVA
with pm.Model() as bayesian_anova:
    # Priors
    mu = pm.Normal('mu', 0, 10)
    sigma = pm.HalfNormal('sigma', 5)
    alpha = pm.Normal('alpha', 0, 5, shape=n_groups)
    
    # Likelihood
    y_obs = pm.Normal('y_obs', 
                     mu=mu + alpha[group_idx], 
                     sigma=sigma, 
                     observed=observations)
    
    # Sampling
    trace = pm.sample(2000, tune=1000)
```

## üí° Key Takeaways

1. **ANOVA Framework**: Systematic approach to understanding variation sources
2. **Design Principles**: Randomization, replication, and blocking are essential
3. **Assumption Checking**: Always validate model assumptions
4. **Effect Sizes**: Statistical significance ‚â† practical importance
5. **Multiple Comparisons**: Control family-wise error rate
6. **Sample Size Planning**: Power analysis prevents underpowered studies
7. **Model Selection**: Choose appropriate design for research question
8. **Interpretation**: Consider manufacturing context and costs

## üîú Next Steps

In the next module (3.1), we'll apply these statistical foundations to **regression analysis for process engineers**, building predictive models for key performance indicators and process optimization.

## üìö Further Reading

1. **"Design and Analysis of Experiments"** by Montgomery
2. **"Statistical Methods for Engineers"** by Vardeman & Jobe  
3. **"Response Surface Methodology"** by Myers, Montgomery & Anderson-Cook
4. **"Mixed-Effects Models in S and S-PLUS"** by Pinheiro & Bates
5. **"Statistical Quality Control"** by Montgomery
6. **"Semiconductor Manufacturing Technology"** by Quirk & Serda

---

Understanding ANOVA and DOE principles transforms raw manufacturing data into actionable insights for process optimization and quality improvement.
