{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cc7fec",
   "metadata": {},
   "source": [
    "# 2.2 Outlier Detection in Semiconductor Manufacturing\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- **Differentiate** types of outliers in semiconductor manufacturing (process, measurement, contextual, collective)\n",
    "- **Implement** statistical outlier detection methods (Z-score, Modified Z-score, IQR)\n",
    "- **Apply** multivariate techniques (Mahalanobis Distance)\n",
    "- **Leverage** machine learning models (Isolation Forest, Local Outlier Factor)\n",
    "- **Monitor** process stability using time-series control (EWMA)\n",
    "- **Design** contextual (recipe-aware) and physics-guided detection strategies\n",
    "- **Evaluate** detection performance with precision/recall metrics\n",
    "\n",
    "## ðŸ—‚ Dataset: SECOM (UCI ML Repository)\n",
    "We'll use the real-world **SECOM** dataset (1567 wafers Ã— 591 features) capturing inline process measurements with a binary pass/fail quality label (âˆ’1 = pass, 1 = fail). The data contains:\n",
    "- High dimensional, noisy sensor signals\n",
    "- Missing values (NaN)\n",
    "- Imbalanced target (few fails)\n",
    "\n",
    "> If the dataset files (`secom.data`, `secom_labels.data`) are not present locally, the notebook will prompt how to download them or optionally generate a smaller synthetic proxy.\n",
    "\n",
    "## ðŸŽ¯ Workflow Overview\n",
    "1. Load & inspect dataset\n",
    "2. Clean & preprocess (missing value handling, variance filtering, scaling)\n",
    "3. Implement univariate detectors\n",
    "4. Implement multivariate & ML-based detectors\n",
    "5. Time-series style EWMA example (using an engineered aggregate signal)\n",
    "6. Contextual (recipe/surrogate grouping) + physics-inspired validation\n",
    "7. Compare methods vs. injected / inferred anomaly labels\n",
    "8. Provide extension exercises\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting and environment\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_palette(\"viridis\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../../datasets').resolve()\n",
    "SECOM_DATA_FILE = DATA_DIR / 'secom.data'\n",
    "SECOM_LABEL_FILE = DATA_DIR / 'secom_labels.data'\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"Looking for dataset in: {DATA_DIR}\")\n",
    "print(f\"secom.data exists: {SECOM_DATA_FILE.exists()} | secom_labels.data exists: {SECOM_LABEL_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c37f96",
   "metadata": {},
   "source": [
    "## ðŸ­ Loading the SECOM Dataset (or Fallback Synthetic Data)\n",
    "\n",
    "The **SECOM** dataset (UCI ID: 179) consists of 1567 samples and 591 continuous features measuring in-process signals. A separate labels file contains:\n",
    "- Quality label (column 0): âˆ’1 = pass, 1 = fail\n",
    "- Timestamp (column 1): DateTime string\n",
    "\n",
    "If the raw files are not available in `datasets/`, you may:\n",
    "1. Download manually from the UCI repository (https://archive.ics.uci.edu/dataset/179/secom) and place them in `datasets/`\n",
    "2. Use `pip install ucimlrepo` and fetch programmatically\n",
    "3. Proceed with an automatically generated smaller synthetic proxy (this notebook will inform you)\n",
    "\n",
    "We'll implement a loader that prefers local files but falls back gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_secom(local_data_path=SECOM_DATA_FILE, local_label_path=SECOM_LABEL_FILE, synthetic_if_missing=True):\n",
    "    \"\"\"Load SECOM dataset or optionally generate synthetic fallback.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix (after basic cleaning)\n",
    "    y : pd.Series | None\n",
    "        Quality label (if available)\n",
    "    meta : dict\n",
    "        Metadata about loading mode\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "    if local_data_path.exists() and local_label_path.exists():\n",
    "        # Load raw feature matrix (space-separated, NaNs present)\n",
    "        X = pd.read_csv(local_data_path, sep=\" \", header=None, na_values='NaN')\n",
    "        labels = pd.read_csv(local_label_path, sep=\" \", header=None, na_values='NaN')\n",
    "        y = labels.iloc[:, 0].replace({-1:0, 1:1})  # Convert to 0=pass,1=fail for convenience\n",
    "        # Basic cleaning: drop all-NaN columns\n",
    "        all_nan_cols = X.columns[X.isna().all()]\n",
    "        if len(all_nan_cols) > 0:\n",
    "            X = X.drop(columns=all_nan_cols)\n",
    "        meta['mode'] = 'secom_real'\n",
    "        meta['dropped_all_nan_columns'] = len(all_nan_cols)\n",
    "        return X, y, meta\n",
    "    elif synthetic_if_missing:\n",
    "        np.random.seed(42)\n",
    "        n_samples, n_features = 600, 40\n",
    "        base = np.random.normal(0, 1, size=(n_samples, n_features))\n",
    "        # Inject correlation structure\n",
    "        for i in range(1, n_features, 5):\n",
    "            base[:, i] = base[:, 0] * 0.6 + np.random.normal(0, 0.5, size=n_samples)\n",
    "        # Inject sparse anomalies\n",
    "        anomaly_idx = np.random.choice(n_samples, size=25, replace=False)\n",
    "        base[anomaly_idx] += np.random.normal(5, 1, size=(25, n_features))\n",
    "        X = pd.DataFrame(base, columns=[f'f{i}' for i in range(n_features)])\n",
    "        y = pd.Series((np.isin(range(n_samples), anomaly_idx)).astype(int), name='is_fail')\n",
    "        meta['mode'] = 'synthetic'\n",
    "        meta['note'] = 'SECOM files missing; using synthetic proxy.'\n",
    "        return X, y, meta\n",
    "    else:\n",
    "        raise FileNotFoundError(\"SECOM dataset not found and synthetic fallback disabled.\")\n",
    "\n",
    "X_raw, y_raw, load_meta = load_secom()\n",
    "print(f\"ðŸ“¦ Data load mode: {load_meta['mode']}\")\n",
    "print(f\"Shape: {X_raw.shape} | Label available: {y_raw is not None}\")\n",
    "X_raw.head().iloc[:3, :8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e174a",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Preprocessing & Feature Reduction\n",
    "\n",
    "Given high dimensionality and missingness, we apply:\n",
    "1. Drop columns with > 40% missing values\n",
    "2. Impute remaining NaNs (median)\n",
    "3. Remove near-zero variance features\n",
    "4. (Optional) Standard scaling\n",
    "5. Keep a manageable subset (e.g., top variance features) for demonstration\n",
    "\n",
    "We'll retain both the processed matrix and a scaled version for algorithms that need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69683da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X, max_missing_ratio=0.4, variance_threshold=1e-5, top_variance=50):\n",
    "    Xc = X.copy()\n",
    "    # 1. Drop high-missing columns\n",
    "    missing_ratio = Xc.isna().mean()\n",
    "    keep_cols = missing_ratio[missing_ratio <= max_missing_ratio].index\n",
    "    Xc = Xc[keep_cols]\n",
    "    # 2. Median impute\n",
    "    medians = Xc.median()\n",
    "    Xc = Xc.fillna(medians)\n",
    "    # 3. Remove near-zero variance\n",
    "    variances = Xc.var()\n",
    "    keep_var_cols = variances[variances > variance_threshold].index\n",
    "    Xc = Xc[keep_var_cols]\n",
    "    # 4. Select top variance features for demo clarity\n",
    "    top_cols = variances[keep_var_cols].sort_values(ascending=False).head(top_variance).index\n",
    "    Xc = Xc[top_cols]\n",
    "    # 5. Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(Xc), columns=Xc.columns)\n",
    "    meta = {\n",
    "        'dropped_missing_cols': int((missing_ratio > max_missing_ratio).sum()),\n",
    "        'remaining_features': Xc.shape[1]\n",
    "    }\n",
    "    return Xc, X_scaled, meta\n",
    "\n",
    "X_proc, X_scaled, prep_meta = preprocess_features(X_raw)\n",
    "print(f\"ðŸ§ª Preprocessing complete. Features kept: {X_proc.shape[1]} | Dropped (missing): {prep_meta['dropped_missing_cols']}\")\n",
    "X_proc.head().iloc[:3, :8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f2519",
   "metadata": {},
   "source": [
    "## ðŸ“Š 1. Univariate Statistical Outlier Detection\n",
    "\n",
    "We implement three foundational methods:\n",
    "- **Z-Score**: Assumes approximate normality\n",
    "- **Modified Z-Score**: Robust (median + MAD)\n",
    "- **IQR**: Non-parametric range-based\n",
    "\n",
    "We'll apply each to one representative high-variance feature and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_flags(series, threshold=3.0):\n",
    "    z = np.abs(stats.zscore(series, nan_policy='omit'))\n",
    "    return (z > threshold)\n",
    "\n",
    "def modified_zscore_flags(series, threshold=3.5):\n",
    "    med = np.median(series)\n",
    "    mad = np.median(np.abs(series - med))\n",
    "    if mad == 0:\n",
    "        return pd.Series([False]*len(series), index=series.index)\n",
    "    mz = 0.6745 * (series - med) / mad\n",
    "    return (np.abs(mz) > threshold)\n",
    "\n",
    "def iqr_flags(series, k=1.5):\n",
    "    q1, q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - k*iqr, q3 + k*iqr\n",
    "    return (series < lower) | (series > upper)\n",
    "\n",
    "feature_example = X_proc.columns[0]\n",
    "uni_df = pd.DataFrame({\n",
    "    feature_example: X_proc[feature_example],\n",
    "    'zscore': zscore_flags(X_proc[feature_example]),\n",
    "    'modified_z': modified_zscore_flags(X_proc[feature_example]),\n",
    "    'iqr': iqr_flags(X_proc[feature_example])\n",
    "})\n",
    "uni_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e103aa3",
   "metadata": {},
   "source": [
    "### Visualization: Comparing Univariate Methods\n",
    "We'll overlay flagged points for the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14747d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "ax.plot(uni_df[feature_example].values, label=feature_example, alpha=0.6)\n",
    "for method, color in [('zscore','orange'),('modified_z','red'),('iqr','purple')]:\n",
    "    idx = uni_df[uni_df[method]].index\n",
    "    ax.scatter(idx, uni_df.loc[idx, feature_example], label=method, color=color, s=60)\n",
    "ax.set_title(f'Univariate Outlier Flags - {feature_example}')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(uni_df[['zscore','modified_z','iqr']].sum().rename('flag_counts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853f39e",
   "metadata": {},
   "source": [
    "## ðŸ”¬ 2. Multivariate Statistical Detection (Mahalanobis)\n",
    "We compute Mahalanobis distance over a reduced subset of features (top variance) to avoid numerical instability and overfitting in high dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_flags(X, feature_subset=None, threshold_percentile=99):\n",
    "    if feature_subset is None:\n",
    "        feature_subset = X.columns[:10]\n",
    "    Xs = X[feature_subset]\n",
    "    mean = Xs.mean().values\n",
    "    cov = np.cov(Xs.values, rowvar=False)\n",
    "    inv_cov = np.linalg.pinv(cov)\n",
    "    dists = []\n",
    "    for row in Xs.values:\n",
    "        dists.append(mahalanobis(row, mean, inv_cov))\n",
    "    dists = np.array(dists)\n",
    "    th = np.percentile(dists, threshold_percentile)\n",
    "    return pd.Series(dists>th, index=Xs.index), dists, th\n",
    "\n",
    "maha_flags, maha_dists, maha_th = mahalanobis_flags(X_scaled)\n",
    "print(f\"Threshold: {maha_th:.3f} | Flagged: {maha_flags.sum()}\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(maha_dists, label='Mahalanobis Distance')\n",
    "plt.axhline(maha_th, color='red', linestyle='--', label='Threshold')\n",
    "plt.title('Mahalanobis Distances (Top Features)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a5132",
   "metadata": {},
   "source": [
    "## ðŸ¤– 3. Machine Learning Based Detection\n",
    "Implement unsupervised models trained on feature matrix:\n",
    "- Isolation Forest (tree ensemble isolating anomalies)\n",
    "- Local Outlier Factor (density-based, local neighborhood)\n",
    "We use scaled features for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_preds = iso.fit_predict(X_scaled)\n",
    "iso_flags = pd.Series(iso_preds==-1, index=X_scaled.index, name='iso_forest')\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_preds = lof.fit_predict(X_scaled)\n",
    "lof_flags = pd.Series(lof_preds==-1, index=X_scaled.index, name='lof')\n",
    "\n",
    "ml_flags = pd.concat([iso_flags, lof_flags], axis=1)\n",
    "print(ml_flags.sum())\n",
    "\n",
    "# Simple 2D projection for visualization (first two scaled features)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X_scaled.iloc[:,0], X_scaled.iloc[:,1], c=iso_flags.map({True:'red', False:'gray'}), alpha=0.6)\n",
    "plt.title('Isolation Forest Flags (Red = Outlier)')\n",
    "plt.xlabel(X_scaled.columns[0]); plt.ylabel(X_scaled.columns[1]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89629209",
   "metadata": {},
   "source": [
    "## â±ï¸ 4. EWMA Monitoring (Synthetic Time Axis)\n",
    "The SECOM dataset lacks raw temporal order context; we simulate an order and create an aggregate signal (mean of selected features) to illustrate EWMA small-shift detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4342a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_series(x, lambda_param=0.2):\n",
    "    ew = []\n",
    "    prev = x[0]\n",
    "    for val in x:\n",
    "        prev = lambda_param * val + (1-lambda_param) * prev\n",
    "        ew.append(prev)\n",
    "    return np.array(ew)\n",
    "\n",
    "agg_signal = X_scaled.iloc[:, :5].mean(axis=1).values\n",
    "ew = ewma_series(agg_signal)\n",
    "mu, sigma = np.mean(agg_signal), np.std(agg_signal)\n",
    "L=3\n",
    "n=len(agg_signal)\n",
    "t = np.arange(1,n+1)\n",
    "var_factor = (lambda_param:=0.2)/(2-lambda_param)*(1-(1-lambda_param)**(2*t))\n",
    "ucl = mu + L*sigma*np.sqrt(var_factor)\n",
    "lcl = mu - L*sigma*np.sqrt(var_factor)\n",
    "flags_ewma = (ew>ucl)|(ew<lcl)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(agg_signal, label='Aggregate Signal', alpha=0.4)\n",
    "plt.plot(ew, label='EWMA', color='orange')\n",
    "plt.plot(ucl, '--', color='red', label='UCL/LCL')\n",
    "plt.plot(lcl, '--', color='red')\n",
    "plt.scatter(np.where(flags_ewma)[0], ew[flags_ewma], color='red', s=50, label='EWMA Flag')\n",
    "plt.title('EWMA Monitoring Example')\n",
    "plt.legend();\n",
    "print(f\"EWMA flags: {flags_ewma.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212c510",
   "metadata": {},
   "source": [
    "## ðŸ§  5. Contextual & Ensemble Strategy\n",
    "Without true recipe metadata, we simulate grouping via k-means on top features to create pseudo-contexts, then perform within-group Z-score detection. Finally, we build an ensemble consensus score across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(X_scaled.iloc[:, :10])\n",
    "X_proc['cluster'] = cluster_labels\n",
    "\n",
    "context_flags = pd.Series(False, index=X_proc.index)\n",
    "feat_for_context = X_proc.columns[0]\n",
    "for c in range(k):\n",
    "    mask = X_proc['cluster']==c\n",
    "    context_series = X_proc.loc[mask, feat_for_context]\n",
    "    context_flags.loc[mask] = zscore_flags(context_series, threshold=2.5)\n",
    "\n",
    "# Ensemble: combine methods\n",
    "methods_df = pd.DataFrame({\n",
    "    'uni_z': uni_df['zscore'],\n",
    "    'uni_modz': uni_df['modified_z'],\n",
    "    'uni_iqr': uni_df['iqr'],\n",
    "    'maha': maha_flags,\n",
    "    'iso': iso_flags,\n",
    "    'lof': lof_flags,\n",
    "    'ewma': pd.Series(flags_ewma, index=X_proc.index),\n",
    "    'context': context_flags\n",
    "}).fillna(False)\n",
    "\n",
    "methods_df['consensus_score'] = methods_df.mean(axis=1)\n",
    "# Flag consensus if >= 0.4 (arbitrary demo threshold)\n",
    "methods_df['consensus_flag'] = methods_df['consensus_score'] >= 0.4\n",
    "\n",
    "print(methods_df['consensus_flag'].value_counts())\n",
    "methods_df.head()\n",
    "\n",
    "if y_raw is not None:\n",
    "    # Align labels to processed frame length (in synthetic fallback lengths match)\n",
    "    y = y_raw.loc[methods_df.index] if len(y_raw)==len(methods_df) else y_raw.iloc[:len(methods_df)]\n",
    "    def safe_metric(y_true, y_pred, name):\n",
    "        if y_true.sum()==0 and y_pred.sum()==0:\n",
    "            return 0.0\n",
    "        return name(y_true, y_pred, zero_division=0)\n",
    "    prec = safe_metric(y, methods_df['consensus_flag'], precision_score)\n",
    "    rec = safe_metric(y, methods_df['consensus_flag'], recall_score)\n",
    "    f1 = safe_metric(y, methods_df['consensus_flag'], f1_score)\n",
    "    print(f\"Consensus Performance -> Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1bdb7",
   "metadata": {},
   "source": [
    "## âœ… Summary & Next Steps\n",
    "We implemented a layered outlier detection workflow:\n",
    "- Robust univariate screening (Z, Modified Z, IQR)\n",
    "- Multivariate correlation-aware distance (Mahalanobis)\n",
    "- Model-based isolation (Isolation Forest, LOF)\n",
    "- Temporal shift detection (EWMA example)\n",
    "- Contextual grouping + ensemble consensus\n",
    "\n",
    "### Key Principles\n",
    "1. Combine complementary methods to reduce false positives\n",
    "2. Use robust & context-aware approaches for manufacturing variability\n",
    "3. Evaluate with precision/recall when labeled data exists\n",
    "4. Start simple (univariate) â†’ escalate to multivariate/ML â†’ ensemble\n",
    "\n",
    "### ðŸš€ Exercises\n",
    "1. Adjust contamination in Isolation Forest & LOF (0.01â€“0.15) and observe trade-offs.\n",
    "2. Replace percentile threshold in Mahalanobis with chi-square cutoff (df = features).\n",
    "3. Modify consensus threshold (0.3â€“0.6). How does F1 change?\n",
    "4. Implement One-Class SVM and add to the ensemble.\n",
    "5. Engineer a physics-based rule: flag rows where first two features diverge beyond 3Ã— their historical correlation residual.\n",
    "6. Add a rolling window variant of Modified Z to simulate streaming detection.\n",
    "\n",
    "### ðŸ”œ Coming Up (2.3)\n",
    "We will extend these foundations into **advanced statistical analysis** (ANOVA / multivariate process exploration) to attribute variation sources.\n",
    "\n",
    "---\n",
    "If running with synthetic fallback, consider downloading the real SECOM dataset for richer structure.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
