{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595b9212",
   "metadata": {},
   "source": [
    "# 2.2 Interactive Outlier Detection for Semiconductor Manufacturing\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this section, you will:\n",
    "- **Master** multiple outlier detection algorithms (statistical and ML-based)\n",
    "- **Implement** real-time anomaly detection for semiconductor processes\n",
    "- **Build** consensus outlier detection frameworks\n",
    "- **Create** interactive visualizations for outlier analysis\n",
    "- **Apply** domain-specific validation rules for manufacturing\n",
    "- **Design** production-ready outlier monitoring systems\n",
    "\n",
    "## üéØ What You'll Build\n",
    "\n",
    "In this interactive notebook, you'll develop a comprehensive **Outlier Detection Pipeline** featuring:\n",
    "\n",
    "1. **üîç Statistical Methods**: Z-Score, IQR, Modified Z-Score with interactive controls\n",
    "2. **ü§ñ ML-Based Detection**: Isolation Forest, One-Class SVM, Local Outlier Factor\n",
    "3. **üéØ Consensus Framework**: Combine multiple methods for robust detection\n",
    "4. **üìä Interactive Visualizations**: Real-time plotting with Plotly widgets\n",
    "5. **‚ö° Real-Time Simulation**: Streaming data outlier detection demo\n",
    "6. **üè≠ Domain Rules**: Semiconductor-specific physics-based validation\n",
    "7. **üìã Comprehensive Reports**: Automated outlier analysis summaries\n",
    "\n",
    "Let's dive into the fascinating world of anomaly detection! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for outlier detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import time\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Interactive widgets and visualization\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas: {pd.__version__} | NumPy: {np.__version__}\")\n",
    "print(f\"üîß Scikit-learn ready | üìà Plotly ready | üéõÔ∏è Widgets ready\")\n",
    "print(\"üöÄ Ready to detect outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd881c77",
   "metadata": {},
   "source": [
    "## üìä Data Preparation: Creating Realistic Semiconductor Data\n",
    "\n",
    "We'll create a **synthetic semiconductor manufacturing dataset** that mimics real-world conditions:\n",
    "\n",
    "- **Multiple process parameters** (temperature, pressure, flow rates, etc.)\n",
    "- **Realistic correlations** between related sensors\n",
    "- **Planted outliers** of different types (point, contextual, collective)\n",
    "- **Missing values** and **noise** typical in manufacturing\n",
    "- **Recipe-based** process variations\n",
    "\n",
    "This will help you understand outlier detection in a controlled environment before applying to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semiconductor_dataset(n_samples=2000, n_features=20, outlier_rate=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a realistic semiconductor manufacturing dataset with planted outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of process runs to simulate\n",
    "    n_features : int  \n",
    "        Number of process parameters/sensors\n",
    "    outlier_rate : float\n",
    "        Fraction of samples that should be outliers\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Semiconductor process data with outliers\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Define realistic semiconductor process parameters\n",
    "    param_names = [\n",
    "        'chamber_pressure', 'plasma_temperature', 'rf_power', 'gas_flow_ar',\n",
    "        'gas_flow_o2', 'etch_rate', 'substrate_temp', 'plasma_density',\n",
    "        'ion_energy', 'residence_time', 'chuck_temp', 'wall_temp',\n",
    "        'impedance_match', 'dc_bias', 'optical_emission', 'mass_spec_signal',\n",
    "        'pump_speed', 'throttle_valve', 'backside_pressure', 'leak_rate'\n",
    "    ]\n",
    "    \n",
    "    # Ensure we have enough parameter names\n",
    "    if n_features > len(param_names):\n",
    "        param_names.extend([f'sensor_{i:03d}' for i in range(len(param_names), n_features)])\n",
    "    \n",
    "    param_names = param_names[:n_features]\n",
    "    \n",
    "    # Create base data with realistic correlations\n",
    "    data = np.random.normal(0, 1, (n_samples, n_features))\n",
    "    \n",
    "    # Add realistic correlations between related parameters\n",
    "    correlations = [\n",
    "        ([0, 1, 2], 0.7),      # Pressure, temperature, RF power\n",
    "        ([3, 4], 0.6),         # Gas flows\n",
    "        ([1, 6, 11], 0.5),     # Various temperatures\n",
    "        ([7, 8, 14], 0.4),     # Plasma-related parameters\n",
    "    ]\n",
    "    \n",
    "    for group, corr_strength in correlations:\n",
    "        if max(group) < n_features:\n",
    "            base_signal = np.random.normal(0, 1, n_samples)\n",
    "            for idx in group:\n",
    "                noise = np.random.normal(0, np.sqrt(1 - corr_strength**2), n_samples)\n",
    "                data[:, idx] = corr_strength * base_signal + noise\n",
    "    \n",
    "    # Add process recipes (different operating conditions)\n",
    "    recipes = ['Recipe_A', 'Recipe_B', 'Recipe_C', 'Recipe_D']\n",
    "    recipe_column = np.random.choice(recipes, n_samples)\n",
    "    \n",
    "    # Apply recipe-specific offsets\n",
    "    recipe_offsets = {\n",
    "        'Recipe_A': [0, 0, 0],      # Baseline\n",
    "        'Recipe_B': [1, -0.5, 0.8], # Higher pressure, lower temp\n",
    "        'Recipe_C': [-1, 1, -0.5],  # Lower pressure, higher temp\n",
    "        'Recipe_D': [0.5, 0.5, 1.2] # Mixed conditions\n",
    "    }\n",
    "    \n",
    "    for recipe, offsets in recipe_offsets.items():\n",
    "        mask = recipe_column == recipe\n",
    "        for i, offset in enumerate(offsets):\n",
    "            if i < n_features:\n",
    "                data[mask, i] += offset\n",
    "    \n",
    "    # Plant different types of outliers\n",
    "    n_outliers = int(outlier_rate * n_samples)\n",
    "    outlier_indices = np.random.choice(n_samples, n_outliers, replace=False)\n",
    "    true_outliers = np.zeros(n_samples, dtype=bool)\n",
    "    true_outliers[outlier_indices] = True\n",
    "    \n",
    "    outlier_types = []\n",
    "    \n",
    "    for idx in outlier_indices:\n",
    "        outlier_type = np.random.choice(['point', 'contextual', 'collective'], p=[0.6, 0.3, 0.1])\n",
    "        outlier_types.append(outlier_type)\n",
    "        \n",
    "        if outlier_type == 'point':\n",
    "            # Point outliers: extreme values in few features\n",
    "            n_features_affected = np.random.randint(1, min(4, n_features))\n",
    "            features_affected = np.random.choice(n_features, n_features_affected, replace=False)\n",
    "            \n",
    "            for feat in features_affected:\n",
    "                magnitude = np.random.uniform(4, 8)  # 4-8 standard deviations\n",
    "                sign = np.random.choice([-1, 1])\n",
    "                data[idx, feat] = sign * magnitude\n",
    "                \n",
    "        elif outlier_type == 'contextual':\n",
    "            # Contextual outliers: values normal individually but abnormal for recipe\n",
    "            current_recipe = recipe_column[idx]\n",
    "            wrong_recipe = np.random.choice([r for r in recipes if r != current_recipe])\n",
    "            wrong_offsets = recipe_offsets[wrong_recipe]\n",
    "            \n",
    "            for i, offset in enumerate(wrong_offsets):\n",
    "                if i < n_features:\n",
    "                    # Apply wrong recipe parameters\n",
    "                    data[idx, i] += offset * 2  # Amplify the difference\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=param_names)\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df['recipe'] = recipe_column\n",
    "    df['timestamp'] = pd.date_range(start='2024-01-01', periods=n_samples, freq='5min')\n",
    "    df['true_outlier'] = true_outliers\n",
    "    \n",
    "    # Add some missing values (typical in manufacturing)\n",
    "    missing_rate = 0.02\n",
    "    for col in param_names:\n",
    "        n_missing = int(np.random.poisson(missing_rate * n_samples))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = np.random.choice(n_samples, min(n_missing, n_samples), replace=False)\n",
    "            df.loc[missing_indices, col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create our semiconductor dataset\n",
    "print(\"üè≠ Generating synthetic semiconductor manufacturing data...\")\n",
    "semiconductor_data = create_semiconductor_dataset(n_samples=2000, n_features=15, outlier_rate=0.08)\n",
    "\n",
    "print(f\"‚úÖ Dataset created successfully!\")\n",
    "print(f\"üìè Shape: {semiconductor_data.shape}\")\n",
    "print(f\"üéØ True outliers: {semiconductor_data['true_outlier'].sum()} ({semiconductor_data['true_outlier'].mean():.1%})\")\n",
    "print(f\"üìù Recipes: {semiconductor_data['recipe'].value_counts().to_dict()}\")\n",
    "print(f\"‚ùì Missing values: {semiconductor_data.isnull().sum().sum()} total\")\n",
    "\n",
    "# Display first few rows\n",
    "display(semiconductor_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37a0ff",
   "metadata": {},
   "source": [
    "## üëÄ Data Preview: Understanding Our Manufacturing Dataset\n",
    "\n",
    "Let's explore our dataset interactively to understand the process parameters and identify obvious patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33548927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive data exploration widget\n",
    "def create_data_explorer():\n",
    "    \"\"\"Create interactive data exploration dashboard.\"\"\"\n",
    "    \n",
    "    # Get feature columns (exclude metadata)\n",
    "    feature_cols = [col for col in semiconductor_data.columns \n",
    "                   if col not in ['recipe', 'timestamp', 'true_outlier']]\n",
    "    \n",
    "    # Create widgets\n",
    "    feature_selector = widgets.Dropdown(\n",
    "        options=feature_cols,\n",
    "        value=feature_cols[0],\n",
    "        description='Parameter:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    plot_type = widgets.RadioButtons(\n",
    "        options=['Histogram', 'Box Plot', 'Time Series', 'Scatter vs True Outliers'],\n",
    "        value='Histogram',\n",
    "        description='Plot Type:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    recipe_filter = widgets.SelectMultiple(\n",
    "        options=['All'] + list(semiconductor_data['recipe'].unique()),\n",
    "        value=['All'],\n",
    "        description='Recipes:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    def update_plot(feature, plot_style, recipes):\n",
    "        \"\"\"Update the plot based on widget selections.\"\"\"\n",
    "        \n",
    "        # Filter data by recipe\n",
    "        if 'All' in recipes:\n",
    "            plot_data = semiconductor_data.copy()\n",
    "        else:\n",
    "            plot_data = semiconductor_data[semiconductor_data['recipe'].isin(recipes)].copy()\n",
    "        \n",
    "        if len(plot_data) == 0:\n",
    "            print(\"‚ùå No data matches the selected filters\")\n",
    "            return\n",
    "        \n",
    "        # Create the plot\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "        \n",
    "        if plot_style == 'Histogram':\n",
    "            # Separate normal and outlier data\n",
    "            normal_data = plot_data[~plot_data['true_outlier']][feature].dropna()\n",
    "            outlier_data = plot_data[plot_data['true_outlier']][feature].dropna()\n",
    "            \n",
    "            fig.add_trace(go.Histogram(\n",
    "                x=normal_data,\n",
    "                name='Normal',\n",
    "                opacity=0.7,\n",
    "                nbinsx=30,\n",
    "                marker_color='blue'\n",
    "            ))\n",
    "            \n",
    "            if len(outlier_data) > 0:\n",
    "                fig.add_trace(go.Histogram(\n",
    "                    x=outlier_data,\n",
    "                    name='True Outliers',\n",
    "                    opacity=0.7,\n",
    "                    nbinsx=30,\n",
    "                    marker_color='red'\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Distribution of {feature}\",\n",
    "                xaxis_title=feature,\n",
    "                yaxis_title=\"Frequency\",\n",
    "                barmode='overlay'\n",
    "            )\n",
    "            \n",
    "        elif plot_style == 'Box Plot':\n",
    "            # Box plot by recipe\n",
    "            for recipe in plot_data['recipe'].unique():\n",
    "                recipe_data = plot_data[plot_data['recipe'] == recipe]\n",
    "                fig.add_trace(go.Box(\n",
    "                    y=recipe_data[feature],\n",
    "                    name=recipe,\n",
    "                    boxpoints='outliers'\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Box Plot of {feature} by Recipe\",\n",
    "                yaxis_title=feature\n",
    "            )\n",
    "            \n",
    "        elif plot_style == 'Time Series':\n",
    "            # Time series plot\n",
    "            normal_data = plot_data[~plot_data['true_outlier']]\n",
    "            outlier_data = plot_data[plot_data['true_outlier']]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=normal_data['timestamp'],\n",
    "                y=normal_data[feature],\n",
    "                mode='markers',\n",
    "                name='Normal',\n",
    "                marker=dict(color='blue', size=4, opacity=0.6)\n",
    "            ))\n",
    "            \n",
    "            if len(outlier_data) > 0:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=outlier_data['timestamp'],\n",
    "                    y=outlier_data[feature],\n",
    "                    mode='markers',\n",
    "                    name='True Outliers',\n",
    "                    marker=dict(color='red', size=8, symbol='x')\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Time Series of {feature}\",\n",
    "                xaxis_title=\"Time\",\n",
    "                yaxis_title=feature\n",
    "            )\n",
    "            \n",
    "        elif plot_style == 'Scatter vs True Outliers':\n",
    "            # Scatter plot colored by outlier status\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=plot_data.index,\n",
    "                y=plot_data[feature],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=plot_data['true_outlier'],\n",
    "                    colorscale=['blue', 'red'],\n",
    "                    size=6,\n",
    "                    colorbar=dict(title=\"Is Outlier\")\n",
    "                ),\n",
    "                text=plot_data['recipe'],\n",
    "                hovertemplate=f\"{feature}: %{{y:.3f}}<br>Recipe: %{{text}}<br>Outlier: %{{marker.color}}<extra></extra>\"\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Scatter Plot of {feature} (Colored by Outlier Status)\",\n",
    "                xaxis_title=\"Sample Index\",\n",
    "                yaxis_title=feature\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=500,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Show summary statistics\n",
    "        print(f\"\\nüìä Summary Statistics for {feature}:\")\n",
    "        print(\"-\" * 50)\n",
    "        summary = plot_data[feature].describe()\n",
    "        for stat, value in summary.items():\n",
    "            print(f\"{stat:>10}: {value:>10.3f}\")\n",
    "        \n",
    "        print(f\"\\nüîç Outlier Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        normal_mean = plot_data[~plot_data['true_outlier']][feature].mean()\n",
    "        outlier_mean = plot_data[plot_data['true_outlier']][feature].mean()\n",
    "        print(f\"Normal mean:   {normal_mean:.3f}\")\n",
    "        print(f\"Outlier mean:  {outlier_mean:.3f}\")\n",
    "        print(f\"Difference:    {abs(outlier_mean - normal_mean):.3f}\")\n",
    "    \n",
    "    # Create interactive widget\n",
    "    widget = widgets.interactive(\n",
    "        update_plot,\n",
    "        feature=feature_selector,\n",
    "        plot_style=plot_type,\n",
    "        recipes=recipe_filter\n",
    "    )\n",
    "    \n",
    "    return widget\n",
    "\n",
    "# Display the interactive explorer\n",
    "print(\"üéõÔ∏è Interactive Data Explorer\")\n",
    "print(\"Use the controls below to explore different process parameters and visualizations:\")\n",
    "data_explorer = create_data_explorer()\n",
    "display(data_explorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762f2d7",
   "metadata": {},
   "source": [
    "## üìà Statistical Outlier Detection Methods\n",
    "\n",
    "Now let's implement and compare different statistical methods for outlier detection. These methods form the foundation of anomaly detection:\n",
    "\n",
    "1. **Z-Score**: Identifies points more than `k` standard deviations from the mean\n",
    "2. **Modified Z-Score**: Robust version using median and MAD\n",
    "3. **IQR Method**: Based on interquartile range\n",
    "4. **Mahalanobis Distance**: Considers correlations between features\n",
    "\n",
    "Each method has different strengths and is suitable for different types of data and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201362f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveStatisticalDetectors:\n",
    "    \"\"\"Interactive statistical outlier detection methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.feature_cols = [col for col in data.columns \n",
    "                           if col not in ['recipe', 'timestamp', 'true_outlier']]\n",
    "        self.results = {}\n",
    "    \n",
    "    def z_score_detection(self, threshold=3.0, features=None):\n",
    "        \"\"\"Z-Score based outlier detection.\"\"\"\n",
    "        if features is None:\n",
    "            features = self.feature_cols\n",
    "        \n",
    "        feature_data = self.data[features].copy()\n",
    "        \n",
    "        # Calculate Z-scores\n",
    "        z_scores = np.abs((feature_data - feature_data.mean()) / feature_data.std())\n",
    "        \n",
    "        # Identify outliers (any feature exceeds threshold)\n",
    "        outliers = (z_scores > threshold).any(axis=1)\n",
    "        \n",
    "        # Anomaly score is the maximum Z-score\n",
    "        scores = z_scores.max(axis=1)\n",
    "        \n",
    "        self.results['z_score'] = {\n",
    "            'outliers': outliers,\n",
    "            'scores': scores,\n",
    "            'threshold': threshold,\n",
    "            'n_outliers': outliers.sum(),\n",
    "            'percentage': outliers.mean() * 100\n",
    "        }\n",
    "        \n",
    "        return outliers, scores\n",
    "    \n",
    "    def modified_z_score_detection(self, threshold=3.5, features=None):\n",
    "        \"\"\"Modified Z-Score based outlier detection (robust).\"\"\"\n",
    "        if features is None:\n",
    "            features = self.feature_cols\n",
    "        \n",
    "        feature_data = self.data[features].copy()\n",
    "        \n",
    "        # Calculate Modified Z-scores using median and MAD\n",
    "        median = feature_data.median()\n",
    "        mad = (feature_data - median).abs().median()\n",
    "        modified_z_scores = 0.6745 * np.abs((feature_data - median) / mad)\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = (modified_z_scores > threshold).any(axis=1)\n",
    "        scores = modified_z_scores.max(axis=1)\n",
    "        \n",
    "        self.results['modified_z_score'] = {\n",
    "            'outliers': outliers,\n",
    "            'scores': scores,\n",
    "            'threshold': threshold,\n",
    "            'n_outliers': outliers.sum(),\n",
    "            'percentage': outliers.mean() * 100\n",
    "        }\n",
    "        \n",
    "        return outliers, scores\n",
    "    \n",
    "    def iqr_detection(self, k=1.5, features=None):\n",
    "        \"\"\"IQR based outlier detection.\"\"\"\n",
    "        if features is None:\n",
    "            features = self.feature_cols\n",
    "        \n",
    "        feature_data = self.data[features].copy()\n",
    "        \n",
    "        q1 = feature_data.quantile(0.25)\n",
    "        q3 = feature_data.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        lower_bound = q1 - k * iqr\n",
    "        upper_bound = q3 + k * iqr\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers_mask = (feature_data < lower_bound) | (feature_data > upper_bound)\n",
    "        outliers = outliers_mask.any(axis=1)\n",
    "        \n",
    "        # Calculate distance from bounds as score\n",
    "        dist_lower = np.maximum(0, lower_bound - feature_data)\n",
    "        dist_upper = np.maximum(0, feature_data - upper_bound)\n",
    "        scores = np.maximum(dist_lower, dist_upper).max(axis=1)\n",
    "        \n",
    "        self.results['iqr'] = {\n",
    "            'outliers': outliers,\n",
    "            'scores': scores,\n",
    "            'k': k,\n",
    "            'n_outliers': outliers.sum(),\n",
    "            'percentage': outliers.mean() * 100\n",
    "        }\n",
    "        \n",
    "        return outliers, scores\n",
    "    \n",
    "    def mahalanobis_detection(self, threshold_percentile=95, features=None):\n",
    "        \"\"\"Mahalanobis distance based outlier detection.\"\"\"\n",
    "        if features is None:\n",
    "            features = self.feature_cols\n",
    "        \n",
    "        feature_data = self.data[features].dropna().copy()\n",
    "        \n",
    "        if len(feature_data) < len(features):\n",
    "            print(\"‚ö†Ô∏è Insufficient data for Mahalanobis distance calculation\")\n",
    "            return np.zeros(len(self.data), dtype=bool), np.zeros(len(self.data))\n",
    "        \n",
    "        # Calculate mean and covariance\n",
    "        mean = feature_data.mean().values\n",
    "        cov_matrix = feature_data.cov().values\n",
    "        \n",
    "        # Use pseudo-inverse for numerical stability\n",
    "        inv_cov = np.linalg.pinv(cov_matrix)\n",
    "        \n",
    "        # Calculate Mahalanobis distances\n",
    "        distances = []\n",
    "        for _, row in self.data[features].iterrows():\n",
    "            if row.isnull().any():\n",
    "                distances.append(np.nan)\n",
    "            else:\n",
    "                dist = mahalanobis(row.values, mean, inv_cov)\n",
    "                distances.append(dist)\n",
    "        \n",
    "        distances = np.array(distances)\n",
    "        \n",
    "        # Set threshold based on percentile\n",
    "        threshold = np.nanpercentile(distances, threshold_percentile)\n",
    "        outliers = distances > threshold\n",
    "        \n",
    "        # Handle NaN values\n",
    "        outliers = np.nan_to_num(outliers, nan=False)\n",
    "        distances = np.nan_to_num(distances, nan=0)\n",
    "        \n",
    "        self.results['mahalanobis'] = {\n",
    "            'outliers': outliers,\n",
    "            'scores': distances,\n",
    "            'threshold': threshold,\n",
    "            'threshold_percentile': threshold_percentile,\n",
    "            'n_outliers': outliers.sum(),\n",
    "            'percentage': outliers.mean() * 100\n",
    "        }\n",
    "        \n",
    "        return outliers, distances\n",
    "\n",
    "# Initialize the detector\n",
    "stat_detector = InteractiveStatisticalDetectors(semiconductor_data)\n",
    "\n",
    "print(\"‚úÖ Statistical outlier detectors initialized!\")\n",
    "print(f\"üìä Ready to analyze {len(stat_detector.feature_cols)} process parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistical_detector_widget():\n",
    "    \"\"\"Create interactive widget for statistical outlier detection.\"\"\"\n",
    "    \n",
    "    # Method selection\n",
    "    method_selector = widgets.Dropdown(\n",
    "        options=['Z-Score', 'Modified Z-Score', 'IQR', 'Mahalanobis'],\n",
    "        value='Z-Score',\n",
    "        description='Method:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Parameter controls\n",
    "    z_threshold = widgets.FloatSlider(\n",
    "        value=3.0,\n",
    "        min=1.0,\n",
    "        max=5.0,\n",
    "        step=0.1,\n",
    "        description='Z Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    mod_z_threshold = widgets.FloatSlider(\n",
    "        value=3.5,\n",
    "        min=1.0,\n",
    "        max=5.0,\n",
    "        step=0.1,\n",
    "        description='Mod-Z Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    iqr_k = widgets.FloatSlider(\n",
    "        value=1.5,\n",
    "        min=0.5,\n",
    "        max=3.0,\n",
    "        step=0.1,\n",
    "        description='IQR K:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    mahal_percentile = widgets.IntSlider(\n",
    "        value=95,\n",
    "        min=85,\n",
    "        max=99,\n",
    "        step=1,\n",
    "        description='Mahal %ile:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Feature selection\n",
    "    feature_selector = widgets.SelectMultiple(\n",
    "        options=stat_detector.feature_cols,\n",
    "        value=stat_detector.feature_cols[:5],  # Select first 5 by default\n",
    "        description='Features:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'height': '120px'}\n",
    "    )\n",
    "    \n",
    "    # Run button\n",
    "    run_button = widgets.Button(\n",
    "        description='üîç Detect Outliers',\n",
    "        button_style='primary',\n",
    "        icon='search'\n",
    "    )\n",
    "    \n",
    "    # Output area\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def run_detection(button):\n",
    "        \"\"\"Run the selected detection method.\"\"\"\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            method = method_selector.value\n",
    "            features = list(feature_selector.value)\n",
    "            \n",
    "            if not features:\n",
    "                print(\"‚ùå Please select at least one feature\")\n",
    "                return\n",
    "            \n",
    "            print(f\"üîç Running {method} detection on {len(features)} features...\")\n",
    "            \n",
    "            # Run the appropriate detection method\n",
    "            if method == 'Z-Score':\n",
    "                outliers, scores = stat_detector.z_score_detection(\n",
    "                    threshold=z_threshold.value, features=features\n",
    "                )\n",
    "                result_key = 'z_score'\n",
    "                \n",
    "            elif method == 'Modified Z-Score':\n",
    "                outliers, scores = stat_detector.modified_z_score_detection(\n",
    "                    threshold=mod_z_threshold.value, features=features\n",
    "                )\n",
    "                result_key = 'modified_z_score'\n",
    "                \n",
    "            elif method == 'IQR':\n",
    "                outliers, scores = stat_detector.iqr_detection(\n",
    "                    k=iqr_k.value, features=features\n",
    "                )\n",
    "                result_key = 'iqr'\n",
    "                \n",
    "            elif method == 'Mahalanobis':\n",
    "                outliers, scores = stat_detector.mahalanobis_detection(\n",
    "                    threshold_percentile=mahal_percentile.value, features=features\n",
    "                )\n",
    "                result_key = 'mahalanobis'\n",
    "            \n",
    "            # Display results\n",
    "            result = stat_detector.results[result_key]\n",
    "            \n",
    "            print(f\"\\nüìä {method} Results:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Outliers detected: {result['n_outliers']} ({result['percentage']:.1f}%)\")\n",
    "            \n",
    "            # True outlier comparison\n",
    "            true_outliers = semiconductor_data['true_outlier'].values\n",
    "            tp = np.sum(outliers & true_outliers)  # True positives\n",
    "            fp = np.sum(outliers & ~true_outliers)  # False positives\n",
    "            fn = np.sum(~outliers & true_outliers)  # False negatives\n",
    "            tn = np.sum(~outliers & ~true_outliers)  # True negatives\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            print(f\"\\nüéØ Performance vs True Outliers:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"True Positives:  {tp:>4d}\")\n",
    "            print(f\"False Positives: {fp:>4d}\")\n",
    "            print(f\"False Negatives: {fn:>4d}\")\n",
    "            print(f\"True Negatives:  {tn:>4d}\")\n",
    "            print(f\"Precision:       {precision:>7.3f}\")\n",
    "            print(f\"Recall:          {recall:>7.3f}\")\n",
    "            print(f\"F1-Score:        {f1:>7.3f}\")\n",
    "            \n",
    "            # Create visualization\n",
    "            create_outlier_visualization(outliers, scores, method, features)\n",
    "    \n",
    "    def create_outlier_visualization(outliers, scores, method, features):\n",
    "        \"\"\"Create visualization of outlier detection results.\"\"\"\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                f'{method} Outlier Detection Results',\n",
    "                'Anomaly Score Distribution',\n",
    "                'Outliers in Time Series',\n",
    "                'Feature Comparison'\n",
    "            ],\n",
    "            specs=[[{\"colspan\": 2}, None],\n",
    "                   [{}, {}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Scatter plot of detection results\n",
    "        detected_outliers = semiconductor_data[outliers]\n",
    "        normal_points = semiconductor_data[~outliers]\n",
    "        true_outliers = semiconductor_data[semiconductor_data['true_outlier']]\n",
    "        \n",
    "        # Normal points\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=normal_points.index,\n",
    "                y=scores[~outliers],\n",
    "                mode='markers',\n",
    "                name='Normal',\n",
    "                marker=dict(color='blue', size=4, opacity=0.6),\n",
    "                hovertemplate=\"Index: %{x}<br>Score: %{y:.3f}<extra></extra>\"\n",
    "            ), row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Detected outliers\n",
    "        if len(detected_outliers) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=detected_outliers.index,\n",
    "                    y=scores[outliers],\n",
    "                    mode='markers',\n",
    "                    name='Detected Outliers',\n",
    "                    marker=dict(color='red', size=8, symbol='diamond'),\n",
    "                    hovertemplate=\"Index: %{x}<br>Score: %{y:.3f}<extra></extra>\"\n",
    "                ), row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # True outliers overlay\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=true_outliers.index,\n",
    "                y=scores[true_outliers.index],\n",
    "                mode='markers',\n",
    "                name='True Outliers',\n",
    "                marker=dict(color='orange', size=10, symbol='x', line=dict(width=2)),\n",
    "                hovertemplate=\"Index: %{x}<br>Score: %{y:.3f}<br>True Outlier<extra></extra>\"\n",
    "            ), row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Score distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=scores,\n",
    "                name='Score Distribution',\n",
    "                nbinsx=30,\n",
    "                opacity=0.7,\n",
    "                marker_color='lightblue'\n",
    "            ), row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add threshold line if applicable\n",
    "        result_key = method.lower().replace('-', '_').replace(' ', '_')\n",
    "        if result_key in stat_detector.results:\n",
    "            threshold_val = stat_detector.results[result_key].get('threshold')\n",
    "            if threshold_val:\n",
    "                fig.add_vline(\n",
    "                    x=threshold_val,\n",
    "                    line_dash=\"dash\",\n",
    "                    line_color=\"red\",\n",
    "                    annotation_text=f\"Threshold: {threshold_val:.2f}\",\n",
    "                    row=2, col=1\n",
    "                )\n",
    "        \n",
    "        # 3. Time series view\n",
    "        if len(features) > 0:\n",
    "            feature = features[0]  # Use first selected feature\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=semiconductor_data['timestamp'],\n",
    "                    y=semiconductor_data[feature],\n",
    "                    mode='markers',\n",
    "                    name=f'{feature} (Normal)',\n",
    "                    marker=dict(color='blue', size=3, opacity=0.5),\n",
    "                    hovertemplate=f\"{feature}: %{{y:.3f}}<br>%{{x}}<extra></extra>\"\n",
    "                ), row=2, col=2\n",
    "            )\n",
    "            \n",
    "            if len(detected_outliers) > 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=detected_outliers['timestamp'],\n",
    "                        y=detected_outliers[feature],\n",
    "                        mode='markers',\n",
    "                        name=f'{feature} (Outliers)',\n",
    "                        marker=dict(color='red', size=8, symbol='diamond'),\n",
    "                        hovertemplate=f\"{feature}: %{{y:.3f}}<br>%{{x}}<extra></extra>\"\n",
    "                    ), row=2, col=2\n",
    "                )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title=f\"{method} Outlier Detection Analysis\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Sample Index\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Anomaly Score\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Anomaly Score\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Time\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"Feature Value\", row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    # Connect button to function\n",
    "    run_button.on_click(run_detection)\n",
    "    \n",
    "    # Create parameter control panel that shows/hides based on method\n",
    "    def update_parameter_visibility(change):\n",
    "        method = change['new']\n",
    "        z_threshold.layout.display = 'block' if method == 'Z-Score' else 'none'\n",
    "        mod_z_threshold.layout.display = 'block' if method == 'Modified Z-Score' else 'none'\n",
    "        iqr_k.layout.display = 'block' if method == 'IQR' else 'none'\n",
    "        mahal_percentile.layout.display = 'block' if method == 'Mahalanobis' else 'none'\n",
    "    \n",
    "    method_selector.observe(update_parameter_visibility, names='value')\n",
    "    \n",
    "    # Initial parameter visibility\n",
    "    mod_z_threshold.layout.display = 'none'\n",
    "    iqr_k.layout.display = 'none'\n",
    "    mahal_percentile.layout.display = 'none'\n",
    "    \n",
    "    # Layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üîß Detection Parameters</h3>\"),\n",
    "        method_selector,\n",
    "        z_threshold,\n",
    "        mod_z_threshold,\n",
    "        iqr_k,\n",
    "        mahal_percentile,\n",
    "        widgets.HTML(\"<h3>üìä Feature Selection</h3>\"),\n",
    "        feature_selector,\n",
    "        run_button\n",
    "    ])\n",
    "    \n",
    "    return widgets.VBox([controls, output])\n",
    "\n",
    "# Display the interactive statistical detector\n",
    "print(\\\"üéõÔ∏è Interactive Statistical Outlier Detection\\\")\\nprint(\\\"Use the controls below to experiment with different detection methods and parameters:\\\")\\nstatistical_widget = create_statistical_detector_widget()\\ndisplay(statistical_widget)\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"VSC-ml-methods\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ü§ñ Machine Learning-Based Outlier Detection\\n\",\n",
    "    \"\\n\",\n",
    "    \"While statistical methods are interpretable and fast, machine learning approaches can capture more complex patterns and relationships in the data. Let's explore three powerful ML-based outlier detection algorithms:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **üå≤ Isolation Forest**: Isolates outliers by randomly partitioning the data\\n\",\n",
    "    \"2. **üéØ One-Class SVM**: Learns a boundary around normal data points\\n\",\n",
    "    \"3. **üìç Local Outlier Factor (LOF)**: Considers local density of data points\\n\",\n",
    "    \"\\n\",\n",
    "    \"These methods are particularly effective for:\\n\",\n",
    "    \"- High-dimensional data\\n\",\n",
    "    \"- Complex, non-linear patterns\\n\",\n",
    "    \"- Data with multiple normal clusters\\n\",\n",
    "    \"- When ground truth is limited\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"VSC-ml-detectors\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class InteractiveMLDetectors:\\n\",\n",
    "    \"    \\\"\\\"\\\"Interactive machine learning-based outlier detection methods.\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self, data):\\n\",\n",
    "    \"        self.data = data\\n\",\n",
    "    \"        self.feature_cols = [col for col in data.columns \\n\",\n",
    "    \"                           if col not in ['recipe', 'timestamp', 'true_outlier']]\\n\",\n",
    "    \"        self.results = {}\\n\",\n",
    "    \"        self.models = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def isolation_forest_detection(self, contamination=0.1, n_estimators=100, features=None, random_state=42):\\n\",\n",
    "    \"        \\\"\\\"\\\"Isolation Forest based outlier detection.\\\"\\\"\\\"\\n\",\n",
    "    \"        if features is None:\\n\",\n",
    "    \"            features = self.feature_cols\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        feature_data = self.data[features].copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Handle missing values\\n\",\n",
    "    \"        feature_data = feature_data.fillna(feature_data.median())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Scale the data\\n\",\n",
    "    \"        scaler = StandardScaler()\\n\",\n",
    "    \"        data_scaled = scaler.fit_transform(feature_data)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Train Isolation Forest\\n\",\n",
    "    \"        model = IsolationForest(\\n\",\n",
    "    \"            contamination=contamination,\\n\",\n",
    "    \"            n_estimators=n_estimators,\\n\",\n",
    "    \"            random_state=random_state,\\n\",\n",
    "    \"            n_jobs=-1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Fit and predict\\n\",\n",
    "    \"        predictions = model.fit_predict(data_scaled)\\n\",\n",
    "    \"        scores = model.decision_function(data_scaled)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert predictions to boolean outliers\\n\",\n",
    "    \"        outliers = predictions == -1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert scores to positive values (higher = more anomalous)\\n\",\n",
    "    \"        anomaly_scores = -scores\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Store model and results\\n\",\n",
    "    \"        self.models['isolation_forest'] = {'model': model, 'scaler': scaler}\\n\",\n",
    "    \"        self.results['isolation_forest'] = {\\n\",\n",
    "    \"            'outliers': outliers,\\n\",\n",
    "    \"            'scores': anomaly_scores,\\n\",\n",
    "    \"            'contamination': contamination,\\n\",\n",
    "    \"            'n_estimators': n_estimators,\\n\",\n",
    "    \"            'n_outliers': outliers.sum(),\\n\",\n",
    "    \"            'percentage': outliers.mean() * 100\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return outliers, anomaly_scores\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def oneclass_svm_detection(self, nu=0.1, kernel='rbf', gamma='scale', features=None):\\n\",\n",
    "    \"        \\\"\\\"\\\"One-Class SVM based outlier detection.\\\"\\\"\\\"\\n\",\n",
    "    \"        if features is None:\\n\",\n",
    "    \"            features = self.feature_cols\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        feature_data = self.data[features].copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Handle missing values\\n\",\n",
    "    \"        feature_data = feature_data.fillna(feature_data.median())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Scale the data (SVM requires scaling)\\n\",\n",
    "    \"        scaler = StandardScaler()\\n\",\n",
    "    \"        data_scaled = scaler.fit_transform(feature_data)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Train One-Class SVM\\n\",\n",
    "    \"        model = OneClassSVM(nu=nu, kernel=kernel, gamma=gamma)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Fit and predict\\n\",\n",
    "    \"        predictions = model.fit_predict(data_scaled)\\n\",\n",
    "    \"        scores = model.decision_function(data_scaled)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert predictions to boolean outliers\\n\",\n",
    "    \"        outliers = predictions == -1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert scores to positive values\\n\",\n",
    "    \"        anomaly_scores = -scores\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Store model and results\\n\",\n",
    "    \"        self.models['oneclass_svm'] = {'model': model, 'scaler': scaler}\\n\",\n",
    "    \"        self.results['oneclass_svm'] = {\\n\",\n",
    "    \"            'outliers': outliers,\\n\",\n",
    "    \"            'scores': anomaly_scores,\\n\",\n",
    "    \"            'nu': nu,\\n\",\n",
    "    \"            'kernel': kernel,\\n\",\n",
    "    \"            'gamma': gamma,\\n\",\n",
    "    \"            'n_outliers': outliers.sum(),\\n\",\n",
    "    \"            'percentage': outliers.mean() * 100\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return outliers, anomaly_scores\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def lof_detection(self, n_neighbors=20, contamination=0.1, features=None):\\n\",\n",
    "    \"        \\\"\\\"\\\"Local Outlier Factor based outlier detection.\\\"\\\"\\\"\\n\",\n",
    "    \"        if features is None:\\n\",\n",
    "    \"            features = self.feature_cols\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        feature_data = self.data[features].copy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Handle missing values\\n\",\n",
    "    \"        feature_data = feature_data.fillna(feature_data.median())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Scale the data\\n\",\n",
    "    \"        scaler = StandardScaler()\\n\",\n",
    "    \"        data_scaled = scaler.fit_transform(feature_data)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Train LOF\\n\",\n",
    "    \"        model = LocalOutlierFactor(\\n\",\n",
    "    \"            n_neighbors=n_neighbors,\\n\",\n",
    "    \"            contamination=contamination\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Fit and predict\\n\",\n",
    "    \"        predictions = model.fit_predict(data_scaled)\\n\",\n",
    "    \"        scores = model.negative_outlier_factor_\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert predictions to boolean outliers\\n\",\n",
    "    \"        outliers = predictions == -1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert scores to positive values\\n\",\n",
    "    \"        anomaly_scores = -scores\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Store results (LOF doesn't store the model for new predictions)\\n\",\n",
    "    \"        self.results['lof'] = {\\n\",\n",
    "    \"            'outliers': outliers,\\n\",\n",
    "    \"            'scores': anomaly_scores,\\n\",\n",
    "    \"            'n_neighbors': n_neighbors,\\n\",\n",
    "    \"            'contamination': contamination,\\n\",\n",
    "    \"            'n_outliers': outliers.sum(),\\n\",\n",
    "    \"            'percentage': outliers.mean() * 100\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return outliers, anomaly_scores\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def predict_new_data(self, new_data, method='isolation_forest'):\\n\",\n",
    "    \"        \\\"\\\"\\\"Predict outliers on new data using trained models.\\\"\\\"\\\"\\n\",\n",
    "    \"        if method not in self.models:\\n\",\n",
    "    \"            raise ValueError(f\\\"Model {method} not trained yet\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        model_info = self.models[method]\\n\",\n",
    "    \"        model = model_info['model']\\n\",\n",
    "    \"        scaler = model_info['scaler']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Preprocess new data\\n\",\n",
    "    \"        new_data_scaled = scaler.transform(new_data.fillna(new_data.median()))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Predict\\n\",\n",
    "    \"        predictions = model.predict(new_data_scaled)\\n\",\n",
    "    \"        scores = model.decision_function(new_data_scaled)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        outliers = predictions == -1\\n\",\n",
    "    \"        anomaly_scores = -scores\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return outliers, anomaly_scores\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize the ML detector\\n\",\n",
    "    \"ml_detector = InteractiveMLDetectors(semiconductor_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Machine Learning outlier detectors initialized!\\\")\\n\",\n",
    "    \"print(f\\\"ü§ñ Ready to train models on {len(ml_detector.feature_cols)} process parameters\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"VSC-interactive-ml\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def create_ml_detector_widget():\\n\",\n",
    "    \"    \\\"\\\"\\\"Create interactive widget for machine learning outlier detection.\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Method selection\\n\",\n",
    "    \"    method_selector = widgets.Dropdown(\\n\",\n",
    "    \"        options=['Isolation Forest', 'One-Class SVM', 'Local Outlier Factor'],\\n\",\n",
    "    \"        value='Isolation Forest',\\n\",\n",
    "    \"        description='ML Method:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Isolation Forest parameters\\n\",\n",
    "    \"    if_contamination = widgets.FloatSlider(\\n\",\n",
    "    \"        value=0.1,\\n\",\n",
    "    \"        min=0.01,\\n\",\n",
    "    \"        max=0.3,\\n\",\n",
    "    \"        step=0.01,\\n\",\n",
    "    \"        description='Contamination:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if_n_estimators = widgets.IntSlider(\\n\",\n",
    "    \"        value=100,\\n\",\n",
    "    \"        min=50,\\n\",\n",
    "    \"        max=300,\\n\",\n",
    "    \"        step=25,\\n\",\n",
    "    \"        description='N Estimators:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # One-Class SVM parameters\\n\",\n",
    "    \"    svm_nu = widgets.FloatSlider(\\n\",\n",
    "    \"        value=0.1,\\n\",\n",
    "    \"        min=0.01,\\n\",\n",
    "    \"        max=0.5,\\n\",\n",
    "    \"        step=0.01,\\n\",\n",
    "    \"        description='Nu:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    svm_kernel = widgets.Dropdown(\\n\",\n",
    "    \"        options=['rbf', 'linear', 'poly', 'sigmoid'],\\n\",\n",
    "    \"        value='rbf',\\n\",\n",
    "    \"        description='Kernel:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    svm_gamma = widgets.Dropdown(\\n\",\n",
    "    \"        options=['scale', 'auto'],\\n\",\n",
    "    \"        value='scale',\\n\",\n",
    "    \"        description='Gamma:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # LOF parameters\\n\",\n",
    "    \"    lof_neighbors = widgets.IntSlider(\\n\",\n",
    "    \"        value=20,\\n\",\n",
    "    \"        min=5,\\n\",\n",
    "    \"        max=50,\\n\",\n",
    "    \"        step=5,\\n\",\n",
    "    \"        description='N Neighbors:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    lof_contamination = widgets.FloatSlider(\\n\",\n",
    "    \"        value=0.1,\\n\",\n",
    "    \"        min=0.01,\\n\",\n",
    "    \"        max=0.3,\\n\",\n",
    "    \"        step=0.01,\\n\",\n",
    "    \"        description='Contamination:',\\n\",\n",
    "    \"        style={'description_width': 'initial'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Feature selection\\n\",\n",
    "    \"    feature_selector = widgets.SelectMultiple(\\n\",\n",
    "    \"        options=ml_detector.feature_cols,\\n\",\n",
    "    \"        value=ml_detector.feature_cols[:8],  # Select first 8 by default\\n\",\n",
    "    \"        description='Features:',\\n\",\n",
    "    \"        style={'description_width': 'initial'},\\n\",\n",
    "    \"        layout={'height': '150px'}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Run button\\n\",\n",
    "    \"    run_button = widgets.Button(\\n\",\n",
    "    \"        description='ü§ñ Train & Detect',\\n\",\n",
    "    \"        button_style='success',\\n\",\n",
    "    \"        icon='cogs'\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Compare button\\n\",\n",
    "    \"    compare_button = widgets.Button(\\n\",\n",
    "    \"        description='‚öñÔ∏è Compare Methods',\\n\",\n",
    "    \"        button_style='info',\\n\",\n",
    "    \"        icon='balance-scale'\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Output area\\n\",\n",
    "    \"    output = widgets.Output()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def run_ml_detection(button):\\n\",\n",
    "    \"        \\\"\\\"\\\"Run the selected ML detection method.\\\"\\\"\\\"\\n\",\n",
    "    \"        with output:\\n\",\n",
    "    \"            clear_output(wait=True)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            method = method_selector.value\\n\",\n",
    "    \"            features = list(feature_selector.value)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if not features:\\n\",\n",
    "    \"                print(\\\"‚ùå Please select at least one feature\\\")\\n\",\n",
    "    \"                return\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"ü§ñ Training {method} on {len(features)} features...\\\")\\n\",\n",
    "    \"            start_time = time.time()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Run the appropriate detection method\\n\",\n",
    "    \"            if method == 'Isolation Forest':\\n\",\n",
    "    \"                outliers, scores = ml_detector.isolation_forest_detection(\\n\",\n",
    "    \"                    contamination=if_contamination.value,\\n\",\n",
    "    \"                    n_estimators=if_n_estimators.value,\\n\",\n",
    "    \"                    features=features\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"                result_key = 'isolation_forest'\\n\",\n",
    "    \"                \\n\",\n",
    "    \"            elif method == 'One-Class SVM':\\n\",\n",
    "    \"                outliers, scores = ml_detector.oneclass_svm_detection(\\n\",\n",
    "    \"                    nu=svm_nu.value,\\n\",\n",
    "    \"                    kernel=svm_kernel.value,\\n\",\n",
    "    \"                    gamma=svm_gamma.value,\\n\",\n",
    "    \"                    features=features\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"                result_key = 'oneclass_svm'\\n\",\n",
    "    \"                \\n\",\n",
    "    \"            elif method == 'Local Outlier Factor':\\n\",\n",
    "    \"                outliers, scores = ml_detector.lof_detection(\\n\",\n",
    "    \"                    n_neighbors=lof_neighbors.value,\\n\",\n",
    "    \"                    contamination=lof_contamination.value,\\n\",\n",
    "    \"                    features=features\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"                result_key = 'lof'\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            training_time = time.time() - start_time\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Display results\\n\",\n",
    "    \"            result = ml_detector.results[result_key]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"\\\\nüìä {method} Results:\\\")\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            print(f\\\"Training time: {training_time:.2f} seconds\\\")\\n\",\n",
    "    \"            print(f\\\"Outliers detected: {result['n_outliers']} ({result['percentage']:.1f}%)\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Performance evaluation\\n\",\n",
    "    \"            true_outliers = semiconductor_data['true_outlier'].values\\n\",\n",
    "    \"            tp = np.sum(outliers & true_outliers)\\n\",\n",
    "    \"            fp = np.sum(outliers & ~true_outliers)\\n\",\n",
    "    \"            fn = np.sum(~outliers & true_outliers)\\n\",\n",
    "    \"            tn = np.sum(~outliers & ~true_outliers)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\\n\",\n",
    "    \"            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\\n\",\n",
    "    \"            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\\n\",\n",
    "    \"            accuracy = (tp + tn) / len(true_outliers)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"\\\\nüéØ Performance Metrics:\\\")\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            print(f\\\"Accuracy:        {accuracy:>7.3f}\\\")\\n\",\n",
    "    \"            print(f\\\"Precision:       {precision:>7.3f}\\\")\\n\",\n",
    "    \"            print(f\\\"Recall:          {recall:>7.3f}\\\")\\n\",\n",
    "    \"            print(f\\\"F1-Score:        {f1:>7.3f}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Model-specific info\\n\",\n",
    "    \"            print(f\\\"\\\\n‚öôÔ∏è Model Parameters:\\\")\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            if method == 'Isolation Forest':\\n\",\n",
    "    \"                print(f\\\"Contamination:   {result['contamination']:.3f}\\\")\\n\",\n",
    "    \"                print(f\\\"N Estimators:    {result['n_estimators']}\\\")\\n\",\n",
    "    \"            elif method == 'One-Class SVM':\\n\",\n",
    "    \"                print(f\\\"Nu:              {result['nu']:.3f}\\\")\\n\",\n",
    "    \"                print(f\\\"Kernel:          {result['kernel']}\\\")\\n\",\n",
    "    \"                print(f\\\"Gamma:           {result['gamma']}\\\")\\n\",\n",
    "    \"            elif method == 'Local Outlier Factor':\\n\",\n",
    "    \"                print(f\\\"N Neighbors:     {result['n_neighbors']}\\\")\\n\",\n",
    "    \"                print(f\\\"Contamination:   {result['contamination']:.3f}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Create visualization\\n\",\n",
    "    \"            create_ml_visualization(outliers, scores, method, features)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def compare_all_methods(button):\\n\",\n",
    "    \"        \\\"\\\"\\\"Compare all ML methods side by side.\\\"\\\"\\\"\\n\",\n",
    "    \"        with output:\\n\",\n",
    "    \"            clear_output(wait=True)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            features = list(feature_selector.value)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if not features:\\n\",\n",
    "    \"                print(\\\"‚ùå Please select at least one feature\\\")\\n\",\n",
    "    \"                return\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"‚öñÔ∏è Comparing all ML methods on {len(features)} features...\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Run all methods\\n\",\n",
    "    \"            methods_results = {}\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Isolation Forest\\n\",\n",
    "    \"            print(\\\"üå≤ Running Isolation Forest...\\\")\\n\",\n",
    "    \"            start_time = time.time()\\n\",\n",
    "    \"            if_outliers, if_scores = ml_detector.isolation_forest_detection(\\n\",\n",
    "    \"                contamination=if_contamination.value,\\n\",\n",
    "    \"                n_estimators=if_n_estimators.value,\\n\",\n",
    "    \"                features=features\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"            methods_results['Isolation Forest'] = {\\n\",\n",
    "    \"                'outliers': if_outliers,\\n\",\n",
    "    \"                'scores': if_scores,\\n\",\n",
    "    \"                'time': time.time() - start_time\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # One-Class SVM\\n\",\n",
    "    \"            print(\\\"üéØ Running One-Class SVM...\\\")\\n\",\n",
    "    \"            start_time = time.time()\\n\",\n",
    "    \"            svm_outliers, svm_scores = ml_detector.oneclass_svm_detection(\\n\",\n",
    "    \"                nu=svm_nu.value,\\n\",\n",
    "    \"                kernel=svm_kernel.value,\\n\",\n",
    "    \"                gamma=svm_gamma.value,\\n\",\n",
    "    \"                features=features\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"            methods_results['One-Class SVM'] = {\\n\",\n",
    "    \"                'outliers': svm_outliers,\\n\",\n",
    "    \"                'scores': svm_scores,\\n\",\n",
    "    \"                'time': time.time() - start_time\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # LOF\\n\",\n",
    "    \"            print(\\\"üìç Running Local Outlier Factor...\\\")\\n\",\n",
    "    \"            start_time = time.time()\\n\",\n",
    "    \"            lof_outliers, lof_scores = ml_detector.lof_detection(\\n\",\n",
    "    \"                n_neighbors=lof_neighbors.value,\\n\",\n",
    "    \"                contamination=lof_contamination.value,\\n\",\n",
    "    \"                features=features\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"            methods_results['LOF'] = {\\n\",\n",
    "    \"                'outliers': lof_outliers,\\n\",\n",
    "    \"                'scores': lof_scores,\\n\",\n",
    "    \"                'time': time.time() - start_time\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Create comparison visualization\\n\",\n",
    "    \"            create_ml_comparison_visualization(methods_results, features)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def create_ml_visualization(outliers, scores, method, features):\\n\",\n",
    "    \"        \\\"\\\"\\\"Create visualization for ML outlier detection results.\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create subplots\\n\",\n",
    "    \"        fig = make_subplots(\\n\",\n",
    "    \"            rows=2, cols=2,\\n\",\n",
    "    \"            subplot_titles=[\\n\",\n",
    "    \"                f'{method} Detection Results',\\n\",\n",
    "    \"                'Anomaly Score Distribution',\\n\",\n",
    "    \"                'Feature Space (2D Projection)',\\n\",\n",
    "    \"                'Performance Analysis'\\n\",\n",
    "    \"            ]\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 1. Detection results scatter\\n\",\n",
    "    \"        normal_indices = np.where(~outliers)[0]\\n\",\n",
    "    \"        outlier_indices = np.where(outliers)[0]\\n\",\n",
    "    \"        true_outlier_indices = np.where(semiconductor_data['true_outlier'])[0]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Normal points\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(\\n\",\n",
    "    \"                x=normal_indices,\\n\",\n",
    "    \"                y=scores[normal_indices],\\n\",\n",
    "    \"                mode='markers',\\n\",\n",
    "    \"                name='Normal',\\n\",\n",
    "    \"                marker=dict(color='blue', size=4, opacity=0.6)\\n\",\n",
    "    \"            ), row=1, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Detected outliers\\n\",\n",
    "    \"        if len(outlier_indices) > 0:\\n\",\n",
    "    \"            fig.add_trace(\\n\",\n",
    "    \"                go.Scatter(\\n\",\n",
    "    \"                    x=outlier_indices,\\n\",\n",
    "    \"                    y=scores[outlier_indices],\\n\",\n",
    "    \"                    mode='markers',\\n\",\n",
    "    \"                    name='Detected Outliers',\\n\",\n",
    "    \"                    marker=dict(color='red', size=8, symbol='diamond')\\n\",\n",
    "    \"                ), row=1, col=1\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # True outliers overlay\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(\\n\",\n",
    "    \"                x=true_outlier_indices,\\n\",\n",
    "    \"                y=scores[true_outlier_indices],\\n\",\n",
    "    \"                mode='markers',\\n\",\n",
    "    \"                name='True Outliers',\\n\",\n",
    "    \"                marker=dict(color='orange', size=10, symbol='x', line=dict(width=2))\\n\",\n",
    "    \"            ), row=1, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 2. Score distribution\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Histogram(\\n\",\n",
    "    \"                x=scores,\\n\",\n",
    "    \"                name='Anomaly Scores',\\n\",\n",
    "    \"                nbinsx=30,\\n\",\n",
    "    \"                opacity=0.7,\\n\",\n",
    "    \"                marker_color='lightblue'\\n\",\n",
    "    \"            ), row=1, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 3. 2D feature space (if we have at least 2 features)\\n\",\n",
    "    \"        if len(features) >= 2:\\n\",\n",
    "    \"            feat1, feat2 = features[0], features[1]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Normal points\\n\",\n",
    "    \"            fig.add_trace(\\n\",\n",
    "    \"                go.Scatter(\\n\",\n",
    "    \"                    x=semiconductor_data.loc[~outliers, feat1],\\n\",\n",
    "    \"                    y=semiconductor_data.loc[~outliers, feat2],\\n\",\n",
    "    \"                    mode='markers',\\n\",\n",
    "    \"                    name=f'Normal ({feat1} vs {feat2})',\\n\",\n",
    "    \"                    marker=dict(color='blue', size=4, opacity=0.6)\\n\",\n",
    "    \"                ), row=2, col=1\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Outliers\\n\",\n",
    "    \"            if len(outlier_indices) > 0:\\n\",\n",
    "    \"                fig.add_trace(\\n\",\n",
    "    \"                    go.Scatter(\\n\",\n",
    "    \"                        x=semiconductor_data.loc[outliers, feat1],\\n\",\n",
    "    \"                        y=semiconductor_data.loc[outliers, feat2],\\n\",\n",
    "    \"                        mode='markers',\\n\",\n",
    "    \"                        name=f'Outliers ({feat1} vs {feat2})',\\n\",\n",
    "    \"                        marker=dict(color='red', size=8, symbol='diamond')\\n\",\n",
    "    \"                    ), row=2, col=1\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 4. Performance metrics visualization\\n\",\n",
    "    \"        true_outliers = semiconductor_data['true_outlier'].values\\n\",\n",
    "    \"        tp = np.sum(outliers & true_outliers)\\n\",\n",
    "    \"        fp = np.sum(outliers & ~true_outliers)\\n\",\n",
    "    \"        fn = np.sum(~outliers & true_outliers)\\n\",\n",
    "    \"        tn = np.sum(~outliers & ~true_outliers)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Confusion matrix heatmap\\n\",\n",
    "    \"        conf_matrix = np.array([[tn, fp], [fn, tp]])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Heatmap(\\n\",\n",
    "    \"                z=conf_matrix,\\n\",\n",
    "    \"                x=['Predicted Normal', 'Predicted Outlier'],\\n\",\n",
    "    \"                y=['Actual Normal', 'Actual Outlier'],\\n\",\n",
    "    \"                colorscale='Blues',\\n\",\n",
    "    \"                text=conf_matrix,\\n\",\n",
    "    \"                texttemplate=\\\"%{text}\\\",\\n\",\n",
    "    \"                textfont={\\\"size\\\": 16},\\n\",\n",
    "    \"                showscale=False\\n\",\n",
    "    \"            ), row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Update layout\\n\",\n",
    "    \"        fig.update_layout(\\n\",\n",
    "    \"            height=800,\\n\",\n",
    "    \"            title=f\\\"{method} Outlier Detection Analysis\\\",\\n\",\n",
    "    \"            showlegend=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Update axes labels\\n\",\n",
    "    \"        fig.update_xaxes(title_text=\\\"Sample Index\\\", row=1, col=1)\\n\",\n",
    "    \"        fig.update_yaxes(title_text=\\\"Anomaly Score\\\", row=1, col=1)\\n\",\n",
    "    \"        fig.update_xaxes(title_text=\\\"Anomaly Score\\\", row=1, col=2)\\n\",\n",
    "    \"        fig.update_yaxes(title_text=\\\"Frequency\\\", row=1, col=2)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(features) >= 2:\\n\",\n",
    "    \"            fig.update_xaxes(title_text=features[0], row=2, col=1)\\n\",\n",
    "    \"            fig.update_yaxes(title_text=features[1], row=2, col=1)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def create_ml_comparison_visualization(methods_results, features):\\n\",\n",
    "    \"        \\\"\\\"\\\"Create comparison visualization for all ML methods.\\\"\\\"\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Calculate performance metrics for each method\\n\",\n",
    "    \"        true_outliers = semiconductor_data['true_outlier'].values\\n\",\n",
    "    \"        performance_data = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for method_name, results in methods_results.items():\\n\",\n",
    "    \"            outliers = results['outliers']\\n\",\n",
    "    \"            tp = np.sum(outliers & true_outliers)\\n\",\n",
    "    \"            fp = np.sum(outliers & ~true_outliers)\\n\",\n",
    "    \"            fn = np.sum(~outliers & true_outliers)\\n\",\n",
    "    \"            tn = np.sum(~outliers & ~true_outliers)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\\n\",\n",
    "    \"            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\\n\",\n",
    "    \"            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\\n\",\n",
    "    \"            accuracy = (tp + tn) / len(true_outliers)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            performance_data.append({\\n\",\n",
    "    \"                'Method': method_name,\\n\",\n",
    "    \"                'Accuracy': accuracy,\\n\",\n",
    "    \"                'Precision': precision,\\n\",\n",
    "    \"                'Recall': recall,\\n\",\n",
    "    \"                'F1-Score': f1,\\n\",\n",
    "    \"                'Time (s)': results['time'],\\n\",\n",
    "    \"                'N_Outliers': outliers.sum()\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create comparison visualizations\\n\",\n",
    "    \"        fig = make_subplots(\\n\",\n",
    "    \"            rows=2, cols=2,\\n\",\n",
    "    \"            subplot_titles=[\\n\",\n",
    "    \"                'Performance Metrics Comparison',\\n\",\n",
    "    \"                'Training Time Comparison',\\n\",\n",
    "    \"                'Outlier Count Comparison',\\n\",\n",
    "    \"                'Method Agreement Analysis'\\n\",\n",
    "    \"            ]\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        methods = list(methods_results.keys())\\n\",\n",
    "    \"        colors = ['blue', 'red', 'green']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 1. Performance metrics\\n\",\n",
    "    \"        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\\n\",\n",
    "    \"        for i, metric in enumerate(metrics):\\n\",\n",
    "    \"            values = [p[metric] for p in performance_data]\\n\",\n",
    "    \"            fig.add_trace(\\n\",\n",
    "    \"                go.Bar(\\n\",\n",
    "    \"                    x=methods,\\n\",\n",
    "    \"                    y=values,\\n\",\n",
    "    \"                    name=metric,\\n\",\n",
    "    \"                    text=[f\\\"{v:.3f}\\\" for v in values],\\n\",\n",
    "    \"                    textposition='auto'\\n\",\n",
    "    \"                ), row=1, col=1\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 2. Training time\\n\",\n",
    "    \"        times = [p['Time (s)'] for p in performance_data]\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(\\n\",\n",
    "    \"                x=methods,\\n\",\n",
    "    \"                y=times,\\n\",\n",
    "    \"                name='Training Time',\\n\",\n",
    "    \"                marker_color='orange',\\n\",\n",
    "    \"                text=[f\\\"{t:.2f}s\\\" for t in times],\\n\",\n",
    "    \"                textposition='auto'\\n\",\n",
    "    \"            ), row=1, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 3. Outlier counts\\n\",\n",
    "    \"        outlier_counts = [p['N_Outliers'] for p in performance_data]\\n\",\n",
    "    \"        true_count = np.sum(true_outliers)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(\\n\",\n",
    "    \"                x=methods,\\n\",\n",
    "    \"                y=outlier_counts,\\n\",\n",
    "    \"                name='Detected Outliers',\\n\",\n",
    "    \"                marker_color='purple',\\n\",\n",
    "    \"                text=outlier_counts,\\n\",\n",
    "    \"                textposition='auto'\\n\",\n",
    "    \"            ), row=2, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add true outlier count line\\n\",\n",
    "    \"        fig.add_hline(\\n\",\n",
    "    \"            y=true_count,\\n\",\n",
    "    \"            line_dash=\\\"dash\\\",\\n\",\n",
    "    \"            line_color=\\\"red\\\",\\n\",\n",
    "    \"            annotation_text=f\\\"True Outliers: {true_count}\\\",\\n\",\n",
    "    \"            row=2, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 4. Method agreement matrix\\n\",\n",
    "    \"        agreement_matrix = np.zeros((len(methods), len(methods)))\\n\",\n",
    "    \"        for i, method1 in enumerate(methods):\\n\",\n",
    "    \"            for j, method2 in enumerate(methods):\\n\",\n",
    "    \"                outliers1 = methods_results[method1]['outliers']\\n\",\n",
    "    \"                outliers2 = methods_results[method2]['outliers']\\n\",\n",
    "    \"                agreement = np.sum(outliers1 == outliers2) / len(outliers1)\\n\",\n",
    "    \"                agreement_matrix[i, j] = agreement\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Heatmap(\\n\",\n",
    "    \"                z=agreement_matrix,\\n\",\n",
    "    \"                x=methods,\\n\",\n",
    "    \"                y=methods,\\n\",\n",
    "    \"                colorscale='RdYlBu_r',\\n\",\n",
    "    \"                text=np.round(agreement_matrix, 3),\\n\",\n",
    "    \"                texttemplate=\\\"%{text}\\\",\\n\",\n",
    "    \"                textfont={\\\"size\\\": 12},\\n\",\n",
    "    \"                colorbar=dict(title=\\\"Agreement\\\")\\n\",\n",
    "    \"            ), row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Update layout\\n\",\n",
    "    \"        fig.update_layout(\\n\",\n",
    "    \"            height=800,\\n\",\n",
    "    \"            title=\\\"ML Methods Comparison Dashboard\\\",\\n\",\n",
    "    \"            showlegend=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Print summary table\\n\",\n",
    "    \"        print(\\\"\\\\nüìä Performance Summary Table:\\\")\\n\",\n",
    "    \"        print(\\\"=\\\" * 80)\\n\",\n",
    "    \"        print(f\\\"{'Method':<18} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Time (s)':<10}\\\")\\n\",\n",
    "    \"        print(\\\"=\\\" * 80)\\n\",\n",
    "    \"        for p in performance_data:\\n\",\n",
    "    \"            print(f\\\"{p['Method']:<18} {p['Accuracy']:<10.3f} {p['Precision']:<10.3f} {p['Recall']:<10.3f} {p['F1-Score']:<10.3f} {p['Time (s)']:<10.2f}\\\")\\n\",\n",
    "    \"        print(\\\"=\\\" * 80)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Connect buttons to functions\\n\",\n",
    "    \"    run_button.on_click(run_ml_detection)\\n\",\n",
    "    \"    compare_button.on_click(compare_all_methods)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create parameter control panels that show/hide based on method\\n\",\n",
    "    \"    def update_parameter_visibility(change):\\n\",\n",
    "    \"        method = change['new']\\n\",\n",
    "    \"        # Isolation Forest parameters\\n\",\n",
    "    \"        if_contamination.layout.display = 'block' if method == 'Isolation Forest' else 'none'\\n\",\n",
    "    \"        if_n_estimators.layout.display = 'block' if method == 'Isolation Forest' else 'none'\\n\",\n",
    "    \"        # SVM parameters\\n\",\n",
    "    \"        svm_nu.layout.display = 'block' if method == 'One-Class SVM' else 'none'\\n\",\n",
    "    \"        svm_kernel.layout.display = 'block' if method == 'One-Class SVM' else 'none'\\n\",\n",
    "    \"        svm_gamma.layout.display = 'block' if method == 'One-Class SVM' else 'none'\\n\",\n",
    "    \"        # LOF parameters\\n\",\n",
    "    \"        lof_neighbors.layout.display = 'block' if method == 'Local Outlier Factor' else 'none'\\n\",\n",
    "    \"        lof_contamination.layout.display = 'block' if method == 'Local Outlier Factor' else 'none'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    method_selector.observe(update_parameter_visibility, names='value')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Initial parameter visibility\\n\",\n",
    "    \"    svm_nu.layout.display = 'none'\\n\",\n",
    "    \"    svm_kernel.layout.display = 'none'\\n\",\n",
    "    \"    svm_gamma.layout.display = 'none'\\n\",\n",
    "    \"    lof_neighbors.layout.display = 'none'\\n\",\n",
    "    \"    lof_contamination.layout.display = 'none'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Layout\\n\",\n",
    "    \"    controls = widgets.VBox([\\n\",\n",
    "    \"        widgets.HTML(\\\"<h3>ü§ñ ML Algorithm Selection</h3>\\\"),\\n\",\n",
    "    \"        method_selector,\\n\",\n",
    "    \"        widgets.HTML(\\\"<h3>‚öôÔ∏è Algorithm Parameters</h3>\\\"),\\n\",\n",
    "    \"        # Isolation Forest\\n\",\n",
    "    \"        if_contamination,\\n\",\n",
    "    \"        if_n_estimators,\\n\",\n",
    "    \"        # One-Class SVM\\n\",\n",
    "    \"        svm_nu,\\n\",\n",
    "    \"        svm_kernel,\\n\",\n",
    "    \"        svm_gamma,\\n\",\n",
    "    \"        # LOF\\n\",\n",
    "    \"        lof_neighbors,\\n\",\n",
    "    \"        lof_contamination,\\n\",\n",
    "    \"        widgets.HTML(\\\"<h3>üìä Feature Selection</h3>\\\"),\\n\",\n",
    "    \"        feature_selector,\\n\",\n",
    "    \"        widgets.HTML(\\\"<h3>üöÄ Actions</h3>\\\"),\\n\",\n",
    "    \"        widgets.HBox([run_button, compare_button])\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return widgets.VBox([controls, output])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display the interactive ML detector\\n\",\n",
    "    \"print(\\\"ü§ñ Interactive Machine Learning Outlier Detection\\\")\\n\",\n",
    "    \"print(\\\"Experiment with different ML algorithms and compare their performance:\\\")\\n\",\n",
    "    \"ml_widget = create_ml_detector_widget()\\ndisplay(ml_widget)\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
