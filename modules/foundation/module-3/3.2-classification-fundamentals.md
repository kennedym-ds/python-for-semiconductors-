---
post_title: "Module 3.2 – Classification Fundamentals for Semiconductor Manufacturing"
author1: "Your Name"
post_slug: "module-3-2-classification-fundamentals"
microsoft_alias: "alias"
featured_image: "/images/classification-wafer-map.png"
categories: [data-science, machine-learning, manufacturing]
tags: [classification, logistic-regression, random-forest, svm, imbalance, semiconductor, yield]
ai_note: "Initial draft assisted by AI; domain validation recommended."
summary: "Foundational supervised classification concepts tailored to semiconductor yield, defect, and excursion detection problems, covering algorithms, metrics, class imbalance strategies, and production workflow."
post_date: 2025-08-31
---

# Module 3.2 – Classification Fundamentals for Semiconductor Manufacturing

## Overview

Classification maps process, metrology, and test-derived features to discrete manufacturing outcomes (e.g., PASS/FAIL die bin, defect category, excursion flag). Correctly framing these problems enables proactive yield protection, scrap reduction, and faster root‑cause isolation.

### Dataset References

- **SECOM (UCI)**: Early binary pass/fail examples; high-dimensional, missingness handling demonstration.
- **WM-811K Wafer Maps (Kaggle)**: Multiclass spatial defect pattern classification (center, edge ring, scratch, etc.). Use for feature representation (flattened matrices vs CNN) and class imbalance strategies.
- **Vision Defect / PCB Proxy**: For introductory vision classification until wafer images are downloaded.

See `datasets/README.md` for acquisition steps and `datasets/download_semiconductor_datasets.py` for automated SECOM retrieval.

## Semiconductor Context & Example Use Cases

- Defect pattern detection (center, edge-ring, scratch) for rapid excursion triage.
- Early die bin / test pass prediction from inline parametrics to shorten feedback loops.
- Tool state health classification (normal, drift, fault) from high-frequency sensor traces.
- Lot hold risk flagging before committing expensive downstream steps.

## Data Characteristics & Challenges

- High-dimensional parametric & sensor feature sets (correlated, varying scales).
- Strong class imbalance (excursions often <1%).
- Temporal / lot hierarchy (risk of leakage if future info used in training features).
- Mixed feature types: continuous metrology, categorical tool/route IDs, engineered ratios & lags.

## Core Algorithms

### 1. Logistic Regression

- Interpretable linear decision boundary; outputs well-behaved probabilities (with proper regularization).
- Regularization (L2, L1, Elastic Net) controls variance; L1 yields sparse, interpretable feature subset.
- Solver choices: lbfgs (general), saga (supports Elastic Net & large sparse), liblinear (small datasets + L1).
- Baseline benchmark: fast to train & provides coefficient directional insight.

### 2. Linear & Kernel SVM (SVC)

- Maximizes margin; robust in high-dimensional spaces.
- Kernels (RBF, polynomial) capture non-linear structure; hyperparameters C and γ require tuning.
- `probability=True` adds CV pass; otherwise use `decision_function` for threshold optimization.
- `class_weight='balanced'` mitigates mild–moderate imbalance without synthetic sampling.

### 3. Decision Trees

- Non-parametric, capture interactions & handle mixed scales without preprocessing.
- Overfitting risk: manage with `max_depth`, `min_samples_leaf`, pruning via `ccp_alpha`.
- Offer transparent rule paths—valuable for engineering review and root-cause hypotheses.

### 4. Ensemble Methods

- Random Forest: Bagged trees + feature subsampling; stable performance, unbiased OOB estimate (approx.).
- Gradient Boosting / HistGradientBoosting: Sequential residual correction; strong accuracy with modest tuning (`learning_rate`, depth/leaves); histogram variant handles missing & categorical (encoded) efficiently.
- AdaBoost: Emphasizes previously misclassified samples; less robust to label noise.
- Stacking / Voting: Blend complementary models (linear + tree + SVM) for potential lift and calibration improvements.

### 5. Other Baselines

- Naive Bayes (log/text-like or count features).
- k-NN (simple, but scaling & distance metric sensitivity; rarely production for high-dim manufacturing data).
- Linear Discriminant Analysis (approximate Gaussian class distributions, shared covariance).

## Feature Engineering Strategies

- Domain ratios (e.g., critical dimension uniformity = 3σ / mean).
- Temporal drift indicators (rolling z-scores by tool; only past data in production window).
- Interaction terms for physically coupled processes (etch depth × implant dose).
- Log transforms for skewed positive metrics (defect density, leak currents).
- Binning continuous sensors to stabilize tree splits (avoid exploding cardinality).
- Outlier flags (from Module 2.2 pipelines) as binary/categorical inputs.

## Handling Class Imbalance

| Approach | When | Notes |
|----------|------|-------|
| Class weights (`class_weight='balanced'`) | Mild imbalance | Fast; adjusts loss function weights. |
| RandomOverSampler | Very rare minority, baseline | Risk of overfitting duplicates. |
| SMOTE / BorderlineSMOTE | Continuous feature space | Synthesizes minority; risk if heavy overlap. |
| SMOTENC / SMOTEN | Mixed or categorical features | Respects categorical modes. |
| ADASYN | Focus on hard examples | May overfit noisy boundary points. |
| Undersampling (Random, Tomek) | Large majority, quick iteration | Possible information loss; watch variance. |
| Balanced RF / EasyEnsemble | Severe imbalance + complex boundary | Combine sampling + ensembles. |

Progression:

1. Start with class weights → evaluate ROC AUC & PR AUC.
2. If insufficient recall at target precision, introduce SMOTE variant inside CV Pipeline (no leakage).
3. Re-check calibration (synthetic sampling can distort probability density).
4. Escalate to ensemble balancing for severe cases (<0.5% minority) or complex boundaries.

## Evaluation Metrics

| Metric | Use Case | Pitfall |
|--------|----------|---------|
| Accuracy | Balanced scenario sanity check | Misleading with extreme imbalance. |
| Precision | High cost of false alarm | Ignores missed excursions. |
| Recall | High cost of miss | May induce many false positives. |
| F1 | Balance P/R | Symmetric; may not match business costs. |
| ROC AUC | Global ranking | Inflated impression under heavy imbalance. |
| PR AUC | Minority-focused ranking | Higher variance; interpret vs baseline prevalence. |
| MCC | Balanced overall correlation | Less intuitive but robust. |
| Balanced Accuracy | Class-normalized accuracy | Still threshold-dependent. |
| Brier Score | Probability calibration | Not class-weighted; affected by prevalence. |
| Log Loss | Penalize overconfident errors | Sensitive to miscalibration. |

Metric Strategy:

- Always pair ROC AUC with PR AUC for imbalanced tasks.
- Track operational KPI: e.g., Recall at Precision ≥ 0.9 (manufacturing review bandwidth constraint).
- Calibrate probabilities if Brier/Log Loss degraded after resampling or boosting.
- Optimize threshold for cost-weighted utility: `argmax_t (Benefit_TP * TP(t) - Cost_FP * FP(t)) / N`.

## Cross-Validation & Data Splitting

- Stratified splits to preserve minority proportion.
- Lot/temporal ordering: time-aware blocked splits (train on earlier lots, validate on later) to reflect deployment reality.
- Group leakage control: GroupKFold on lot ID when multiple wafers per lot.
- Nested CV for unbiased threshold & hyperparameter selection where high-stakes decisions rely on precision/recall guarantees.
- Keep any over/under-sampling strictly inside training folds via `imblearn.pipeline.Pipeline`.

## Production Pipeline Pattern

1. Ingest raw extracts (parametrics, sensor summaries).
2. Deterministic feature engineering (ratios, lags) without future leakage.
3. Preprocess (impute, scale) as needed for chosen estimator.
4. Optional imbalance sampler (training only) inside Pipeline.
5. Hyperparameter tuning (multi-metric scoring: {'roc_auc': primary, 'average_precision': secondary}).
6. Probability calibration if operational thresholding depends on probability reliability.
7. Threshold selection & lock with documented business rationale.
8. Persist artifacts (pipeline.pkl, threshold.json, metrics.json, feature_manifest.json, metadata.yaml).
9. Monitor: feature drift (PSI/Kolmogorov), calibration drift, minority prevalence shift, recall at locked precision.

## Probability Calibration

- Evaluate calibration curves & Brier Score post-selection.
- Calibrate after model & threshold selection (avoid leakage); use a held-out validation or cross-validated approach.
- Avoid calibrating on synthetically oversampled distribution; prefer natural class mix subset.

## Interpretability & Diagnostics

- Coefficient analysis (logistic) for directional process signals.
- Permutation importance over impurity importance to mitigate bias toward high-cardinality features.
- SHAP values (optional library) for local + global explanation of tree/boosting models.
- Partial dependence / ALE to validate expected monotonic relationships (e.g., defect density vs fail probability).

## Common Pitfalls

- Performing SMOTE before train/test split (leakage).
- Relying on accuracy with 1:100 imbalance.
- Selecting threshold on test set (optimistic performance).
- Ignoring cost asymmetry between FN (missed excursion) and FP (manual review).
- Over-interpreting impurity-based feature importances for correlated inputs.

## Governance & Reproducibility

- Version all artifacts with model + data hash; include sampling strategy and threshold.
- Log training command (CLI args, git commit) and environment (library versions) for audit.
- Track class distribution at train & inference time; alert on significant prevalence shifts.
- Schedule periodic recalibration / retraining when drift or performance degradation detected.

## Next Steps in Module 3.2

1. Build exploratory notebook: synthetic + SECOM (derive binary target), baseline models, imbalance experiments, threshold tuning.
2. Implement `ClassificationPipeline` production script (mirrors regression pipeline architecture).
3. Create quick reference sheet (metrics, imbalance decision flow, thresholding formulae).
4. Add automated test (train small pipeline, assert recall at precision threshold; verify persistence & deterministic predictions with fixed seed).
5. Update README & requirements (add imbalanced-learn; optionally shap later).

## References

- scikit-learn User Guide (classification algorithms, metrics, calibration).
- imbalanced-learn documentation (RandomOverSampler, SMOTE variants, ADASYN, ensemble strategies).
- Manufacturing analytics best practices (leakage avoidance, lot-based validation protocols).

## Summary

This guide frames supervised classification for semiconductor yield & excursion detection: selecting appropriate algorithms, engineering leakage-safe features, addressing class imbalance, calibrating probabilities, and deploying threshold-governed, monitored models. Subsequent assets (notebook, pipeline, quick reference) will operationalize these principles.
