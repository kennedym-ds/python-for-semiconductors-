---
post_title: "Module 3.1 Regression FAQ"
author1: "Your Name"
post_slug: "module-3-1-regression-faq"
microsoft_alias: "alias"
featured_image: ""
categories: ["regression","semiconductors"]
tags: ["regression","manufacturing","statistics","modeling","feature-engineering"]
ai_note: "Partial AI assistance"
summary: "Frequently asked questions, troubleshooting tips, diagrams, and analogies for applying regression analysis to semiconductor manufacturing data."
post_date: "2025-09-03"
---

# Module 3.1 Regression FAQ

## Purpose

Targeted answers to common questions when implementing regression models for semiconductor process optimization, yield prediction, and equipment health monitoring.

## Quick Navigation

- [Core Concepts](#core-concepts)
- [Data & Features](#data--features)
- [Modeling & Validation](#modeling--validation)
- [Interpretability](#interpretability)
- [Production & Monitoring](#production--monitoring)
- [Analogies](#analogies)
- [Mermaid Diagrams](#mermaid-diagrams)
- [Troubleshooting Matrix](#troubleshooting-matrix)

---

## Core Concepts

**Q1. When should I start with linear regression vs a tree-based model?**  
Start with linear if you need transparent coefficients, have modest feature interactions, and expect approximately linear physical relationships. Jump to trees (Random Forest, Gradient Boosted Trees) when: (1) strong non-linearities, (2) high-order interactions, (3) many missing values or mixed scales.

**Q2. How do I know if polynomial features are physically meaningful?**  
Validate against domain physics: lithography (dose-focus curvature), diffusion (Arrhenius temperature dependence), plasma etch (non-linear pressure × power). If coefficients contradict known monotonic relationships, re‑examine scaling or leakage.

**Q3. What is the minimum sample size per feature?**  
Rule-of-thumb: linear models ~10–20 samples per predictor after feature selection. With 600 raw sensors (e.g., SECOM), you must reduce dimensionality (filtering, PCA, regularization) before fitting.

**Q4. When is R² misleading?**  
High R² with unstable coefficients (multicollinearity), data leakage, or narrow operating window. Always pair with residual diagnostics and prediction interval calibration.

## Data & Features

**Q5. How do I handle 50%+ missingness in a sensor?**  
Drop if redundant sensors exist or if missingness is not process-informative. Otherwise: create a missing-indicator feature + iterative imputation. Never impute across tool boundaries without stratification.

**Q6. Should I scale features before PCA if using physical units?**  
Yes—standardize unless preserving absolute magnitude is physically critical. Different sensor units (°C, Torr, sccm) distort PCA axes without scaling.

**Q7. How do I encode recipe/tool categorical variables?**  
High-cardinality: target or impact encoding with time-split leakage guards. Moderate: one-hot. Always evaluate whether tool identity should instead trigger model segmentation.

**Q8. How to create drift indicators?**  
Cumulative wafers since maintenance, exponentially weighted moving averages (EWMA) of key sensor deviations, rolling stability metrics (std/mean) for RF power, temperature uniformity ranges.

## Modeling & Validation

**Q9. What cross-validation for temporally ordered lots?**  
Use expanding window (walk-forward). Avoid random K-fold (leaks future patterns). Optionally block by tool to reduce dependence.

**Q10. How do I choose between Ridge, Lasso, Elastic Net?**  

- Ridge: multicollinearity, keep all features.  
- Lasso: aggressive sparsity, risk of dropping correlated clusters.  
- Elastic Net: grouped sensor clusters, balances selection and stability.

(Choose based on correlation structure + interpretability needs.)

**Q11. Why are coefficients unstable run-to-run?**  
Likely multicollinearity or insufficient regularization. Diagnose with VIF, condition number; apply feature clustering or Elastic Net.

**Q12. How do I prevent target leakage?**  
Audit feature generation: remove post-process QC metrics, future timestamps, aggregated yield metrics computed after lot completion.

## Interpretability

**Q13. How to explain model to a process engineer?**  
Use: standardized coefficients (linear), permutation importance (trees), partial dependence for top 5 features, and real wafer examples: “A +5°C drift here increases predicted defectivity by X%.”

**Q14. Are SHAP values overkill?**  
Use only when localized explanations matter (per-lot root cause). For daily monitoring, aggregated permutation importance and stability charts may suffice.

**Q15. What if two features appear equally important?**  
They may be proxies (pressure vs chamber pressure average). Consider combining into engineered stability metric or selecting one based on sensor reliability.

## Production & Monitoring

**Q16. What metrics trigger retraining?**  
Rolling 30-lot RMSE > baseline + 2σ, feature distribution population stability index (PSI) > 0.2, new tool introduction, or sustained drift in top-importance sensor.

**Q17. How to monitor feature drift efficiently?**  
Daily KS test or Jensen–Shannon divergence for top 20 features; alert if two consecutive exceed thresholds.

**Q18. How to version models?**  
Tag with: model family, data window, feature hash, hyperparameters, code commit SHA, training timestamp.

**Q19. Handling real-time latency constraints?**  
Move heavy transformations (PCA, scaling) into a serialized pipeline artifact; pre-compute static recipe encodings; ensure inference < process decision threshold (e.g., <300 ms).

**Q20. How to fail safe?**  
Implement prediction sanity bounds; on breach, fallback to last stable model or rule-based heuristic; log event for investigation.

## Analogies

- Multicollinearity: like having multiple thermometers taped together—more readings but not more information.
- Regularization: turning a noisy radio dial toward a clearer signal by damping extreme static spikes.
- Drift: slow tool wear is like tire tread wearing down—performance degrades gradually unless rotated/replaced.
- Feature selection: cleaning a cluttered lab bench so critical instruments are accessible.
- Residual analysis: inspecting the leftover “scrap” after machining to judge process stability.

## Mermaid Diagrams

### End-to-End Regression Workflow

```mermaid
digraph G {
  node [shape=rectangle, style=rounded, fontsize=12];
  A[Raw Fab Data] -> B[Data Cleaning / Imputation];
  B -> C[Feature Engineering];
  C -> D[Dimensionality Reduction / Selection];
  D -> E[Model Training];
  E -> F[Validation & Diagnostics];
  F -> G[Model Packaging];
  G -> H[Deployment];
  H -> I[Monitoring & Drift Detection];
  I -> C[label="Retrain Trigger", style=dashed];
}
```

### Bias–Variance Tradeoff Intuition

```mermaid
graph LR
  L[Underfit\nHigh Bias] -- Increase Complexity --> M[Balanced]\n
  M -- Too Complex --> H[Overfit\nHigh Variance]
  subgraph Signals
    L1[Simple Linear]:::low
    M1[Ridge / Elastic Net]:::mid
    H1[High-degree Polynomial]:::high
  end
  classDef low fill:#d0e6ff,stroke:#1b6aa5;
  classDef mid fill:#d8f9d3,stroke:#2d7d2d;
  classDef high fill:#ffe1cc,stroke:#c24e00;
```

## Troubleshooting Matrix

| Symptom | Likely Cause | Diagnostic | Action |
|---------|--------------|-----------|--------|
| R² high, predictions useless in production | Leakage | Time-split validation fails | Remove future-derived features |
| Coefficients flip sign between retrains | Multicollinearity | VIF > 10 / κ > 30 | Cluster & drop, use Elastic Net |
| Training fast, inference slow | Unoptimized pipeline | Profiling shows transform bottleneck | Pre-compute, cache encoders |
| Good CV, poor live performance | Distribution shift | PSI > 0.2 on key sensors | Retrain with recent data |
| Feature importance unstable | Correlated proxies | Jackknife resampling variance high | Combine or remove redundant sensors |
| Residuals fan-shaped | Heteroscedasticity | Residual vs fitted pattern | Weighted regression or transform |
| Too many selected sensors | Overfitting risk | Validation gap widens | Stronger regularization / feature pruning |
| Drift alerts noisy | Thresholds too tight | Frequent single-feature spikes | Use rolling aggregation & hysteresis |

## References & Further Reading

- Montgomery, Peck & Vining. Introduction to Linear Regression Analysis.
- SEMI Standards for equipment data collection.
- scikit-learn documentation (model evaluation & linear models).
- Domain papers on lithography process window modeling.

---

**Back to Fundamentals**: [3.1 Regression Fundamentals](3.1-regression-fundamentals.md)
