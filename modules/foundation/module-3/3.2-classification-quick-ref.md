# 3.2 Classification Quick Reference (Semiconductor Manufacturing)

## Core Model Families

- Logistic Regression: linear decision boundary; calibrated (with regularization) probabilities.
- Linear SVM: maximum margin; use decision_function + threshold; needs scaling.
- Tree / Random Forest: non-linear interactions, minimal preprocessing; check permutation importance.
- Gradient Boosting (HistGradientBoostingClassifier): strong accuracy, handles missing values, tune learning_rate & depth.
- Ensemble Voting/Stacking: combine complementary biases (linear + tree) for marginal gains.

## Typical Semiconductor Classification Targets

- Excursion / Rare Event Flag (sensor anomaly, process drift)
- Wafer Map Pattern Category (edge ring, center, scratch) – multi-class extension
- Early Bin / Final Test Pass Prediction
- Tool State (normal, drift, fault)

## Imbalance Handling Strategy Ladder

| Step | Technique | When | Key Caution |
|------|-----------|------|-------------|
| 1 | Class weights (`class_weight='balanced'`) | Mild imbalance | Still tune threshold |
| 2 | SMOTE (inside CV Pipeline) | Continuous features, need recall lift | Risk of boundary noise |
| 3 | BorderlineSMOTE / ADASYN | Hard boundary minority | Can amplify mislabeled noise |
| 4 | Ensemble (Balanced RF / EasyEnsemble) | Severe (<0.5%) | Complexity & compute |
| 5 | Cost-sensitive thresholding | Clear FP vs FN cost ratio | Needs reliable probabilities |

Always apply sampling inside training folds only (Pipeline) to prevent leakage.

## Key Metrics (Rare Event Focus)

| Metric | Purpose | Watch Out |
|--------|---------|-----------|
| PR AUC | Ranking performance on positive class | Higher variance; compare to prevalence |
| ROC AUC | Overall separability | Can look good under extreme imbalance |
| Recall (Sensitivity) | Catch excursions | May inflate false positives |
| Precision | Limit false alarms | Ignores missed events |
| F1 / Fβ | Balance P/R (or cost-weight) | β > 1 emphasises recall |
| MCC | Balanced correlation summary | Harder to explain to ops |
| Balanced Accuracy | Class-normalized accuracy | Threshold-dependent |
| Brier Score | Calibration quality | Impacted by prevalence |

Baseline PR AUC ≈ positive prevalence; beat it meaningfully.

## Threshold Tuning

Objective form (maximize business utility):

```text
U(t) = (Benefit_TP * TP(t) - Cost_FP * FP(t)) / N
```

Feasible threshold search via precision-recall curve; pick highest F1 (or utility) under constraints:

precision >= P_min, recall >= R_min

## Minimal Production Pipeline (With Optional SMOTE)

```python
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipe = Pipeline([
  ('impute', SimpleImputer(strategy='median')),
  ('scale', StandardScaler()),
  ('smote', SMOTE(random_state=42)),  # remove if using class_weight only
  ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', C=1.0, random_state=42))
])
pipe.fit(X_train, y_train)
probs = pipe.predict_proba(X_val)[:,1]
```

## Interpretation Cheat Sheet

- Logistic Coefficients (after scaling) ⇒ directional influence.
- Permutation Importance > impurity importance to avoid bias.
- SHAP (optional) for tree/boosting local + global explanations.
- Partial Dependence / ALE to validate expected monotonic trends.

## Common Pitfalls & Remedies

| Pitfall | Symptom | Remedy |
|---------|---------|--------|
| SMOTE before split | Inflated validation metrics | Oversample only inside CV folds |
| Using accuracy alone | 99% accuracy with 1% minority | Report PR AUC, recall@precision |
| Default 0.5 threshold | Suboptimal business outcome | Optimize via PR curve / cost function |
| Ignoring cost asymmetry | Missed excursions costly | Use weighted utility or recall constraint |
| Misinterpreting impurity importance | High-cardinality bias | Use permutation importance |
| Probability drift | Calibration degradation | Periodic Brier/log loss monitoring |

## Quick Workflow

1. Define business costs (FP vs FN) + operating precision/recall target.
2. Baseline logistic (class weights) → evaluate ROC AUC & PR AUC.
3. Threshold tune on validation set (precision ≥ target, maximize recall or utility).
4. If recall insufficient → add SMOTE variant (inside CV) or move to ensemble.
5. Evaluate calibration; apply Platt / isotonic if needed.
6. Interpret (permutation importance, SHAP optional).
7. Persist pipeline + threshold + metrics + feature list.
8. Monitor drift (prevalence, calibration, recall@precision) and retrain triggers.

## Persistence Metadata (Minimal Fields)

```jsonc
{
  "trained_at": "2025-09-03T12:34:56Z",
  "model_type": "LogisticRegression",
  "sampler": "SMOTE",
  "threshold": 0.37,
  "metrics": {"roc_auc": 0.94, "pr_auc": 0.61, "precision": 0.91, "recall": 0.54},
  "feature_list": ["temperature", "pressure", "flow", ...],
  "data_hash": "sha256:...",
  "code_commit": "<git sha>"
}
```

## Escalation Triggers

| Trigger | Escalate To |
|---------|-------------|
| Precision plateau below target | Tune threshold / SMOTE / ensemble |
| High variance in PR AUC across folds | Increase data / simpler model |
| Calibration drift (Brier ↑ 30%) | Recalibrate or retrain |
| Feature drift (PSI > 0.2 key feature) | Investigate process change |
| Minority prevalence shift | Recompute threshold / retrain |

---
**Guiding Principle:** Start with the simplest calibrated solution that meets operational precision/recall; escalate only when justified by objective metrics or drift.
