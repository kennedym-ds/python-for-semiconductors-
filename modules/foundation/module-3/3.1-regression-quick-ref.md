# 3.1 Regression Quick Reference (Semiconductor Manufacturing)

## Core Models

- Ordinary Least Squares (OLS): minimize RSS = ∑(y - Xβ)^2; solution β = (XᵀX)^{-1}Xᵀy
- Ridge: minimize ∑(y - Xβ)^2 + α‖β‖₂² (L2 shrinkage)
- Lasso: minimize ∑(y - Xβ)^2 + α‖β‖₁ (sparsity)
- Elastic Net: ∑(y - Xβ)^2 + α[(1-λ)/2 ‖β‖₂² + λ‖β‖₁]
- Polynomial Regression: feature expansion (interactions / powers) then linear model
- Robust: Huber (quadratic near 0, linear for large residuals), RANSAC (inlier consensus)
- Ensemble: Random Forest / Bagging for non-linear interactions

## Typical Semiconductor Predictors

- Lithography: dose, focus, temperature, pressure, CD uniformity indicators
- CVD / Deposition: temperature, pressure, gas flows, time, power, chamber ID
- Implant / Anneal: dose, energy, tilt, anneal temp/time, ambient
- Metrology Derived: mean, range, variance, drift index, interaction terms

## Manufacturing-Specific Metrics

- MAE: mean absolute error (process shift magnitude)
- RMSE: penalizes larger excursions (out-of-control risk)
- R²: variance explained (stability of relationship)
- Prediction Within Spec (PWS): fraction of predictions within [LSL, USL]
- Estimated Loss: ∑ max(0, |y - ŷ| - tolerance) * cost_per_unit

## Diagnostic Checklist

| Aspect | Tool | Red Flag | Mitigation |
|--------|------|----------|------------|
| Linearity | Residuals vs fitted | Curvature | Add interactions / polynomial terms |
| Normality | Q-Q plot | Heavy tails | Robust model / transform target |
| Homoscedasticity | Residual vs fitted spread | Funnel shape | Weighted regression / transform |
| Multicollinearity | VIF > 10 | Unstable coeffs | Drop / combine / regularize |
| Influential Points | Cook's D > 4/n | Leverage distortions | Investigate root cause / robust fit |
| Autocorrelation | Durbin-Watson < 1.5 | Serial dependence | Include lag features / time series CV |

## Feature Engineering Patterns

- Center critical continuous variables (improves interpretability)
- Interaction: (temp×flow), (dose×focus)
- Non-linear: squared (pressure²), log transforms (dose, time), polynomial expansion
- Aggregates: mean per lot, rolling mean, range, standard deviation
- Drift indices: cumulative average, EWMA metrics

## Regularization Selection

| Situation | Strategy |
|-----------|----------|
| Many correlated predictors | Ridge |
| Need sparse, interpretable model | Lasso |
| Mixed sparsity + grouping | Elastic Net |
| Severe multicollinearity + prediction focus | Ridge / Elastic Net |

## Quick Workflow

1. Define objective + target (yield %, CD, Vth)
2. Consolidate datasets; align keys (lot, wafer, time)
3. Exploratory plots & correlation heatmap
4. Baseline linear model (standardized)
5. Feature engineering (interactions + domain signals)
6. Model family comparison (OLS, Ridge, Lasso, RF)
7. Hyperparameter tuning (CV, nested if required)
8. Diagnostics (residuals, VIF, Cook's, DW)
9. Robust / ensemble fallback if violations persist
10. Package pipeline (impute → scale → select → PCA → model)
11. Persist model + metadata (version, train timestamp)
12. Monitor drift (input stats, error distribution) post-deployment

## Common Pitfalls & Remedies

| Pitfall | Symptom | Remedy |
|---------|---------|--------|
| Data Leakage | Unrealistically high CV scores | Temporal splits; no future stats leakage |
| Unscaled Regularization | Poor convergence / mis-weighted penalties | Standardize predictors |
| Over-Polynomialization | High variance, poor generalization | CV degree selection + regularization |
| Ignoring Tool Effects | Unexplained variance | Encode tool IDs / random effects |
| Measurement Outliers | Skewed coefficients | Robust regression / filtering rules |
| Misaligned Time Stamps | Negative lag effects | Align sources, consistent windows |

## Interpretation Tips

- Use standardized coefficients (post-scaling) for relative importance
- Check confidence intervals: interval crossing 0 ⇒ weak evidence
- Compare linear vs regularized coefficient shrinkage to detect noise predictors
- Use partial dependence or conditional plots for non-linear surfaces
- Random Forest feature importance: supplement, not replace statistical inference

## Production Pipeline Components

- Imputer (median) → handles sensor dropout
- StandardScaler → normalizes scale for regularization
- SelectKBest(f_regression) → removes weak predictors early
- PCA (retain 95% variance) → compress high-dimensional sensors (e.g., SECOM)
- Estimator (Ridge default) → balanced bias/variance with multicollinearity robustness
- Metadata: trained_at, model_type, n_components, params, metrics

## Deployment Validation

| Phase | Check |
|-------|-------|
| Pre-Deploy | Train/test metrics stable; diagnostics acceptable |
| Serialization | Model loads, reproduces predictions bitwise (seed fixed) |
| Rollout | Shadow mode compare vs incumbent model |
| Monitoring | Track MAE, PWS, input distribution drift (KL, PSI) |
| Retraining Trigger | MAE ↑ > 20% baseline or PWS ↓ below SLA |

## Formulas Snapshot

- RSS = ∑ (yᵢ - ŷᵢ)²
- MAE = (1/n)∑|yᵢ - ŷᵢ|
- RMSE = sqrt((1/n)∑(yᵢ - ŷᵢ)²)
- R² = 1 - RSS/TSS
- Ridge Objective: RSS + α∑βⱼ²
- Lasso Objective: RSS + α∑|βⱼ|
- Elastic Net: RSS + α[(1-λ)/2 ∑βⱼ² + λ∑|βⱼ|]
 
## Minimal Code Pattern

```python
pipeline = Pipeline([
  ('impute', SimpleImputer(strategy='median')),
  ('scale', StandardScaler()),
  ('select', SelectKBest(f_regression, k=30)),
  ('pca', PCA(n_components=0.95, random_state=42)),
  ('model', Ridge(alpha=1.0, random_state=42))
])
```

## When to Escalate Beyond Linear

| Trigger | Consider |
|---------|----------|
| Strong interaction curvature | Polynomial / RF / Gradient Boosting |
| Heavy-tailed residuals | Huber / Quantile Regression |
| High-dimensional sensor array | PCA / Autoencoder embeddings |
| Real-time drift | Online learning (SGDRegressor) |

---
**Remember:** Simplicity first. Only escalate complexity after diagnostics justify it.
