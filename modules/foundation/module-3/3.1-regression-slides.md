---
post_title: "Module 3.1 Regression Slide Deck"
author1: "Your Name"
post_slug: "module-3-1-regression-slides"
microsoft_alias: "alias"
featured_image: ""
categories: ["regression","semiconductors"]
tags: ["slides","training","manufacturing","regression"]
ai_note: "Partial AI assistance"
summary: "Instructor-friendly slide deck (markdown) with speaker notes for teaching regression analysis in semiconductor manufacturing contexts."
post_date: "2025-09-03"
---

# Regression Analysis Fundamentals (Slides)

Module 3.1 • Predictive Modeling Foundations

> Speaker Notes: Set context—link to prior statistics module and preview transition to classification & ensembles. Emphasize why quantitative modeling is now actionable with collected sensor data.

---

## Slide 2: Learning Objectives

1. Frame regression problems in fab operations  
2. Engineer physics-aware features  
3. Select & validate linear vs regularized models  
4. Interpret coefficients & residuals for action  
5. Prepare models for monitored deployment

> Speaker Notes: Encourage learners to map each objective to an active project (yield, CD control, drift early detection).

---

## Slide 3: Why Regression in the Fab?

| Business Driver | Example | Value |
|-----------------|---------|-------|
| Yield Uplift | Predict param combos | Fewer excursions |
| Tool Stability | Drift early warning | Reduced scrap |
| Recipe Optimization | Dose/Focus tuning | Throughput + quality |
| Cost Reduction | Fewer experiments | Lower cycle time |

> Speaker Notes: Reinforce that regression is not only prediction—it's structured process knowledge extraction.

---

## Slide 4: Core Equation (Simple)

```text
Y = β₀ + β₁X + ε
```

Interpretation: incremental change, baseline, noise.

> Speaker Notes: Use a concrete example (Vth vs implant dose). Ask what β₁ physically encodes.

---

## Slide 5: Scaling Up (Multiple)

```text
Y = β₀ + Σ βⱼXⱼ + ε
```

Challenges: multicollinearity, overfitting, interpretability.

> Speaker Notes: Show sensor correlation heatmap in live demo if time.

---

## Slide 6: Assumption Checklist

Linearity • Independence • Homoscedasticity • Normal Errors • Low Multicollinearity

> Speaker Notes: Clarify which matter for prediction vs inference. Emphasize independence/time ordering often violated.

---

## Slide 7: Semiconductor Feature Engineering

- Arrhenius temperature transforms  
- Interaction (Dose×Focus, Power×Pressure)  
- Drift counters & EWMA stability metrics  
- Process window distances

> Speaker Notes: Tie each to underlying physics (reaction rates, plasma dynamics, optical focus). Avoid blind polynomial explosion.

---

## Slide 8: Drift & Time Awareness

Chamber_Age, Cumulative_Wafers, Rolling Std, Tool Switch Flags.

> Speaker Notes: Warn about leakage when using future windows; only past context allowed.

---

## Slide 9: Regularization Map

| Method | Penalty | Useful For |
|--------|---------|------------|
| Ridge | L2 | Multicollinearity, stability |
| Lasso | L1 | Sparse selection |
| Elastic Net | L1+L2 | Correlated sensor clusters |

> Speaker Notes: Provide intuition: Ridge shrinks, Lasso zeroes, Elastic Net balances group retention.

---

## Slide 10: Bias–Variance Intuition

```mermaid
graph LR
  A[High Bias\nUnderfit] --> B[Optimal Balance]
  B --> C[High Variance\nOverfit]
```

> Speaker Notes: Relate underfit to ignoring known physics; overfit to memorizing noise due to micro-vibrations or transient sensor spikes.

---

## Slide 11: Validation Strategy

Use time-based expanding window + tool stratification.  
Report: MAE, RMSE, R², PWS (within spec %)  
Log distribution drift metrics.

> Speaker Notes: Show contrast of naive random CV leakage vs proper temporal split.

---

## Slide 12: Residual Diagnostics

Plot residuals vs fitted, Q-Q, time order, leverage/Cook's.  
Flag heteroscedasticity & autocorrelation.

> Speaker Notes: Provide threshold heuristics (Cook's > 4/n, standardized residual > 3σ).

---

## Slide 13: Feature Importance Choices

Linear: standardized coefficients  
Trees: impurity / permutation  
Fallback: permutation on sampled training subset

> Speaker Notes: Stress reproducibility—log random seeds & feature set hash.

---

## Slide 14: Interpreting Coefficients

"A +1 sccm MFC drift increases predicted defectivity by 0.4% (holding others)."  
Convert math to process actionable statements.

> Speaker Notes: Encourage unit rescales for readability.

---

## Slide 15: Handling Missingness

>50% missing: drop or indicator+impute  
Structured patterns: model missingness explicitly  
Tool boundary: segregate imputations.

> Speaker Notes: Show missingness heatmap if possible.

---

## Slide 16: Dimensionality Workflow

```mermaid
digraph G {
  Raw[Raw 600 Sensors] -> Filter[Remove >50% Missing];
  Filter -> Corr[Cluster Correlated];
  Corr -> Select[SelectKBest / VIF];
  Select -> PCA[PCA (optional)];
  PCA -> Model[Regularized Model];
}
```

> Speaker Notes: Emphasize avoiding PCA before interpretability unless necessary for stability.

---

## Slide 17: Production Architecture

Raw Data → Validation → Feature Pipeline → Model → Prediction API → Monitoring

> Speaker Notes: Contrast batch vs near-real-time flows (latency budget <300 ms).

---

## Slide 18: Monitoring Signals

- Rolling RMSE  
- Feature PSI  
- Data latency  
- Prediction distribution shift

> Speaker Notes: Provide retrain trigger policy template.

---

## Slide 19: Failure Modes & Safeguards

| Failure | Safeguard |
|---------|-----------|
| Drift | PSI & alerting |
| Latency spike | Pre-compute heavy transforms |
| Sensor dropout | Fallback defaults + imputer |
| Out-of-range prediction | Clamp + anomaly log |

> Speaker Notes: Tie to quality system / CAPA processes.

---

## Slide 20: Case Study Snapshot (SECOM)

High dimensional (590) → filter, impute, select 50 → regularized regression → monitor drift.  
Outcome: interpretable drivers for potential yield proxy.

> Speaker Notes: Encourage learners to replicate baseline experiment.

---

## Slide 21: Action Checklist

1. Audit data leakage  
2. Engineer physics-informed features  
3. Regularize & validate temporally  
4. Package reproducibly  
5. Deploy with monitoring

> Speaker Notes: Use as end-of-session recap.

---

## Slide 22: Discussion Prompts

- Which sensor transformations add most insight?  
- When is PCA justifiable?  
- How to define a retrain trigger threshold?

> Speaker Notes: Solicit real fab pain points.

---

## Slide 23: Next Modules

Classification (3.2) • Ensembles (4.1) • Time Series (5.1)

> Speaker Notes: Show continuity—residual diagnostics carry forward.

---

## Slide 24: Resources

- Regression Fundamentals doc  
- FAQ & Troubleshooting  
- scikit-learn Guide  
- Lithography process window literature

> Speaker Notes: Provide internal wiki links if applicable.

---

## Slide 25: Thank You / Q&A

> Speaker Notes: Encourage posting lingering questions to knowledge base; preview office hours.

---

**Supplemental**: [FAQ](3.1-regression-faq.md) • [Fundamentals](3.1-regression-fundamentals.md)
