{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98f5c4f",
   "metadata": {},
   "source": [
    "# 3.1 Regression Analysis Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on implementation of regression techniques for semiconductor manufacturing process engineering. Follow the sections sequentially.\n",
    "\n",
    "Outline:\n",
    "1. Import Required Libraries\n",
    "2. Generate Synthetic Semiconductor Manufacturing Data\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Simple Linear Regression\n",
    "5. Multiple Linear Regression & Feature Engineering\n",
    "6. Polynomial Regression\n",
    "7. Regularization (Ridge, Lasso, Elastic Net)\n",
    "8. Model Validation & Cross-Validation\n",
    "9. Performance Metrics & Comparison\n",
    "10. Residual Analysis & Diagnostics\n",
    "11. Feature Importance & Interpretation\n",
    "12. SECOM Dataset Analysis\n",
    "13. Advanced Regression Techniques\n",
    "14. Production Implementation Pipeline\n",
    "\n",
    "> Note: Synthetic data replicates common semiconductor relationships (dose vs CD, temperature vs yield, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed3cf9",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import core scientific Python libraries for regression analysis and visualization.\n",
    "\n",
    "Key Libraries:\n",
    "- numpy / pandas: numerical & tabular processing\n",
    "- matplotlib / seaborn: visualization\n",
    "- scikit-learn: modeling & evaluation\n",
    "- scipy: statistical tests / distributions\n",
    "- statsmodels: detailed regression diagnostics (optional)\n",
    "\n",
    "Set a master random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98798ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "# Modeling & preprocessing\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, RANSACRegressor)\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score, TimeSeriesSplit, KFold)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "\n",
    "# Stats & diagnostics\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_DIR = Path('../../../../datasets')\n",
    "SECOM_DATA_PATH = DATA_DIR / 'secom.data'\n",
    "SECOM_LABELS_PATH = DATA_DIR / 'secom_labels.data'\n",
    "SECOM_NAMES_PATH = DATA_DIR / 'secom.names'\n",
    "\n",
    "print('Library versions:')\n",
    "print('numpy', np.__version__)\n",
    "print('pandas', pd.__version__)\n",
    "import sklearn; print('sklearn', sklearn.__version__)\n",
    "print('statsmodels', sm.__version__)\n",
    "\n",
    "# Helper to display section banners\n",
    "def section(title: str):\n",
    "    print(f\"\\n{'='*len(title)}\\n{title}\\n{'='*len(title)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ab677",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Semiconductor Manufacturing Data\n",
    "\n",
    "We generate multiple synthetic datasets modeling typical semiconductor process relationships:\n",
    "- Dose vs Critical Dimension (CD) with focus interaction\n",
    "- Temperature / Pressure / Gas Flow vs Yield\n",
    "- Implant Dose / Energy / Anneal parameters vs Threshold Voltage (Vth)\n",
    "\n",
    "Each dataset embeds controlled noise and known coefficients for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86903390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_photolithography_cd(n=500, seed=RANDOM_SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dose = rng.normal(loc=30, scale=3, size=n)\n",
    "    focus = rng.normal(loc=0, scale=0.15, size=n)\n",
    "    # True coefficients\n",
    "    b0, b_dose, b_focus = 100, -1.2, 4.0\n",
    "    b_dose2, b_focus2, b_inter = 0.02, -6.0, -0.5\n",
    "    noise = rng.normal(0, 2, size=n)\n",
    "    cd = (b0 + b_dose*dose + b_focus*focus + b_dose2*dose**2 +\n",
    "          b_focus2*focus**2 + b_inter*dose*focus + noise)\n",
    "    return pd.DataFrame({\n",
    "        'dose': dose,\n",
    "        'focus': focus,\n",
    "        'cd': cd\n",
    "    })\n",
    "\n",
    "def generate_yield_process(n=600, seed=RANDOM_SEED):\n",
    "    rng = np.random.default_rng(seed+1)\n",
    "    temp = rng.normal(450, 15, n)    # CVD temperature (°C)\n",
    "    pressure = rng.normal(2.5, 0.3, n)  # Torr\n",
    "    flow = rng.normal(120, 10, n)   # sccm\n",
    "    time = rng.normal(60, 5, n)     # minutes\n",
    "    # True relationship (non-linear + interaction)\n",
    "    noise = rng.normal(0, 3, n)\n",
    "    yield_pct = (70 + 0.05*(temp-450) - 1.5*(pressure-2.5)**2 + 0.04*flow +\n",
    "                 0.2*time + 0.0005*(temp-450)*(flow-120) + noise)\n",
    "    yield_pct = np.clip(yield_pct, 0, 100)\n",
    "    return pd.DataFrame({\n",
    "        'temperature': temp,\n",
    "        'pressure': pressure,\n",
    "        'flow': flow,\n",
    "        'time': time,\n",
    "        'yield_pct': yield_pct\n",
    "    })\n",
    "\n",
    "def generate_implant_vth(n=500, seed=RANDOM_SEED):\n",
    "    rng = np.random.default_rng(seed+2)\n",
    "    dose = rng.lognormal(mean=13, sigma=0.2, size=n)  # ions/cm^2 scale\n",
    "    energy = rng.normal(60, 5, size=n)  # keV\n",
    "    anneal_temp = rng.normal(1000, 20, size=n)  # °C\n",
    "    anneal_time = rng.normal(30, 3, size=n)  # seconds\n",
    "    # Arrhenius-like relationship simplified\n",
    "    k1, k2, k3, k4, k5 = 0.9, -0.002, -0.1, 0.03, -0.00002\n",
    "    noise = rng.normal(0, 0.02, size=n)\n",
    "    vth = (k1 + k2*(dose/1e13) + k3*np.log(energy) + k4*np.log(anneal_time) +\n",
    "           k5*(anneal_temp) + noise)\n",
    "    return pd.DataFrame({\n",
    "        'implant_dose': dose,\n",
    "        'implant_energy': energy,\n",
    "        'anneal_temp': anneal_temp,\n",
    "        'anneal_time': anneal_time,\n",
    "        'vth': vth\n",
    "    })\n",
    "\n",
    "cd_df = generate_photolithography_cd()\n",
    "yield_df = generate_yield_process()\n",
    "vth_df = generate_implant_vth()\n",
    "\n",
    "print(cd_df.head())\n",
    "print(yield_df.head())\n",
    "print(vth_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3e4eb",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis and Visualization\n",
    "\n",
    "We explore distributions, relationships, and correlations in the synthetic datasets to inform feature engineering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3964fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,4))\n",
    "axes[0].hist(cd_df['cd'], bins=30, color='steelblue', alpha=0.7)\n",
    "axes[0].set_title('CD Distribution')\n",
    "axes[1].hist(yield_df['yield_pct'], bins=30, color='darkgreen', alpha=0.7)\n",
    "axes[1].set_title('Yield % Distribution')\n",
    "axes[2].hist(vth_df['vth'], bins=30, color='purple', alpha=0.7)\n",
    "axes[2].set_title('Vth Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Scatter: dose vs cd with color by focus\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(cd_df['dose'], cd_df['cd'], c=cd_df['focus'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(label='focus')\n",
    "plt.xlabel('dose')\n",
    "plt.ylabel('cd')\n",
    "plt.title('Dose vs CD (color=focus)')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for yield subset\n",
    "sns.pairplot(yield_df.sample(200, random_state=RANDOM_SEED), diag_kind='hist')\n",
    "plt.suptitle('Yield Process Pairplot (Sample)', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for yield process\n",
    "plt.figure(figsize=(6,4))\n",
    "cor = yield_df.corr(numeric_only=True)\n",
    "sns.heatmap(cor, annot=True, cmap='vlag', fmt='.2f')\n",
    "plt.title('Yield Process Correlations')\n",
    "plt.show()\n",
    "\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae179cd",
   "metadata": {},
   "source": [
    "## 4. Simple Linear Regression Implementation\n",
    "We begin with a single predictor relationship (dose → cd) to derive Ordinary Least Squares (OLS) estimates manually and compare with scikit-learn's `LinearRegression`. Steps:\n",
    "1. Center variables (optional) for numerical stability\n",
    "2. Compute slope & intercept using closed-form equations\n",
    "3. Validate against library implementation\n",
    "4. Plot regression line & residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression: dose -> cd\n",
    "X = cd_df[['dose']].values\n",
    "y = cd_df['cd'].values\n",
    "\n",
    "# Manual OLS (with intercept)\n",
    "x = X.flatten()\n",
    "x_mean, y_mean = x.mean(), y.mean()\n",
    "S_xy = np.sum((x - x_mean)*(y - y_mean))\n",
    "S_xx = np.sum((x - x_mean)**2)\n",
    "b1 = S_xy / S_xx\n",
    "b0 = y_mean - b1 * x_mean\n",
    "print(f\"Manual OLS coefficients: intercept={b0:.4f}, slope={b1:.4f}\")\n",
    "\n",
    "# Sklearn implementation\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(f\"sklearn coefficients: intercept={lr.intercept_:.4f}, slope={lr.coef_[0]:.4f}\")\n",
    "\n",
    "# Predictions & residuals\n",
    "x_line = np.linspace(x.min(), x.max(), 200)\n",
    "y_line = b0 + b1 * x_line\n",
    "y_pred = lr.predict(X)\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Plot regression line\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(x, y, alpha=0.5, label='observed')\n",
    "plt.plot(x_line, y_line, color='red', label='manual OLS line')\n",
    "plt.xlabel('dose')\n",
    "plt.ylabel('cd')\n",
    "plt.title('Simple Linear Regression (Dose → CD)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted CD')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "print('MAE:', mean_absolute_error(y, y_pred))\n",
    "print('RMSE:', mean_squared_error(y, y_pred, squared=False))\n",
    "print('R2:', r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13455b8f",
   "metadata": {},
   "source": [
    "## 5. Multiple Linear Regression with Feature Engineering\n",
    "We extend to multiple predictors (temperature, pressure, flow, time) predicting yield. We'll:\n",
    "1. Create engineered features (interactions, non-linear terms)\n",
    "2. Standardize features\n",
    "3. Fit linear model & evaluate\n",
    "4. Compare with model sans engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base features\n",
    "y_base = yield_df['yield_pct'].values\n",
    "X_base = yield_df[['temperature','pressure','flow','time']].copy()\n",
    "\n",
    "# Feature Engineering\n",
    "yield_eng = X_base.copy()\n",
    "yield_eng['temp_centered'] = yield_eng['temperature'] - yield_eng['temperature'].mean()\n",
    "yield_eng['pressure_sq'] = yield_eng['pressure']**2\n",
    "yield_eng['flow_time_inter'] = yield_eng['flow'] * yield_eng['time']\n",
    "yield_eng['temp_flow_inter'] = yield_eng['temperature'] * yield_eng['flow']\n",
    "\n",
    "# Train/test split\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_base, y_base, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(yield_eng, y_base, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# Pipelines\n",
    "base_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "eng_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "base_pipe.fit(X_train_b, y_train_b)\n",
    "eng_pipe.fit(X_train_e, y_train_e)\n",
    "\n",
    "pred_b = base_pipe.predict(X_test_b)\n",
    "pred_e = eng_pipe.predict(X_test_e)\n",
    "\n",
    "metrics = lambda y,yh: { 'MAE': mean_absolute_error(y,yh), 'RMSE': mean_squared_error(y,yh,squared=False), 'R2': r2_score(y,yh)}\n",
    "print('Base Metrics:', metrics(y_test_b, pred_b))\n",
    "print('Engineered Metrics:', metrics(y_test_e, pred_e))\n",
    "\n",
    "improvement = r2_score(y_test_e, pred_e) - r2_score(y_test_b, pred_b)\n",
    "print(f\"R2 Improvement from feature engineering: {improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8248e",
   "metadata": {},
   "source": [
    "## 6. Polynomial Regression for Non-Linear Relationships\n",
    "We model non-linear behavior in CD vs (dose, focus) using polynomial feature expansion. We'll compare degrees and use cross-validation to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_poly_base = cd_df[['dose','focus']].values\n",
    "y_poly = cd_df['cd'].values\n",
    "\n",
    "results = []\n",
    "for deg in [1,2,3,4]:\n",
    "    pipe = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "        ('scale', StandardScaler()),\n",
    "        ('model', Ridge(alpha=1.0))\n",
    "    ])\n",
    "    scores = cross_val_score(pipe, X_poly_base, y_poly, cv=5, scoring='r2')\n",
    "    results.append({'degree': deg, 'mean_r2': scores.mean(), 'std_r2': scores.std()})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "best_deg = results_df.sort_values('mean_r2', ascending=False).iloc[0]['degree']\n",
    "print('Best degree:', best_deg)\n",
    "\n",
    "best_pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=int(best_deg), include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "best_pipe.fit(X_poly_base, y_poly)\n",
    "\n",
    "# Visualize fit on a grid for focus=0 slice\n",
    "x_grid = np.linspace(cd_df['dose'].min(), cd_df['dose'].max(), 200)\n",
    "focus_zero = np.zeros_like(x_grid)\n",
    "X_grid = np.vstack([x_grid, focus_zero]).T\n",
    "cd_pred = best_pipe.predict(X_grid)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(cd_df['dose'], cd_df['cd'], alpha=0.3, label='observed')\n",
    "plt.plot(x_grid, cd_pred, color='red', label=f'Polynomial deg={int(best_deg)} (focus=0)')\n",
    "plt.xlabel('dose')\n",
    "plt.ylabel('cd')\n",
    "plt.title('Polynomial Regression Fit (slice focus=0)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d9432",
   "metadata": {},
   "source": [
    "## 7. Regularization Techniques (Ridge, Lasso, Elastic Net)\n",
    "We mitigate multicollinearity and overfitting using regularization. We'll tune hyperparameters via grid search and compare performance.\n",
    "\n",
    "Approach:\n",
    "- Build pipeline with scaling\n",
    "- Parameter grids for each model\n",
    "- 5-fold cross-validation on engineered yield dataset\n",
    "- Compare R² / MAE / coefficient sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg = yield_eng.copy()\n",
    "y_reg = y_base\n",
    "\n",
    "scale_impute = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "X_reg_t = scale_impute.fit_transform(X_reg)\n",
    "\n",
    "ridge_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "lasso_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
    "elastic_grid = {'alpha': [0.01, 0.1, 1.0], 'l1_ratio': [0.2, 0.5, 0.8]}\n",
    "\n",
    "ridge_cv = GridSearchCV(Ridge(random_state=RANDOM_SEED), ridge_grid, scoring='r2', cv=5)\n",
    "lasso_cv = GridSearchCV(Lasso(random_state=RANDOM_SEED, max_iter=5000), lasso_grid, scoring='r2', cv=5)\n",
    "elastic_cv = GridSearchCV(ElasticNet(random_state=RANDOM_SEED, max_iter=5000), elastic_grid, scoring='r2', cv=5)\n",
    "\n",
    "ridge_cv.fit(X_reg_t, y_reg)\n",
    "lasso_cv.fit(X_reg_t, y_reg)\n",
    "elastic_cv.fit(X_reg_t, y_reg)\n",
    "\n",
    "print('Best Ridge:', ridge_cv.best_params_, 'R2:', ridge_cv.best_score_)\n",
    "print('Best Lasso:', lasso_cv.best_params_, 'R2:', lasso_cv.best_score_)\n",
    "print('Best Elastic:', elastic_cv.best_params_, 'R2:', elastic_cv.best_score_)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': X_reg.columns,\n",
    "    'ridge_coef': ridge_cv.best_estimator_.coef_,\n",
    "    'lasso_coef': lasso_cv.best_estimator_.coef_,\n",
    "    'elastic_coef': elastic_cv.best_estimator_.coef_\n",
    "})\n",
    "print(coef_df.head())\n",
    "print('Non-zero Lasso Coefficients:', (coef_df['lasso_coef']!=0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba370a",
   "metadata": {},
   "source": [
    "## 8. Model Validation and Cross-Validation\n",
    "We implement multiple validation strategies:\n",
    "- K-Fold (random shuffle)\n",
    "- TimeSeriesSplit (simulated chronological data)\n",
    "- Nested CV for unbiased hyperparameter performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold CV on polynomial best model from earlier (reusing best_pipe)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "poly_scores = cross_val_score(best_pipe, X_poly_base, y_poly, cv=kf, scoring='r2')\n",
    "print('K-Fold R2 mean:', poly_scores.mean(), 'std:', poly_scores.std())\n",
    "\n",
    "# TimeSeriesSplit simulation (assume yield_df sorted by time surrogate index)\n",
    "ts = TimeSeriesSplit(n_splits=5)\n",
    "base_ts_scores = []\n",
    "for train_idx, test_idx in ts.split(X_reg_t):\n",
    "    X_tr, X_te = X_reg_t[train_idx], X_reg_t[test_idx]\n",
    "    y_tr, y_te = y_reg[train_idx], y_reg[test_idx]\n",
    "    model = Ridge(alpha=ridge_cv.best_params_['alpha'])\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_te_pred = model.predict(X_te)\n",
    "    base_ts_scores.append(r2_score(y_te, y_te_pred))\n",
    "print('TimeSeriesSplit R2 scores:', base_ts_scores)\n",
    "print('TimeSeriesSplit mean R2:', np.mean(base_ts_scores))\n",
    "\n",
    "# Nested CV (simplified)\n",
    "outer = KFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "outer_scores = []\n",
    "for train_idx, test_idx in outer.split(X_reg_t):\n",
    "    X_tr, X_te = X_reg_t[train_idx], X_reg_t[test_idx]\n",
    "    y_tr, y_te = y_reg[train_idx], y_reg[test_idx]\n",
    "    inner = KFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "    inner_cv = GridSearchCV(Ridge(), {'alpha':[0.1,1,10]}, cv=inner, scoring='r2')\n",
    "    inner_cv.fit(X_tr, y_tr)\n",
    "    y_te_pred = inner_cv.best_estimator_.predict(X_te)\n",
    "    outer_scores.append(r2_score(y_te, y_te_pred))\n",
    "print('Nested CV R2 scores:', outer_scores, 'mean:', np.mean(outer_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52084a",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics and Model Comparison\n",
    "We compute standard regression metrics plus manufacturing-specific metrics:\n",
    "- MAE, RMSE, R²\n",
    "- Prediction Within Spec (PWS)\n",
    "- Estimated Yield Loss beyond tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models: Dict[str, Any], X, y, tolerance=2.0, spec_low=60, spec_high=100, cost_per_unit=1.0):\n",
    "    rows = []\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        within_spec = ((y_pred >= spec_low) & (y_pred <= spec_high)).mean()\n",
    "        loss_components = np.maximum(0, np.abs(y - y_pred) - tolerance)\n",
    "        est_loss = np.sum(loss_components) * cost_per_unit\n",
    "        rows.append({'model': name, 'MAE': mae, 'RMSE': rmse, 'R2': r2,\n",
    "                     'PWS': within_spec, 'Estimated_Loss': est_loss})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Prepare models using already fit objects\n",
    "models_to_compare = {\n",
    "    'Linear_Base': base_pipe,\n",
    "    'Linear_Eng': eng_pipe,\n",
    "    'Ridge': ridge_cv.best_estimator_,\n",
    "    'Lasso': lasso_cv.best_estimator_,\n",
    "    'ElasticNet': elastic_cv.best_estimator_\n",
    "}\n",
    "\n",
    "metrics_df = evaluate_models(models_to_compare, scale_impute.transform(X_reg), y_reg)\n",
    "metrics_df.sort_values('R2', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bdda79",
   "metadata": {},
   "source": [
    "## 10. Residual Analysis and Diagnostics\n",
    "We assess assumptions:\n",
    "- Residuals vs Fitted (linearity, homoscedasticity)\n",
    "- Q-Q Plot (normality)\n",
    "- Cook's Distance (influence)\n",
    "- Variance Inflation Factor (VIF) (multicollinearity)\n",
    "- Durbin-Watson (autocorrelation surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use engineered linear model residuals\n",
    "lin_model = eng_pipe.named_steps['model']\n",
    "X_eng_scaled = eng_pipe.named_steps['scale'].transform(eng_pipe.named_steps['impute'].transform(yield_eng))\n",
    "lin_pred = lin_model.predict(X_eng_scaled)\n",
    "lin_res = y_reg - lin_pred\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,8))\n",
    "axes[0,0].scatter(lin_pred, lin_res, alpha=0.5)\n",
    "axes[0,0].axhline(0, color='red', linestyle='--')\n",
    "axes[0,0].set_title('Residuals vs Fitted')\n",
    "axes[0,0].set_xlabel('Fitted')\n",
    "axes[0,0].set_ylabel('Residuals')\n",
    "\n",
    "stats.probplot(lin_res, dist=\"norm\", plot=axes[0,1])\n",
    "axes[0,1].set_title('Q-Q Plot')\n",
    "\n",
    "axes[1,0].hist(lin_res, bins=30, alpha=0.7)\n",
    "axes[1,0].set_title('Residual Distribution')\n",
    "\n",
    "axes[1,1].scatter(range(len(lin_res)), lin_res, alpha=0.4)\n",
    "axes[1,1].axhline(0, color='red', linestyle='--')\n",
    "axes[1,1].set_title('Residuals vs Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statsmodels for influence\n",
    "X_sm = sm.add_constant(X_eng_scaled)\n",
    "sm_model = sm.OLS(y_reg, X_sm).fit()\n",
    "influence = sm_model.get_influence()\n",
    "(c, p) = influence.cooks_distance\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.stem(np.arange(len(c)), c, markerfmt=\",\", use_line_collection=True)\n",
    "plt.title(\"Cook's Distance\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Cook')\n",
    "plt.show()\n",
    "\n",
    "# VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif_values = []\n",
    "for i in range(1, X_sm.shape[1]):  # skip constant\n",
    "    vif_values.append({'feature': yield_eng.columns[i-1], 'VIF': variance_inflation_factor(X_sm, i)})\n",
    "vif_df = pd.DataFrame(vif_values).sort_values('VIF', ascending=False)\n",
    "print(vif_df.head())\n",
    "\n",
    "# Durbin-Watson\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw = durbin_watson(lin_res)\n",
    "print('Durbin-Watson:', dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093bc515",
   "metadata": {},
   "source": [
    "## 11. Feature Importance and Coefficient Interpretation\n",
    "We interpret model coefficients using:\n",
    "- Standardized coefficients\n",
    "- Confidence intervals (statsmodels)\n",
    "- Statistical significance (t-stats / p-values)\n",
    "- Partial dependence style visualization (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde37e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized coefficients using ridge best (already scaled data used earlier)\n",
    "ridge_best = ridge_cv.best_estimator_\n",
    "std_coefs = ridge_best.coef_  # because scale_impute scaled features\n",
    "importance_df = pd.DataFrame({'feature': X_reg.columns, 'standardized_coef': std_coefs})\n",
    "importance_df = importance_df.reindex(importance_df.standardized_coef.abs().sort_values(ascending=False).index)\n",
    "print(importance_df.head())\n",
    "\n",
    "# Confidence intervals with statsmodels (refit OLS on scaled data)\n",
    "ols_model = sm.OLS(y_reg, sm.add_constant(X_reg_t)).fit()\n",
    "conf_int = ols_model.conf_int()\n",
    "summary_df = pd.DataFrame({\n",
    "    'feature': ['const'] + list(X_reg.columns),\n",
    "    'coef': ols_model.params,\n",
    "    'p_value': ols_model.pvalues,\n",
    "    'ci_lower': conf_int[0],\n",
    "    'ci_upper': conf_int[1]\n",
    "})\n",
    "print(summary_df.head())\n",
    "\n",
    "# Plot top absolute standardized coefficients\n",
    "plt.figure(figsize=(6,5))\n",
    "imp_top = importance_df.head(10)\n",
    "plt.barh(imp_top['feature'], imp_top['standardized_coef'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top Standardized Coefficients (Ridge)')\n",
    "plt.xlabel('Standardized Coefficient')\n",
    "plt.show()\n",
    "\n",
    "# Simple partial dependence approximation for temperature\n",
    "temp_idx = list(X_reg.columns).index('temperature')\n",
    "# Vary temperature over quantiles while holding others at mean\n",
    "quantiles = np.linspace(0.05,0.95,20)\n",
    "temps = np.quantile(X_reg['temperature'], quantiles)\n",
    "X_mean = X_reg_t.mean(axis=0)\n",
    "pd_preds = []\n",
    "for t in temps:\n",
    "    X_mod = X_mean.copy()\n",
    "    # replace scaled value for temp: find original scale then transform\n",
    "    temp_orig_mean = X_reg['temperature'].mean()\n",
    "    temp_orig_std = X_reg['temperature'].std()\n",
    "    X_mod[temp_idx] = (t - temp_orig_mean)/temp_orig_std\n",
    "    pd_preds.append(ridge_best.predict(X_mod.reshape(1,-1))[0])\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(temps, pd_preds, marker='o')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Predicted Yield')\n",
    "plt.title('Partial Dependence (Approx) Temperature vs Yield')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a2a7e",
   "metadata": {},
   "source": [
    "## 12. SECOM Dataset Analysis\n",
    "We load the real SECOM dataset (high-dimensional, missing values). Steps:\n",
    "1. Load data and labels\n",
    "2. Handle missing values (remove >50% missing, impute remaining)\n",
    "3. Scale & dimensionality reduction (PCA)\n",
    "4. Fit baseline and regularized models\n",
    "5. Interpret component contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SECOM dataset (unsupervised regression style using continuous proxy target)\n",
    "# The original target is binary; we will create a synthetic continuous quality metric\n",
    "# by smoothing label sequence to emulate yield-like continuous response.\n",
    "\n",
    "if SECOM_DATA_PATH.exists():\n",
    "    secom_df = pd.read_csv(SECOM_DATA_PATH, sep=' ', header=None)\n",
    "    labels_df = pd.read_csv(SECOM_LABELS_PATH, sep=' ', header=None, usecols=[0])\n",
    "    labels = labels_df[0].values\n",
    "    # Create smoothed continuous target via rolling mean of binary labels (window=25)\n",
    "    quality_metric = pd.Series(labels).rolling(window=25, min_periods=1, center=True).mean().values\n",
    "\n",
    "    # Missingness analysis\n",
    "    missing_pct = secom_df.isna().mean()*100\n",
    "    keep_cols = missing_pct[missing_pct < 50].index\n",
    "    secom_reduced = secom_df[keep_cols]\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_secom = imputer.fit_transform(secom_reduced)\n",
    "    scaler = StandardScaler()\n",
    "    X_secom_scaled = scaler.fit_transform(X_secom)\n",
    "\n",
    "    # PCA retain 95% variance\n",
    "    pca = PCA(n_components=0.95, random_state=RANDOM_SEED)\n",
    "    X_secom_pca = pca.fit_transform(X_secom_scaled)\n",
    "    print('Original shape:', secom_df.shape, 'Reduced shape:', X_secom_pca.shape)\n",
    "\n",
    "    # Train/test split (chronological surrogate)\n",
    "    n = X_secom_pca.shape[0]\n",
    "    split = int(n*0.8)\n",
    "    X_train_s, X_test_s = X_secom_pca[:split], X_secom_pca[split:]\n",
    "    y_train_s, y_test_s = quality_metric[:split], quality_metric[split:]\n",
    "\n",
    "    base_lin = LinearRegression().fit(X_train_s, y_train_s)\n",
    "    ridge_s = Ridge(alpha=1.0).fit(X_train_s, y_train_s)\n",
    "\n",
    "    preds = {\n",
    "        'Linear': base_lin.predict(X_test_s),\n",
    "        'Ridge': ridge_s.predict(X_test_s)\n",
    "    }\n",
    "\n",
    "    for name, pr in preds.items():\n",
    "        print(name, 'MAE', mean_absolute_error(y_test_s, pr), 'RMSE', mean_squared_error(y_test_s, pr, squared=False), 'R2', r2_score(y_test_s, pr))\n",
    "\n",
    "    # Explained variance per component\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(np.cumsum(evr)*100)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Variance (%)')\n",
    "    plt.title('PCA Cumulative Variance Retained')\n",
    "    plt.axhline(95, color='red', linestyle='--')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('SECOM dataset files not found; skip Section 12 execution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c938696",
   "metadata": {},
   "source": [
    "## 13. Advanced Regression Techniques\n",
    "We explore robust and ensemble methods:\n",
    "- HuberRegressor (robust to outliers)\n",
    "- RANSACRegressor (inlier consensus)\n",
    "- Bagging + Random Forest (non-linear, feature importance)\n",
    "- Weighted regression (simulate heteroscedastic errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad630816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust methods on CD dataset (inject some outliers)\n",
    "cd_robust = cd_df.copy()\n",
    "outlier_idx = np.random.choice(cd_robust.index, size=15, replace=False)\n",
    "cd_robust.loc[outlier_idx, 'cd'] += np.random.normal(40,5,size=15)\n",
    "X_cd = cd_robust[['dose','focus']].values\n",
    "y_cd = cd_robust['cd'].values\n",
    "\n",
    "huber = HuberRegressor().fit(X_cd, y_cd)\n",
    "ransac = RANSACRegressor(LinearRegression(), random_state=RANDOM_SEED).fit(X_cd, y_cd)\n",
    "base_lin_cd = LinearRegression().fit(X_cd, y_cd)\n",
    "\n",
    "for name, model in [('Linear', base_lin_cd), ('Huber', huber), ('RANSAC', ransac)]:\n",
    "    pred = model.predict(X_cd)\n",
    "    print(name, 'RMSE', mean_squared_error(y_cd, pred, squared=False))\n",
    "\n",
    "# Ensemble on yield dataset\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=RANDOM_SEED)\n",
    "rf.fit(X_reg_t, y_reg)\n",
    "bag = BaggingRegressor(base_estimator=LinearRegression(), n_estimators=50, random_state=RANDOM_SEED)\n",
    "bag.fit(X_reg_t, y_reg)\n",
    "for name, model in [('RF', rf), ('BaggingLin', bag)]:\n",
    "    pred = model.predict(X_reg_t)\n",
    "    print(name, 'R2', r2_score(y_reg, pred))\n",
    "\n",
    "# Weighted regression (simulate higher variance at high flow)\n",
    "measurement_uncertainty = 1 + 0.01*(yield_eng['flow'] - yield_eng['flow'].mean())**2\n",
    "weights = 1 / measurement_uncertainty\n",
    "w_model = LinearRegression()\n",
    "w_model.fit(X_reg_t, y_reg, sample_weight=weights)\n",
    "weighted_pred = w_model.predict(X_reg_t)\n",
    "print('Weighted Linear R2', r2_score(y_reg, weighted_pred))\n",
    "\n",
    "# Feature importance from RF\n",
    "rf_imp = pd.Series(rf.feature_importances_, index=X_reg.columns).sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(6,4))\n",
    "rf_imp.plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Random Forest Top Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b57dd",
   "metadata": {},
   "source": [
    "## 14. Production Implementation Pipeline\n",
    "We assemble a reusable pipeline function for preprocessing + modeling + evaluation, suitable for deployment adaptation. Includes:\n",
    "- Data validation\n",
    "- Preprocessing (impute, scale, select, reduce)\n",
    "- Model training & persistence\n",
    "- Monitoring hooks (placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e56791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "class RegressionProductionPipeline:\n",
    "    def __init__(self, model=None, n_components=0.95, k_best=30):\n",
    "        self.model = model or Ridge(alpha=1.0, random_state=RANDOM_SEED)\n",
    "        self.n_components = n_components\n",
    "        self.k_best = k_best\n",
    "        self.pipeline = None\n",
    "        self.metadata = {}\n",
    "\n",
    "    def build(self):\n",
    "        self.pipeline = Pipeline([\n",
    "            ('impute', SimpleImputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('select', SelectKBest(score_func=f_regression, k=min(self.k_best, X_reg.shape[1]))),\n",
    "            ('reduce', PCA(n_components=self.n_components, random_state=RANDOM_SEED)),\n",
    "            ('model', self.model)\n",
    "        ])\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.pipeline is None:\n",
    "            self.build()\n",
    "        self.pipeline.fit(X, y)\n",
    "        self.metadata = {\n",
    "            'trained_at': datetime.utcnow().isoformat(),\n",
    "            'model_type': type(self.model).__name__,\n",
    "            'n_features_in': X.shape[1],\n",
    "            'n_components': self.pipeline.named_steps['reduce'].n_components_,\n",
    "            'k_best': self.pipeline.named_steps['select'].k\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipeline.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y, preds),\n",
    "            'RMSE': mean_squared_error(y, preds, squared=False),\n",
    "            'R2': r2_score(y, preds)\n",
    "        }\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        joblib.dump({'pipeline': self.pipeline, 'metadata': self.metadata}, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: Path):\n",
    "        obj = joblib.load(path)\n",
    "        inst = RegressionProductionPipeline()\n",
    "        inst.pipeline = obj['pipeline']\n",
    "        inst.metadata = obj['metadata']\n",
    "        return inst\n",
    "\n",
    "# Demonstrate on engineered yield dataset\n",
    "prod = RegressionProductionPipeline(Ridge(alpha=ridge_cv.best_params_['alpha']))\n",
    "prod.fit(X_reg_t, y_reg)\n",
    "metrics_prod = prod.evaluate(X_reg_t, y_reg)\n",
    "print('Production Pipeline Metrics:', metrics_prod)\n",
    "print('Metadata:', prod.metadata)\n",
    "\n",
    "MODEL_PATH = Path('regression_yield_pipeline.joblib')\n",
    "prod.save(MODEL_PATH)\n",
    "print('Saved model to', MODEL_PATH)\n",
    "\n",
    "loaded = RegressionProductionPipeline.load(MODEL_PATH)\n",
    "print('Loaded metadata:', loaded.metadata)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
