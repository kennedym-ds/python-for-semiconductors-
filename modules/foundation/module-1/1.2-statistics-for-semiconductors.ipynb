{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bdfb5a4d",
      "metadata": {},
      "source": [
        "# Module 1.2: Statistical Foundations for Semiconductor Analysis\n",
        "\n",
        "## Interactive Learning Notebook\n",
        "\n",
        "Welcome to the hands-on statistical analysis module! This notebook will guide you through essential statistical concepts specifically applied to semiconductor manufacturing and testing.\n",
        "\n",
        "### What You'll Learn\n",
        "- Descriptive statistics with real semiconductor process data\n",
        "- Process capability analysis (Cp, Cpk calculations)\n",
        "- Distribution analysis and yield prediction\n",
        "- Hypothesis testing for process validation\n",
        "- Control chart implementation for process monitoring\n",
        "\n",
        "### Prerequisites\n",
        "- Python fundamentals from Module 1.1\n",
        "- Basic understanding of statistics\n",
        "- NumPy and Pandas familiarity\n",
        "\n",
        "Let's begin our statistical journey in semiconductor analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbeedd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Essential Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, t, chi2, shapiro, anderson\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set styling for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 4)\n",
        "\n",
        "print(\"\u2705 All libraries imported successfully!\")\n",
        "print(\"\ud83d\udcca Ready for statistical analysis of semiconductor data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc7ba51",
      "metadata": {},
      "source": [
        "## 1. Generating Realistic Semiconductor Data\n",
        "\n",
        "Before diving into statistics, let's create realistic semiconductor process data that mimics real-world manufacturing scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d62a88a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_semiconductor_data():\n",
        "    \"\"\"Generate realistic semiconductor process data\"\"\"\n",
        "    n_wafers = 100\n",
        "    n_die_per_wafer = 200\n",
        "    \n",
        "    # Generate wafer-level data\n",
        "    wafer_data = {\n",
        "        'wafer_id': range(1, n_wafers + 1),\n",
        "        'lot_id': np.repeat(['LOT_' + str(i) for i in range(1, 21)], 5),\n",
        "        'process_tool': np.random.choice(['TOOL_A', 'TOOL_B', 'TOOL_C'], n_wafers),\n",
        "        'operator': np.random.choice(['OP1', 'OP2', 'OP3', 'OP4'], n_wafers),\n",
        "        'timestamp': pd.date_range('2024-01-01', periods=n_wafers, freq='6H')\n",
        "    }\n",
        "    \n",
        "    # Generate electrical parameters with realistic distributions\n",
        "    # Threshold voltage (mV) - normally distributed\n",
        "    vth_mean = 650  # Target 650mV\n",
        "    vth_std = 25    # Process variation\n",
        "    vth_data = np.random.normal(vth_mean, vth_std, n_wafers)\n",
        "    \n",
        "    # Leakage current (nA) - log-normal distribution\n",
        "    leakage_median = 10\n",
        "    leakage_std = 0.3\n",
        "    leakage_data = np.random.lognormal(np.log(leakage_median), leakage_std, n_wafers)\n",
        "    \n",
        "    # Critical dimension (nm) - normally distributed with tool bias\n",
        "    cd_target = 100\n",
        "    cd_std = 2\n",
        "    tool_bias = {'TOOL_A': 0, 'TOOL_B': 1.5, 'TOOL_C': -1.2}\n",
        "    cd_data = []\n",
        "    for tool in wafer_data['process_tool']:\n",
        "        cd_data.append(np.random.normal(cd_target + tool_bias[tool], cd_std))\n",
        "    \n",
        "    # Add all electrical parameters to wafer data\n",
        "    wafer_data.update({\n",
        "        'threshold_voltage_mv': vth_data,\n",
        "        'leakage_current_na': leakage_data,\n",
        "        'critical_dimension_nm': cd_data,\n",
        "        'yield_percent': np.random.beta(20, 2, n_wafers) * 100  # High yield with some variation\n",
        "    })\n",
        "    \n",
        "    return pd.DataFrame(wafer_data)\n",
        "\n",
        "# Generate our dataset\n",
        "df = generate_semiconductor_data()\n",
        "\n",
        "print(f\"\ud83d\udcc8 Generated semiconductor dataset with {len(df)} wafers\")\n",
        "print(f\"\ud83d\udd2c Parameters: {list(df.select_dtypes(include=[np.number]).columns)}\")\n",
        "print(\"\\n\ud83d\udccb First 5 rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ba5a69",
      "metadata": {},
      "source": [
        "## 2. Descriptive Statistics for Process Data\n",
        "\n",
        "Let's start with fundamental descriptive statistics to understand our process parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98fabc29",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_descriptive_stats(data, parameter_name):\n",
        "    \"\"\"Calculate comprehensive descriptive statistics\"\"\"\n",
        "    stats_dict = {\n",
        "        'Count': len(data),\n",
        "        'Mean': np.mean(data),\n",
        "        'Median': np.median(data),\n",
        "        'Mode': stats.mode(data, keepdims=True)[0][0] if len(stats.mode(data, keepdims=True)[0]) > 0 else np.nan,\n",
        "        'Std Dev': np.std(data, ddof=1),\n",
        "        'Variance': np.var(data, ddof=1),\n",
        "        'Min': np.min(data),\n",
        "        'Max': np.max(data),\n",
        "        'Range': np.max(data) - np.min(data),\n",
        "        'Q1 (25%)': np.percentile(data, 25),\n",
        "        'Q3 (75%)': np.percentile(data, 75),\n",
        "        'IQR': np.percentile(data, 75) - np.percentile(data, 25),\n",
        "        'Skewness': stats.skew(data),\n",
        "        'Kurtosis': stats.kurtosis(data),\n",
        "        'CV (%)': (np.std(data, ddof=1) / np.mean(data)) * 100\n",
        "    }\n",
        "    \n",
        "    return pd.DataFrame(list(stats_dict.items()), columns=['Statistic', parameter_name])\n",
        "\n",
        "# Calculate descriptive statistics for key parameters\n",
        "parameters = ['threshold_voltage_mv', 'leakage_current_na', 'critical_dimension_nm', 'yield_percent']\n",
        "\n",
        "print(\"\ud83d\udcca DESCRIPTIVE STATISTICS FOR SEMICONDUCTOR PARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for param in parameters:\n",
        "    print(f\"\\n\ud83d\udd0d {param.replace('_', ' ').title()}\")\n",
        "    stats_df = calculate_descriptive_stats(df[param], param)\n",
        "    print(stats_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fcc351",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization of parameter distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Semiconductor Parameter Distributions', fontsize=16, fontweight='bold')\n",
        "\n",
        "parameters_info = {\n",
        "    'threshold_voltage_mv': {'title': 'Threshold Voltage (mV)', 'color': 'skyblue'},\n",
        "    'leakage_current_na': {'title': 'Leakage Current (nA)', 'color': 'lightcoral'},\n",
        "    'critical_dimension_nm': {'title': 'Critical Dimension (nm)', 'color': 'lightgreen'},\n",
        "    'yield_percent': {'title': 'Yield (%)', 'color': 'gold'}\n",
        "}\n",
        "\n",
        "for i, (param, info) in enumerate(parameters_info.items()):\n",
        "    row, col = i // 2, i % 2\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Histogram with KDE\n",
        "    ax.hist(df[param], bins=20, alpha=0.7, color=info['color'], density=True, edgecolor='black')\n",
        "    \n",
        "    # Add normal distribution overlay\n",
        "    mu, sigma = df[param].mean(), df[param].std()\n",
        "    x = np.linspace(df[param].min(), df[param].max(), 100)\n",
        "    normal_curve = stats.norm.pdf(x, mu, sigma)\n",
        "    ax.plot(x, normal_curve, 'r-', linewidth=2, label=f'Normal(\u03bc={mu:.2f}, \u03c3={sigma:.2f})')\n",
        "    \n",
        "    # Add statistics annotations\n",
        "    ax.axvline(mu, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mu:.2f}')\n",
        "    ax.axvline(np.median(df[param]), color='orange', linestyle='--', alpha=0.8, label=f'Median: {np.median(df[param]):.2f}')\n",
        "    \n",
        "    ax.set_title(info['title'], fontweight='bold')\n",
        "    ax.set_xlabel('Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e0d4ddc",
      "metadata": {},
      "source": [
        "## 3. Process Capability Analysis\n",
        "\n",
        "Process capability indices are crucial for assessing whether a process can consistently produce parts within specification limits.\n",
        "\n",
        "### Key Capability Indices:\n",
        "- **Cp**: Process Capability (spread only)\n",
        "- **Cpk**: Process Capability Index (accounts for centering)\n",
        "- **Pp**: Process Performance (long-term)\n",
        "- **Ppk**: Process Performance Index (long-term with centering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0302fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_process_capability(data, lsl, usl, target=None):\n",
        "    \"\"\"\n",
        "    Calculate process capability indices\n",
        "    \n",
        "    Parameters:\n",
        "    data: array-like, process data\n",
        "    lsl: float, lower specification limit\n",
        "    usl: float, upper specification limit\n",
        "    target: float, target value (optional, defaults to midpoint)\n",
        "    \n",
        "    Returns:\n",
        "    dict: capability indices and related statistics\n",
        "    \"\"\"\n",
        "    if target is None:\n",
        "        target = (lsl + usl) / 2\n",
        "    \n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data, ddof=1)\n",
        "    \n",
        "    # Basic capability indices\n",
        "    cp = (usl - lsl) / (6 * std)\n",
        "    cpu = (usl - mean) / (3 * std)\n",
        "    cpl = (mean - lsl) / (3 * std)\n",
        "    cpk = min(cpu, cpl)\n",
        "    \n",
        "    # Process performance (assumes long-term data)\n",
        "    pp = cp  # Same calculation for this example\n",
        "    ppk = cpk  # Same calculation for this example\n",
        "    \n",
        "    # Additional metrics\n",
        "    percent_defective = (stats.norm.cdf(lsl, mean, std) + \n",
        "                        (1 - stats.norm.cdf(usl, mean, std))) * 100\n",
        "    \n",
        "    sigma_level = min(cpu, cpl) * 3\n",
        "    \n",
        "    results = {\n",
        "        'Mean': mean,\n",
        "        'Std Dev': std,\n",
        "        'LSL': lsl,\n",
        "        'USL': usl,\n",
        "        'Target': target,\n",
        "        'Cp': cp,\n",
        "        'Cpu': cpu,\n",
        "        'Cpl': cpl,\n",
        "        'Cpk': cpk,\n",
        "        'Pp': pp,\n",
        "        'Ppk': ppk,\n",
        "        'Percent Defective': percent_defective,\n",
        "        'Sigma Level': sigma_level,\n",
        "        'Process Centered': abs(mean - target) < (usl - lsl) * 0.05  # Within 5% of spec width\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Define specification limits for our parameters\n",
        "specifications = {\n",
        "    'threshold_voltage_mv': {'lsl': 600, 'usl': 700, 'target': 650},\n",
        "    'critical_dimension_nm': {'lsl': 95, 'usl': 105, 'target': 100},\n",
        "    'leakage_current_na': {'lsl': 0, 'usl': 50, 'target': 10},\n",
        "    'yield_percent': {'lsl': 90, 'usl': 100, 'target': 95}\n",
        "}\n",
        "\n",
        "print(\"\ud83c\udfaf PROCESS CAPABILITY ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "capability_results = {}\n",
        "for param, specs in specifications.items():\n",
        "    print(f\"\\n\ud83d\udcca {param.replace('_', ' ').title()}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    results = calculate_process_capability(df[param], **specs)\n",
        "    capability_results[param] = results\n",
        "    \n",
        "    # Display key results\n",
        "    print(f\"Cp:  {results['Cp']:.3f} | Cpk: {results['Cpk']:.3f}\")\n",
        "    print(f\"Mean: {results['Mean']:.2f} | Std: {results['Std Dev']:.2f}\")\n",
        "    print(f\"Defective: {results['Percent Defective']:.4f}%\")\n",
        "    print(f\"Sigma Level: {results['Sigma Level']:.2f}\")\n",
        "    print(f\"Centered: {'\u2705' if results['Process Centered'] else '\u274c'}\")\n",
        "    \n",
        "    # Capability interpretation\n",
        "    if results['Cpk'] >= 1.33:\n",
        "        capability_status = \"\ud83d\udfe2 Excellent (Cpk \u2265 1.33)\"\n",
        "    elif results['Cpk'] >= 1.0:\n",
        "        capability_status = \"\ud83d\udfe1 Adequate (1.0 \u2264 Cpk < 1.33)\"\n",
        "    else:\n",
        "        capability_status = \"\ud83d\udd34 Poor (Cpk < 1.0)\"\n",
        "    \n",
        "    print(f\"Status: {capability_status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a1bf94",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize process capability for threshold voltage\n",
        "def plot_process_capability(data, specs, title):\n",
        "    \"\"\"Create a comprehensive process capability plot\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Left plot: Histogram with specification limits\n",
        "    ax1.hist(data, bins=20, alpha=0.7, color='skyblue', density=True, edgecolor='black')\n",
        "    \n",
        "    # Add specification limits\n",
        "    ax1.axvline(specs['lsl'], color='red', linestyle='-', linewidth=2, label=f'LSL: {specs[\"lsl\"]}')\n",
        "    ax1.axvline(specs['usl'], color='red', linestyle='-', linewidth=2, label=f'USL: {specs[\"usl\"]}')\n",
        "    ax1.axvline(specs['target'], color='green', linestyle='--', linewidth=2, label=f'Target: {specs[\"target\"]}')\n",
        "    ax1.axvline(np.mean(data), color='blue', linestyle='--', linewidth=2, label=f'Mean: {np.mean(data):.2f}')\n",
        "    \n",
        "    # Add normal distribution overlay\n",
        "    mu, sigma = np.mean(data), np.std(data, ddof=1)\n",
        "    x = np.linspace(data.min(), data.max(), 100)\n",
        "    normal_curve = stats.norm.pdf(x, mu, sigma)\n",
        "    ax1.plot(x, normal_curve, 'black', linewidth=2, alpha=0.8, label='Normal Fit')\n",
        "    \n",
        "    ax1.set_title(f'{title} - Process Capability', fontweight='bold')\n",
        "    ax1.set_xlabel('Value')\n",
        "    ax1.set_ylabel('Density')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Right plot: Capability metrics visualization\n",
        "    results = calculate_process_capability(data, **specs)\n",
        "    \n",
        "    metrics = ['Cp', 'Cpk', 'Cpu', 'Cpl']\n",
        "    values = [results[metric] for metric in metrics]\n",
        "    colors = ['blue' if val >= 1.33 else 'orange' if val >= 1.0 else 'red' for val in values]\n",
        "    \n",
        "    bars = ax2.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    # Add benchmark lines\n",
        "    ax2.axhline(y=1.0, color='orange', linestyle='--', alpha=0.8, label='Minimum (1.0)')\n",
        "    ax2.axhline(y=1.33, color='green', linestyle='--', alpha=0.8, label='Target (1.33)')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    ax2.set_title('Capability Indices', fontweight='bold')\n",
        "    ax2.set_ylabel('Index Value')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim(0, max(values) * 1.2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot capability for threshold voltage\n",
        "plot_process_capability(df['threshold_voltage_mv'], \n",
        "                       specifications['threshold_voltage_mv'], \n",
        "                       'Threshold Voltage (mV)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480be881",
      "metadata": {},
      "source": [
        "## 4. Distribution Analysis and Normality Testing\n",
        "\n",
        "Understanding the underlying distribution of process parameters is crucial for proper statistical analysis and yield prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11bc4c36",
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_normality_tests(data, parameter_name):\n",
        "    \"\"\"Perform multiple normality tests\"\"\"\n",
        "    print(f\"\ud83d\udd0d NORMALITY TESTING: {parameter_name}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Shapiro-Wilk Test\n",
        "    shapiro_stat, shapiro_p = shapiro(data)\n",
        "    print(f\"Shapiro-Wilk: W = {shapiro_stat:.4f}, p = {shapiro_p:.4f}\")\n",
        "    \n",
        "    # Anderson-Darling Test\n",
        "    anderson_result = anderson(data, dist='norm')\n",
        "    print(f\"Anderson-Darling: A\u00b2 = {anderson_result.statistic:.4f}\")\n",
        "    \n",
        "    # Kolmogorov-Smirnov Test\n",
        "    # First fit normal distribution\n",
        "    mu, sigma = stats.norm.fit(data)\n",
        "    ks_stat, ks_p = stats.kstest(data, lambda x: stats.norm.cdf(x, mu, sigma))\n",
        "    print(f\"Kolmogorov-Smirnov: D = {ks_stat:.4f}, p = {ks_p:.4f}\")\n",
        "    \n",
        "    # D'Agostino's Test\n",
        "    dagostino_stat, dagostino_p = stats.normaltest(data)\n",
        "    print(f\"D'Agostino: \u03c7\u00b2 = {dagostino_stat:.4f}, p = {dagostino_p:.4f}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    alpha = 0.05\n",
        "    tests_passed = sum([\n",
        "        shapiro_p > alpha,\n",
        "        ks_p > alpha,\n",
        "        dagostino_p > alpha\n",
        "    ])\n",
        "    \n",
        "    print(f\"\\nResult: {tests_passed}/3 tests support normality (\u03b1 = {alpha})\")\n",
        "    if tests_passed >= 2:\n",
        "        print(\"\u2705 Data appears to be normally distributed\")\n",
        "    else:\n",
        "        print(\"\u274c Data may not be normally distributed\")\n",
        "    \n",
        "    return {\n",
        "        'shapiro_w': shapiro_stat,\n",
        "        'shapiro_p': shapiro_p,\n",
        "        'anderson_a2': anderson_result.statistic,\n",
        "        'ks_d': ks_stat,\n",
        "        'ks_p': ks_p,\n",
        "        'dagostino_chi2': dagostino_stat,\n",
        "        'dagostino_p': dagostino_p,\n",
        "        'normal_likely': tests_passed >= 2\n",
        "    }\n",
        "\n",
        "# Test normality for all parameters\n",
        "normality_results = {}\n",
        "for param in parameters:\n",
        "    normality_results[param] = comprehensive_normality_tests(df[param], param.replace('_', ' ').title())\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a99039",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Q-Q plots for visual normality assessment\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Q-Q Plots for Normality Assessment', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, param in enumerate(parameters):\n",
        "    row, col = i // 2, i % 2\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Create Q-Q plot\n",
        "    stats.probplot(df[param], dist=\"norm\", plot=ax)\n",
        "    ax.set_title(f'{param.replace(\"_\", \" \").title()} Q-Q Plot', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add R\u00b2 value for linearity assessment\n",
        "    # Fit line to Q-Q plot data\n",
        "    theoretical_quantiles, sample_quantiles = stats.probplot(df[param], dist=\"norm\")[:2]\n",
        "    slope, intercept, r_value, _, _ = stats.linregress(theoretical_quantiles, sample_quantiles)\n",
        "    ax.text(0.05, 0.95, f'R\u00b2 = {r_value**2:.4f}', transform=ax.transAxes, \n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
        "            verticalalignment='top', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb149957",
      "metadata": {},
      "source": [
        "## 5. Hypothesis Testing for Process Validation\n",
        "\n",
        "Hypothesis testing helps us make data-driven decisions about process changes, tool comparisons, and quality improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6f90c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_t_test_analysis():\n",
        "    \"\"\"Compare process parameters between different tools\"\"\"\n",
        "    print(\"\ud83e\uddea HYPOTHESIS TESTING: Tool Comparison\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Compare threshold voltage between Tool A and Tool B\n",
        "    tool_a_vth = df[df['process_tool'] == 'TOOL_A']['threshold_voltage_mv']\n",
        "    tool_b_vth = df[df['process_tool'] == 'TOOL_B']['threshold_voltage_mv']\n",
        "    \n",
        "    print(f\"Tool A samples: {len(tool_a_vth)}\")\n",
        "    print(f\"Tool B samples: {len(tool_b_vth)}\")\n",
        "    print(f\"Tool A mean: {tool_a_vth.mean():.2f} \u00b1 {tool_a_vth.std():.2f}\")\n",
        "    print(f\"Tool B mean: {tool_b_vth.mean():.2f} \u00b1 {tool_b_vth.std():.2f}\")\n",
        "    \n",
        "    # Test for equal variances (Levene's test)\n",
        "    levene_stat, levene_p = stats.levene(tool_a_vth, tool_b_vth)\n",
        "    equal_var = levene_p > 0.05\n",
        "    \n",
        "    print(f\"\\nLevene's test for equal variances: p = {levene_p:.4f}\")\n",
        "    print(f\"Equal variances assumed: {'Yes' if equal_var else 'No'}\")\n",
        "    \n",
        "    # Perform two-sample t-test\n",
        "    t_stat, t_p = stats.ttest_ind(tool_a_vth, tool_b_vth, equal_var=equal_var)\n",
        "    \n",
        "    print(f\"\\nTwo-sample t-test:\")\n",
        "    print(f\"t-statistic: {t_stat:.4f}\")\n",
        "    print(f\"p-value: {t_p:.4f}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    alpha = 0.05\n",
        "    if t_p < alpha:\n",
        "        print(f\"\u2705 Significant difference detected (p < {alpha})\")\n",
        "        print(\"\ud83d\udd27 Recommendation: Investigate tool calibration\")\n",
        "    else:\n",
        "        print(f\"\u274c No significant difference (p \u2265 {alpha})\")\n",
        "        print(\"\u2705 Tools appear to perform similarly\")\n",
        "    \n",
        "    # Effect size (Cohen's d)\n",
        "    pooled_std = np.sqrt(((len(tool_a_vth) - 1) * tool_a_vth.var() + \n",
        "                         (len(tool_b_vth) - 1) * tool_b_vth.var()) / \n",
        "                        (len(tool_a_vth) + len(tool_b_vth) - 2))\n",
        "    cohens_d = (tool_a_vth.mean() - tool_b_vth.mean()) / pooled_std\n",
        "    \n",
        "    print(f\"Effect size (Cohen's d): {cohens_d:.4f}\")\n",
        "    \n",
        "    if abs(cohens_d) < 0.2:\n",
        "        effect_interpretation = \"Small effect\"\n",
        "    elif abs(cohens_d) < 0.5:\n",
        "        effect_interpretation = \"Medium effect\"\n",
        "    else:\n",
        "        effect_interpretation = \"Large effect\"\n",
        "    \n",
        "    print(f\"Effect interpretation: {effect_interpretation}\")\n",
        "    \n",
        "    return {\n",
        "        'tool_a_mean': tool_a_vth.mean(),\n",
        "        'tool_b_mean': tool_b_vth.mean(),\n",
        "        't_statistic': t_stat,\n",
        "        'p_value': t_p,\n",
        "        'cohens_d': cohens_d,\n",
        "        'significant': t_p < alpha\n",
        "    }\n",
        "\n",
        "# Perform t-test analysis\n",
        "t_test_results = perform_t_test_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a380c6a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_anova_analysis():\n",
        "    \"\"\"Compare process parameters across all tools using ANOVA\"\"\"\n",
        "    print(\"\\n\ud83e\uddea ANOVA: Multi-Tool Comparison\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Prepare data for ANOVA\n",
        "    tool_groups = []\n",
        "    for tool in df['process_tool'].unique():\n",
        "        tool_data = df[df['process_tool'] == tool]['critical_dimension_nm']\n",
        "        tool_groups.append(tool_data)\n",
        "        print(f\"{tool}: n={len(tool_data)}, mean={tool_data.mean():.2f}, std={tool_data.std():.2f}\")\n",
        "    \n",
        "    # Perform one-way ANOVA\n",
        "    f_stat, f_p = stats.f_oneway(*tool_groups)\n",
        "    \n",
        "    print(f\"\\nOne-way ANOVA Results:\")\n",
        "    print(f\"F-statistic: {f_stat:.4f}\")\n",
        "    print(f\"p-value: {f_p:.4f}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    alpha = 0.05\n",
        "    if f_p < alpha:\n",
        "        print(f\"\u2705 Significant differences between tools detected (p < {alpha})\")\n",
        "        print(\"\ud83d\udd27 Recommendation: Perform post-hoc analysis to identify which tools differ\")\n",
        "        \n",
        "        # Perform pairwise t-tests with Bonferroni correction\n",
        "        from itertools import combinations\n",
        "        tools = df['process_tool'].unique()\n",
        "        n_comparisons = len(list(combinations(tools, 2)))\n",
        "        bonferroni_alpha = alpha / n_comparisons\n",
        "        \n",
        "        print(f\"\\nPost-hoc pairwise comparisons (Bonferroni \u03b1 = {bonferroni_alpha:.4f}):\")\n",
        "        \n",
        "        for tool1, tool2 in combinations(tools, 2):\n",
        "            group1 = df[df['process_tool'] == tool1]['critical_dimension_nm']\n",
        "            group2 = df[df['process_tool'] == tool2]['critical_dimension_nm']\n",
        "            \n",
        "            t_stat, t_p = stats.ttest_ind(group1, group2)\n",
        "            significant = t_p < bonferroni_alpha\n",
        "            \n",
        "            print(f\"{tool1} vs {tool2}: p = {t_p:.4f} {'*' if significant else ''}\")\n",
        "    else:\n",
        "        print(f\"\u274c No significant differences between tools (p \u2265 {alpha})\")\n",
        "        print(\"\u2705 All tools perform similarly for critical dimension\")\n",
        "    \n",
        "    return {\n",
        "        'f_statistic': f_stat,\n",
        "        'p_value': f_p,\n",
        "        'significant': f_p < alpha\n",
        "    }\n",
        "\n",
        "# Perform ANOVA analysis\n",
        "anova_results = perform_anova_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36d438f",
      "metadata": {},
      "source": [
        "## 6. Statistical Process Control (SPC) and Control Charts\n",
        "\n",
        "Control charts are essential tools for monitoring process stability and detecting special cause variation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb1ab078",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_control_chart_data():\n",
        "    \"\"\"Create time-series data for control chart demonstration\"\"\"\n",
        "    # Sort data by timestamp for time series analysis\n",
        "    df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    \n",
        "    # Create subgroups (every 5 consecutive wafers)\n",
        "    subgroup_size = 5\n",
        "    n_subgroups = len(df_sorted) // subgroup_size\n",
        "    \n",
        "    subgroup_data = []\n",
        "    for i in range(n_subgroups):\n",
        "        start_idx = i * subgroup_size\n",
        "        end_idx = start_idx + subgroup_size\n",
        "        subgroup = df_sorted.iloc[start_idx:end_idx]\n",
        "        \n",
        "        subgroup_stats = {\n",
        "            'subgroup': i + 1,\n",
        "            'timestamp': subgroup['timestamp'].iloc[0],\n",
        "            'mean': subgroup['threshold_voltage_mv'].mean(),\n",
        "            'range': subgroup['threshold_voltage_mv'].max() - subgroup['threshold_voltage_mv'].min(),\n",
        "            'std': subgroup['threshold_voltage_mv'].std(),\n",
        "            'median': subgroup['threshold_voltage_mv'].median()\n",
        "        }\n",
        "        subgroup_data.append(subgroup_stats)\n",
        "    \n",
        "    return pd.DataFrame(subgroup_data)\n",
        "\n",
        "def calculate_control_limits(data, chart_type='xbar'):\n",
        "    \"\"\"Calculate control limits for different chart types\"\"\"\n",
        "    \n",
        "    # Control chart constants for subgroup size = 5\n",
        "    A2 = 0.577  # For X-bar chart\n",
        "    D3 = 0.0    # For R chart (lower limit)\n",
        "    D4 = 2.114  # For R chart (upper limit)\n",
        "    \n",
        "    if chart_type == 'xbar':\n",
        "        center_line = data['mean'].mean()\n",
        "        r_bar = data['range'].mean()\n",
        "        \n",
        "        ucl = center_line + A2 * r_bar\n",
        "        lcl = center_line - A2 * r_bar\n",
        "        \n",
        "        return center_line, ucl, lcl\n",
        "    \n",
        "    elif chart_type == 'range':\n",
        "        center_line = data['range'].mean()\n",
        "        \n",
        "        ucl = D4 * center_line\n",
        "        lcl = D3 * center_line\n",
        "        \n",
        "        return center_line, ucl, lcl\n",
        "\n",
        "# Create control chart data\n",
        "control_data = create_control_chart_data()\n",
        "\n",
        "print(f\"\ud83d\udcca CONTROL CHART ANALYSIS\")\n",
        "print(f\"Subgroups created: {len(control_data)}\")\n",
        "print(f\"Subgroup size: 5 wafers\")\n",
        "print(f\"Parameter: Threshold Voltage (mV)\")\n",
        "\n",
        "# Calculate control limits\n",
        "xbar_cl, xbar_ucl, xbar_lcl = calculate_control_limits(control_data, 'xbar')\n",
        "r_cl, r_ucl, r_lcl = calculate_control_limits(control_data, 'range')\n",
        "\n",
        "print(f\"\\nX-bar Chart Limits:\")\n",
        "print(f\"UCL: {xbar_ucl:.2f} mV\")\n",
        "print(f\"CL:  {xbar_cl:.2f} mV\")\n",
        "print(f\"LCL: {xbar_lcl:.2f} mV\")\n",
        "\n",
        "print(f\"\\nR Chart Limits:\")\n",
        "print(f\"UCL: {r_ucl:.2f} mV\")\n",
        "print(f\"CL:  {r_cl:.2f} mV\")\n",
        "print(f\"LCL: {r_lcl:.2f} mV\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00405439",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create X-bar and R control charts\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "fig.suptitle('Statistical Process Control Charts - Threshold Voltage', fontsize=16, fontweight='bold')\n",
        "\n",
        "# X-bar Chart\n",
        "ax1.plot(control_data['subgroup'], control_data['mean'], 'bo-', linewidth=2, markersize=6, label='Subgroup Means')\n",
        "ax1.axhline(y=xbar_cl, color='green', linestyle='-', linewidth=2, label=f'Center Line ({xbar_cl:.2f})')\n",
        "ax1.axhline(y=xbar_ucl, color='red', linestyle='--', linewidth=2, label=f'UCL ({xbar_ucl:.2f})')\n",
        "ax1.axhline(y=xbar_lcl, color='red', linestyle='--', linewidth=2, label=f'LCL ({xbar_lcl:.2f})')\n",
        "\n",
        "# Check for out-of-control points\n",
        "ooc_points_xbar = control_data[(control_data['mean'] > xbar_ucl) | (control_data['mean'] < xbar_lcl)]\n",
        "if not ooc_points_xbar.empty:\n",
        "    ax1.scatter(ooc_points_xbar['subgroup'], ooc_points_xbar['mean'], \n",
        "               color='red', s=100, marker='x', linewidth=3, label='Out of Control')\n",
        "\n",
        "ax1.set_title('X-bar Chart (Process Centering)', fontweight='bold')\n",
        "ax1.set_xlabel('Subgroup Number')\n",
        "ax1.set_ylabel('Mean Threshold Voltage (mV)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# R Chart\n",
        "ax2.plot(control_data['subgroup'], control_data['range'], 'go-', linewidth=2, markersize=6, label='Subgroup Ranges')\n",
        "ax2.axhline(y=r_cl, color='green', linestyle='-', linewidth=2, label=f'Center Line ({r_cl:.2f})')\n",
        "ax2.axhline(y=r_ucl, color='red', linestyle='--', linewidth=2, label=f'UCL ({r_ucl:.2f})')\n",
        "ax2.axhline(y=r_lcl, color='red', linestyle='--', linewidth=2, label=f'LCL ({r_lcl:.2f})')\n",
        "\n",
        "# Check for out-of-control points\n",
        "ooc_points_r = control_data[(control_data['range'] > r_ucl) | (control_data['range'] < r_lcl)]\n",
        "if not ooc_points_r.empty:\n",
        "    ax2.scatter(ooc_points_r['subgroup'], ooc_points_r['range'], \n",
        "               color='red', s=100, marker='x', linewidth=3, label='Out of Control')\n",
        "\n",
        "ax2.set_title('R Chart (Process Variation)', fontweight='bold')\n",
        "ax2.set_xlabel('Subgroup Number')\n",
        "ax2.set_ylabel('Range (mV)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze control chart results\n",
        "print(\"\\n\ud83d\udd0d CONTROL CHART ANALYSIS RESULTS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "xbar_ooc = len(ooc_points_xbar)\n",
        "r_ooc = len(ooc_points_r)\n",
        "\n",
        "print(f\"X-bar Chart: {xbar_ooc}/{len(control_data)} points out of control\")\n",
        "print(f\"R Chart: {r_ooc}/{len(control_data)} points out of control\")\n",
        "\n",
        "if xbar_ooc == 0 and r_ooc == 0:\n",
        "    print(\"\u2705 Process appears to be in statistical control\")\n",
        "    print(\"\u2705 No special cause variation detected\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Special cause variation detected\")\n",
        "    if xbar_ooc > 0:\n",
        "        print(f\"  \ud83d\udccd Process centering issues in subgroups: {ooc_points_xbar['subgroup'].tolist()}\")\n",
        "    if r_ooc > 0:\n",
        "        print(f\"  \ud83d\udccd Process variation issues in subgroups: {ooc_points_r['subgroup'].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b51df8",
      "metadata": {},
      "source": [
        "## 7. Yield Prediction and Modeling\n",
        "\n",
        "Using statistical models to predict yield based on process parameters and specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bb0871",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_yield_from_distribution(data, lsl, usl):\n",
        "    \"\"\"Predict yield based on normal distribution assumption\"\"\"\n",
        "    mu = np.mean(data)\n",
        "    sigma = np.std(data, ddof=1)\n",
        "    \n",
        "    # Calculate probability of being within specifications\n",
        "    prob_within_spec = stats.norm.cdf(usl, mu, sigma) - stats.norm.cdf(lsl, mu, sigma)\n",
        "    yield_percent = prob_within_spec * 100\n",
        "    \n",
        "    # Calculate defect rates\n",
        "    lower_defects = stats.norm.cdf(lsl, mu, sigma) * 100\n",
        "    upper_defects = (1 - stats.norm.cdf(usl, mu, sigma)) * 100\n",
        "    \n",
        "    return {\n",
        "        'predicted_yield': yield_percent,\n",
        "        'lower_defects': lower_defects,\n",
        "        'upper_defects': upper_defects,\n",
        "        'total_defects': lower_defects + upper_defects\n",
        "    }\n",
        "\n",
        "def monte_carlo_yield_simulation(n_simulations=10000):\n",
        "    \"\"\"Monte Carlo simulation for yield prediction\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Simulate process parameters\n",
        "    vth_sim = np.random.normal(650, 25, n_simulations)\n",
        "    cd_sim = np.random.normal(100, 2, n_simulations)\n",
        "    leakage_sim = np.random.lognormal(np.log(10), 0.3, n_simulations)\n",
        "    \n",
        "    # Apply specifications\n",
        "    vth_pass = (vth_sim >= 600) & (vth_sim <= 700)\n",
        "    cd_pass = (cd_sim >= 95) & (cd_sim <= 105)\n",
        "    leakage_pass = leakage_sim <= 50\n",
        "    \n",
        "    # Overall yield (all parameters must pass)\n",
        "    overall_pass = vth_pass & cd_pass & leakage_pass\n",
        "    simulated_yield = np.mean(overall_pass) * 100\n",
        "    \n",
        "    # Individual parameter yields\n",
        "    vth_yield = np.mean(vth_pass) * 100\n",
        "    cd_yield = np.mean(cd_pass) * 100\n",
        "    leakage_yield = np.mean(leakage_pass) * 100\n",
        "    \n",
        "    return {\n",
        "        'overall_yield': simulated_yield,\n",
        "        'vth_yield': vth_yield,\n",
        "        'cd_yield': cd_yield,\n",
        "        'leakage_yield': leakage_yield,\n",
        "        'n_simulations': n_simulations\n",
        "    }\n",
        "\n",
        "print(\"\ud83c\udfaf YIELD PREDICTION ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Predict yield for each parameter\n",
        "yield_predictions = {}\n",
        "for param, specs in specifications.items():\n",
        "    if param != 'yield_percent':  # Skip actual yield column\n",
        "        prediction = predict_yield_from_distribution(df[param], specs['lsl'], specs['usl'])\n",
        "        yield_predictions[param] = prediction\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca {param.replace('_', ' ').title()}\")\n",
        "        print(f\"Predicted Yield: {prediction['predicted_yield']:.2f}%\")\n",
        "        print(f\"Lower Defects: {prediction['lower_defects']:.4f}%\")\n",
        "        print(f\"Upper Defects: {prediction['upper_defects']:.4f}%\")\n",
        "\n",
        "# Monte Carlo simulation\n",
        "print(f\"\\n\ud83c\udfb2 MONTE CARLO SIMULATION\")\n",
        "print(\"-\" * 30)\n",
        "mc_results = monte_carlo_yield_simulation()\n",
        "\n",
        "print(f\"Simulations: {mc_results['n_simulations']:,}\")\n",
        "print(f\"Overall Yield: {mc_results['overall_yield']:.2f}%\")\n",
        "print(f\"VTH Yield: {mc_results['vth_yield']:.2f}%\")\n",
        "print(f\"CD Yield: {mc_results['cd_yield']:.2f}%\")\n",
        "print(f\"Leakage Yield: {mc_results['leakage_yield']:.2f}%\")\n",
        "\n",
        "# Compare with actual yield\n",
        "actual_yield = df['yield_percent'].mean()\n",
        "print(f\"\\nActual Average Yield: {actual_yield:.2f}%\")\n",
        "print(f\"Prediction Accuracy: {abs(mc_results['overall_yield'] - actual_yield):.2f}% difference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c15d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize yield prediction results\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Yield Prediction Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Individual parameter yield predictions\n",
        "params = list(yield_predictions.keys())\n",
        "yields = [yield_predictions[p]['predicted_yield'] for p in params]\n",
        "param_names = [p.replace('_', ' ').title() for p in params]\n",
        "\n",
        "bars1 = ax1.bar(param_names, yields, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
        "ax1.set_title('Predicted Yield by Parameter', fontweight='bold')\n",
        "ax1.set_ylabel('Yield (%)')\n",
        "ax1.set_ylim(90, 101)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, yield_val in zip(bars1, yields):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{yield_val:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Monte Carlo vs Actual Yield Comparison\n",
        "methods = ['Monte Carlo', 'Actual Data']\n",
        "yield_values = [mc_results['overall_yield'], actual_yield]\n",
        "colors = ['gold', 'coral']\n",
        "\n",
        "bars2 = ax2.bar(methods, yield_values, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax2.set_title('Yield Prediction Validation', fontweight='bold')\n",
        "ax2.set_ylabel('Overall Yield (%)')\n",
        "\n",
        "# Add value labels\n",
        "for bar, yield_val in zip(bars2, yield_values):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{yield_val:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Defect breakdown for threshold voltage\n",
        "vth_prediction = yield_predictions['threshold_voltage_mv']\n",
        "defect_types = ['Lower Defects', 'Upper Defects', 'Good Parts']\n",
        "defect_values = [vth_prediction['lower_defects'], \n",
        "                vth_prediction['upper_defects'],\n",
        "                vth_prediction['predicted_yield']]\n",
        "colors = ['red', 'orange', 'green']\n",
        "\n",
        "ax3.pie(defect_values, labels=defect_types, colors=colors, autopct='%1.2f%%', startangle=90)\n",
        "ax3.set_title('Threshold Voltage Defect Breakdown', fontweight='bold')\n",
        "\n",
        "# Plot 4: Process capability vs yield correlation\n",
        "cp_values = [capability_results[p]['Cp'] for p in params]\n",
        "yield_values_cp = [yield_predictions[p]['predicted_yield'] for p in params]\n",
        "\n",
        "scatter = ax4.scatter(cp_values, yield_values_cp, c=['blue', 'red', 'green'], \n",
        "                     s=100, alpha=0.7, edgecolors='black')\n",
        "ax4.set_xlabel('Process Capability (Cp)')\n",
        "ax4.set_ylabel('Predicted Yield (%)')\n",
        "ax4.set_title('Capability vs Yield Relationship', fontweight='bold')\n",
        "\n",
        "# Add parameter labels\n",
        "for i, param in enumerate(param_names):\n",
        "    ax4.annotate(param, (cp_values[i], yield_values_cp[i]), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9b51f1",
      "metadata": {},
      "source": [
        "## 8. Regression Analysis for Process Relationships\n",
        "\n",
        "Understanding relationships between process parameters helps optimize manufacturing conditions and predict outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fce4365",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis between parameters\n",
        "correlation_matrix = df[parameters].corr()\n",
        "\n",
        "print(\"\ud83d\udd17 CORRELATION ANALYSIS\")\n",
        "print(\"=\" * 30)\n",
        "print(correlation_matrix.round(4))\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, \n",
        "            mask=mask,\n",
        "            annot=True, \n",
        "            cmap='RdBu_r', \n",
        "            center=0,\n",
        "            square=True,\n",
        "            fmt='.3f',\n",
        "            cbar_kws={\"shrink\": .8})\n",
        "plt.title('Parameter Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Linear regression: Predict yield from threshold voltage\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "X = df[['threshold_voltage_mv']].values\n",
        "y = df['yield_percent'].values\n",
        "\n",
        "# Fit linear regression\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X, y)\n",
        "y_pred = reg_model.predict(X)\n",
        "\n",
        "# Calculate metrics\n",
        "r2 = r2_score(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 LINEAR REGRESSION: Yield vs Threshold Voltage\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Slope: {reg_model.coef_[0]:.4f} %/mV\")\n",
        "print(f\"Intercept: {reg_model.intercept_:.4f} %\")\n",
        "print(f\"R\u00b2 Score: {r2:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f} %\")\n",
        "\n",
        "# Statistical significance test\n",
        "n = len(X)\n",
        "t_stat = reg_model.coef_[0] * np.sqrt((n-2) * np.sum((X - X.mean())**2)) / (rmse * np.sqrt(n))\n",
        "p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n-2))\n",
        "\n",
        "print(f\"t-statistic: {t_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"\u2705 Relationship is statistically significant\")\n",
        "else:\n",
        "    print(\"\u274c Relationship is not statistically significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f4ae65",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize regression analysis\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Regression plot\n",
        "ax1.scatter(df['threshold_voltage_mv'], df['yield_percent'], alpha=0.6, color='blue', label='Data Points')\n",
        "ax1.plot(X, y_pred, 'r-', linewidth=2, label=f'Regression Line (R\u00b2 = {r2:.3f})')\n",
        "\n",
        "# Add confidence interval\n",
        "from scipy.stats import t\n",
        "confidence = 0.95\n",
        "alpha = 1 - confidence\n",
        "t_val = t.ppf(1 - alpha/2, n-2)\n",
        "se = rmse * np.sqrt(1/n + (X - X.mean())**2 / np.sum((X - X.mean())**2))\n",
        "ci_lower = y_pred - t_val * se.flatten()\n",
        "ci_upper = y_pred + t_val * se.flatten()\n",
        "\n",
        "# Sort for plotting\n",
        "sort_idx = np.argsort(X.flatten())\n",
        "ax1.fill_between(X[sort_idx].flatten(), ci_lower[sort_idx], ci_upper[sort_idx], \n",
        "                alpha=0.2, color='red', label=f'{confidence*100}% Confidence Interval')\n",
        "\n",
        "ax1.set_xlabel('Threshold Voltage (mV)')\n",
        "ax1.set_ylabel('Yield (%)')\n",
        "ax1.set_title('Yield vs Threshold Voltage Regression', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Residual plot\n",
        "residuals = y - y_pred\n",
        "ax2.scatter(y_pred, residuals, alpha=0.6, color='green')\n",
        "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "ax2.set_xlabel('Predicted Yield (%)')\n",
        "ax2.set_ylabel('Residuals (%)')\n",
        "ax2.set_title('Residual Plot', fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add residual statistics\n",
        "residual_std = np.std(residuals)\n",
        "ax2.text(0.05, 0.95, f'Residual Std: {residual_std:.3f}%\\nMean: {np.mean(residuals):.3f}%', \n",
        "         transform=ax2.transAxes, verticalalignment='top',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56444399",
      "metadata": {},
      "source": [
        "## 9. Summary and Key Takeaways\n",
        "\n",
        "Let's summarize our statistical analysis findings and provide actionable recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d572bdfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_statistical_summary():\n",
        "    \"\"\"Generate comprehensive summary of statistical analysis\"\"\"\n",
        "    \n",
        "    print(\"\ud83d\udccb STATISTICAL ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    print(\"\\n\ud83c\udfaf PROCESS CAPABILITY ASSESSMENT\")\n",
        "    print(\"-\" * 35)\n",
        "    for param, results in capability_results.items():\n",
        "        status = \"\ud83d\udfe2 Excellent\" if results['Cpk'] >= 1.33 else \"\ud83d\udfe1 Adequate\" if results['Cpk'] >= 1.0 else \"\ud83d\udd34 Poor\"\n",
        "        print(f\"{param.replace('_', ' ').title()}: Cpk = {results['Cpk']:.3f} {status}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca NORMALITY ASSESSMENT\")\n",
        "    print(\"-\" * 25)\n",
        "    for param, results in normality_results.items():\n",
        "        status = \"\u2705 Normal\" if results['normal_likely'] else \"\u274c Non-normal\"\n",
        "        print(f\"{param.replace('_', ' ').title()}: {status}\")\n",
        "    \n",
        "    print(\"\\n\ud83e\uddea HYPOTHESIS TESTING RESULTS\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Tool Comparison (t-test): {'Significant' if t_test_results['significant'] else 'Not significant'}\")\n",
        "    print(f\"Multi-tool ANOVA: {'Significant' if anova_results['significant'] else 'Not significant'}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcc8 PROCESS CONTROL STATUS\")\n",
        "    print(\"-\" * 25)\n",
        "    total_points = len(control_data)\n",
        "    ooc_xbar = len(control_data[(control_data['mean'] > xbar_ucl) | (control_data['mean'] < xbar_lcl)])\n",
        "    ooc_r = len(control_data[(control_data['range'] > r_ucl) | (control_data['range'] < r_lcl)])\n",
        "    \n",
        "    if ooc_xbar == 0 and ooc_r == 0:\n",
        "        print(\"\u2705 Process in statistical control\")\n",
        "    else:\n",
        "        print(f\"\u26a0\ufe0f {ooc_xbar + ooc_r}/{total_points} points out of control\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udfaf YIELD PREDICTIONS\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Monte Carlo Simulation: {mc_results['overall_yield']:.2f}%\")\n",
        "    print(f\"Actual Average Yield: {actual_yield:.2f}%\")\n",
        "    print(f\"Prediction Error: {abs(mc_results['overall_yield'] - actual_yield):.2f}%\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udd17 KEY RELATIONSHIPS\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Yield vs Threshold Voltage: R\u00b2 = {r2:.3f}\")\n",
        "    print(f\"Statistical Significance: {'Yes' if p_value < 0.05 else 'No'} (p = {p_value:.4f})\")\n",
        "\n",
        "def provide_recommendations():\n",
        "    \"\"\"Provide data-driven recommendations\"\"\"\n",
        "    \n",
        "    print(\"\\n\\n\ud83d\udca1 RECOMMENDATIONS FOR PROCESS IMPROVEMENT\")\n",
        "    print(\"=\" * 55)\n",
        "    \n",
        "    # Capability-based recommendations\n",
        "    print(\"\\n\ud83c\udfaf Process Capability Improvements:\")\n",
        "    for param, results in capability_results.items():\n",
        "        if results['Cpk'] < 1.33:\n",
        "            if not results['Process Centered']:\n",
        "                print(f\"  \ud83d\udccd {param.replace('_', ' ').title()}: Center process (Cpk = {results['Cpk']:.3f})\")\n",
        "            else:\n",
        "                print(f\"  \ud83d\udccd {param.replace('_', ' ').title()}: Reduce variation (Cpk = {results['Cpk']:.3f})\")\n",
        "    \n",
        "    # Tool-specific recommendations\n",
        "    print(\"\\n\ud83d\udd27 Tool Management:\")\n",
        "    if t_test_results['significant']:\n",
        "        print(\"  \ud83d\udccd Investigate tool calibration differences\")\n",
        "        print(\"  \ud83d\udccd Consider tool matching procedures\")\n",
        "    else:\n",
        "        print(\"  \u2705 Tools performing consistently\")\n",
        "    \n",
        "    # Control chart recommendations\n",
        "    print(\"\\n\ud83d\udcca Process Monitoring:\")\n",
        "    if ooc_xbar > 0 or ooc_r > 0:\n",
        "        print(\"  \ud83d\udccd Investigate out-of-control signals\")\n",
        "        print(\"  \ud83d\udccd Review process parameters during OOC periods\")\n",
        "    else:\n",
        "        print(\"  \u2705 Continue current SPC monitoring\")\n",
        "    \n",
        "    # Yield optimization\n",
        "    print(\"\\n\ud83d\udcc8 Yield Optimization:\")\n",
        "    worst_yield_param = min(yield_predictions.keys(), \n",
        "                          key=lambda x: yield_predictions[x]['predicted_yield'])\n",
        "    print(f\"  \ud83d\udccd Focus on {worst_yield_param.replace('_', ' ').title()} optimization\")\n",
        "    print(f\"  \ud83d\udccd Current limiting yield: {yield_predictions[worst_yield_param]['predicted_yield']:.2f}%\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udd2c Further Analysis Recommendations:\")\n",
        "    print(\"  \ud83d\udccd Implement real-time SPC monitoring\")\n",
        "    print(\"  \ud83d\udccd Conduct designed experiments for optimization\")\n",
        "    print(\"  \ud83d\udccd Establish automated capability studies\")\n",
        "    print(\"  \ud83d\udccd Develop predictive yield models\")\n",
        "\n",
        "# Generate summary and recommendations\n",
        "generate_statistical_summary()\n",
        "provide_recommendations()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7275aba7",
      "metadata": {},
      "source": [
        "## 10. Practice Exercises\n",
        "\n",
        "Now it's your turn! Complete these exercises to reinforce your learning:\n",
        "\n",
        "### Exercise 1: Process Capability Study\n",
        "1. Create your own semiconductor dataset with different specification limits\n",
        "2. Calculate Cp and Cpk for your parameters\n",
        "3. Determine if the process is capable and centered\n",
        "\n",
        "### Exercise 2: Hypothesis Testing\n",
        "1. Compare performance between two different process conditions\n",
        "2. Use appropriate statistical tests\n",
        "3. Interpret results and make recommendations\n",
        "\n",
        "### Exercise 3: Control Chart Implementation\n",
        "1. Create control charts for a new parameter\n",
        "2. Simulate some out-of-control conditions\n",
        "3. Detect and analyze the special causes\n",
        "\n",
        "### Exercise 4: Yield Optimization\n",
        "1. Use regression analysis to identify yield drivers\n",
        "2. Predict yield improvement from process changes\n",
        "3. Recommend optimization strategies\n",
        "\n",
        "Try these exercises with your own data or modify the existing dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84880e90",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've completed Module 1.2 on Statistical Foundations for Semiconductor Analysis. \n",
        "\n",
        "### What You've Learned:\n",
        "- \u2705 Descriptive statistics for process characterization\n",
        "- \u2705 Process capability analysis (Cp, Cpk calculations)\n",
        "- \u2705 Distribution analysis and normality testing\n",
        "- \u2705 Hypothesis testing for process validation\n",
        "- \u2705 Statistical process control and control charts\n",
        "- \u2705 Yield prediction using statistical models\n",
        "- \u2705 Regression analysis for process relationships\n",
        "\n",
        "### Key Skills Developed:\n",
        "- Statistical analysis with Python\n",
        "- Process capability assessment\n",
        "- Quality control implementation\n",
        "- Data-driven decision making\n",
        "- Yield optimization strategies\n",
        "\n",
        "### Next Steps:\n",
        "- Module 1.3: Data Manipulation and Preprocessing\n",
        "- Advanced statistical modeling\n",
        "- Machine learning foundations\n",
        "- Process optimization techniques\n",
        "\n",
        "Keep practicing these statistical concepts - they form the foundation for all advanced semiconductor data analysis!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
