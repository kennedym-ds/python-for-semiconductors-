# Module 1.2: Statistics Quick Reference Guide

A concise reference for statistical analysis in semiconductor manufacturing.

## Key Statistical Concepts

### Central Tendency & Variability
```python
import numpy as np
import scipy.stats as stats

# Basic statistics
mean = np.mean(data)
median = np.median(data)
mode = stats.mode(data)[0]

std_dev = np.std(data, ddof=1)    # Sample standard deviation
variance = np.var(data, ddof=1)    # Sample variance
cv = std_dev / mean               # Coefficient of variation
```

### Distribution Testing
```python
# Normality tests
shapiro_stat, shapiro_p = stats.shapiro(data)      # Best for n < 5000
ks_stat, ks_p = stats.kstest(data, 'norm')         # Kolmogorov-Smirnov
ad_stat, ad_crit, ad_sig = stats.anderson(data)    # Anderson-Darling

# Rule of thumb: p > 0.05 suggests normality
```

## Process Capability Analysis

### Capability Indices Formulas
```
Cp  = (USL - LSL) / (6σ)           # Process potential
Cpk = min(CPU, CPL)                # Process capability
CPU = (USL - μ) / (3σ)             # Upper capability
CPL = (μ - LSL) / (3σ)             # Lower capability

where μ = process mean, σ = process standard deviation
```

### Capability Classification
| Cpk Range | Classification | Defect Rate (ppm) |
|-----------|---------------|-------------------|
| ≥ 1.67    | World Class   | < 0.6             |
| 1.33-1.67 | Adequate      | 0.6 - 64          |
| 1.0-1.33  | Marginal      | 64 - 2700         |
| 0.67-1.0  | Poor          | 2700 - 45000      |
| < 0.67    | Unacceptable  | > 45000           |

### Quick Capability Check
```python
from modules.foundation.module-1.statistical_analysis_tools import ProcessCapabilityAnalyzer, SpecificationLimits

# Define specs
specs = SpecificationLimits(lsl=610, usl=690, target=650)

# Analyze capability
analyzer = ProcessCapabilityAnalyzer()
results = analyzer.analyze(data, specs)
print(f"Cpk: {results['capability_indices']['cpk']:.3f}")
```

## Statistical Process Control (SPC)

### Control Chart Constants (Subgroup Size n)
| n | A₂    | D₃   | D₄    | d₂    |
|---|-------|------|-------|-------|
| 2 | 1.880 | 0    | 3.267 | 1.128 |
| 3 | 1.023 | 0    | 2.574 | 1.693 |
| 4 | 0.729 | 0    | 2.282 | 2.059 |
| 5 | 0.577 | 0    | 2.114 | 2.326 |

### Control Limits Formulas
```
X̄-Chart:
  UCL = X̄̄ + A₂R̄
  CL  = X̄̄
  LCL = X̄̄ - A₂R̄

R-Chart:
  UCL = D₄R̄
  CL  = R̄
  LCL = D₃R̄
```

### Western Electric Rules (Out-of-Control Signals)
1. **Rule 1**: 1 point beyond 3σ limits
2. **Rule 2**: 9 consecutive points on same side of centerline
3. **Rule 3**: 6 consecutive increasing or decreasing points
4. **Rule 4**: 14 consecutive alternating up/down points
5. **Rule 5**: 2 of 3 consecutive points in Zone A (2-3σ)
6. **Rule 6**: 4 of 5 consecutive points in Zone B (1-2σ)

### Quick Control Chart Setup
```python
from modules.foundation.module-1.statistical_analysis_tools import ControlChartAnalyzer

# Setup analyzer
analyzer = ControlChartAnalyzer(subgroup_size=5)

# Calculate limits from Phase I data
limits = analyzer.calculate_control_limits(phase1_subgroups)

# Monitor Phase II data
results = analyzer.analyze(phase1_subgroups, phase2_subgroups)
```

## Hypothesis Testing

### Common Tests Decision Tree
```
Comparing Means:
├─ 1 Sample → One-sample t-test
├─ 2 Samples → Two-sample t-test
└─ 3+ Samples → ANOVA

Comparing Distributions:
├─ Normality → Shapiro-Wilk, KS test
├─ Equal Variance → Levene's test
└─ Goodness of Fit → Chi-square, KS test
```

### Test Selection Guide
| Data Type | Sample Size | Test |
|-----------|-------------|------|
| Continuous, Normal | Any | t-test |
| Continuous, Non-normal | Large (n>30) | t-test (CLT) |
| Continuous, Non-normal | Small (n<30) | Mann-Whitney U |
| Categorical | Any | Chi-square |

### Quick Hypothesis Test
```python
from modules.foundation.module-1.statistical_analysis_tools import HypothesisTestAnalyzer

analyzer = HypothesisTestAnalyzer(confidence_level=0.95)

# One-sample test (H₀: μ = target)
result = analyzer.one_sample_ttest(data, target_value)

# Two-sample test (H₀: μ₁ = μ₂)
result = analyzer.two_sample_ttest(group1, group2)

print(f"p-value: {result.p_value:.4f}")
print(f"Result: {result.interpretation}")
```

## Distribution Analysis

### Common Semiconductor Distributions
| Parameter | Typical Distribution | Use Case |
|-----------|---------------------|----------|
| Threshold Voltage | Normal | Process control |
| Leakage Current | Log-normal | Reliability analysis |
| Time to Failure | Weibull | Reliability testing |
| Defect Counts | Poisson | Yield analysis |

### Distribution Parameters
```python
# Normal: stats.norm(loc=μ, scale=σ)
# Log-normal: stats.lognorm(s=σ, scale=exp(μ))
# Weibull: stats.weibull_min(c=β, scale=η)
# Poisson: stats.poisson(μ=λ)
```

### Quick Distribution Fitting
```python
from modules.foundation.module-1.statistical_analysis_tools import DistributionAnalyzer

analyzer = DistributionAnalyzer()

# Compare multiple distributions
results = analyzer.compare_distributions(data)
best_fit = results[0]  # Sorted by AIC (lower is better)

print(f"Best distribution: {best_fit['distribution']}")
print(f"AIC: {best_fit['goodness_of_fit']['aic']:.2f}")
```

## Yield Analysis

### Yield Calculations
```python
# First-pass yield
yield_pct = (passing_units / total_units) * 100

# Defect rate (parts per million)
defect_ppm = ((total_units - passing_units) / total_units) * 1e6

# Theoretical yield (normal distribution)
theoretical_yield = (stats.norm.cdf(usl, μ, σ) - stats.norm.cdf(lsl, μ, σ)) * 100
```

### Yield Prediction Ranges
| Cpk | Predicted Yield | Sigma Level |
|-----|----------------|-------------|
| 2.0 | 99.9997%       | 6σ          |
| 1.67| 99.99%         | 5σ          |
| 1.33| 99.38%         | 4σ          |
| 1.0 | 97.3%          | 3σ          |

### Quick Yield Analysis
```python
from modules.foundation.module-1.statistical_analysis_tools import YieldAnalyzer, SpecificationLimits

analyzer = YieldAnalyzer()
specs = SpecificationLimits(lsl=610, usl=690)

# Current yield
current = analyzer.calculate_current_yield(data, specs)

# Monte Carlo prediction
prediction = analyzer.monte_carlo_yield_simulation(
    mean=np.mean(data), 
    std_dev=np.std(data, ddof=1), 
    specs=specs
)

print(f"Current yield: {current['yield_percent']:.2f}%")
print(f"Predicted yield: {prediction['yield_results']['predicted_yield_percent']:.2f}%")
```

## Common Statistical Formulas

### Sample Size Calculations
```python
# For mean estimation
n = (z_alpha/2 * σ / E)²

# For proportion estimation  
n = (z_alpha/2)² * p * (1-p) / E²

where:
- z_alpha/2: critical value (1.96 for 95% confidence)
- σ: population standard deviation
- E: margin of error
- p: expected proportion
```

### Confidence Intervals
```python
# Mean (normal distribution)
ci = mean ± t_critical * (std_dev / √n)

# Proportion
ci = p ± z_critical * √(p*(1-p)/n)

# Standard deviation (chi-square)
ci = [(n-1)*s²/χ²_upper, (n-1)*s²/χ²_lower]
```

## Measurement System Analysis (MSA)

### Gage R&R Components
```
Total Variation = Equipment Variation + Appraiser Variation + Part Variation

%R&R = 100 * (σ_equipment² + σ_appraiser²)^0.5 / σ_total

Acceptability:
- %R&R < 10%: Excellent
- %R&R 10-30%: Acceptable  
- %R&R > 30%: Unacceptable
```

### MSA Study Setup
- **Parts**: 10 parts spanning full range
- **Appraisers**: 2-3 operators
- **Trials**: 2-3 measurements per part per appraiser
- **Total measurements**: 40-90 measurements

## Quick Data Quality Checks

### Outlier Detection
```python
# IQR method
Q1, Q3 = np.percentile(data, [25, 75])
IQR = Q3 - Q1
outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]

# Z-score method (|z| > 3)
z_scores = np.abs(stats.zscore(data))
outliers = data[z_scores > 3]

# Modified Z-score method (more robust)
median = np.median(data)
mad = stats.median_abs_deviation(data)
modified_z = 0.6745 * (data - median) / mad
outliers = data[np.abs(modified_z) > 3.5]
```

### Data Validation Checklist
- [ ] Check for missing values (`np.isnan()`, `pd.isnull()`)
- [ ] Verify data types (`data.dtype`)
- [ ] Look for outliers (multiple methods)
- [ ] Check measurement resolution/precision
- [ ] Validate against specification limits
- [ ] Assess measurement system capability

## Common Python Libraries

### Essential Imports
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import norm, lognorm, weibull_min
import warnings
warnings.filterwarnings('ignore')
```

### Plotting for Analysis
```python
# Histogram with normal overlay
plt.hist(data, bins=30, density=True, alpha=0.7)
x = np.linspace(data.min(), data.max(), 100)
plt.plot(x, stats.norm.pdf(x, data.mean(), data.std()))

# Q-Q plot for normality
stats.probplot(data, dist="norm", plot=plt)

# Control chart
plt.plot(subgroup_means, 'b.-')
plt.axhline(y=ucl, color='r', linestyle='--', label='UCL')
plt.axhline(y=lcl, color='r', linestyle='--', label='LCL')
plt.axhline(y=centerline, color='g', linestyle='-', label='CL')
```

## Troubleshooting Guide

### Common Issues & Solutions

| Problem | Possible Cause | Solution |
|---------|---------------|----------|
| Non-normal data | Skewness, outliers | Transform data, use non-parametric tests |
| High variability | Measurement error, process instability | Conduct MSA, implement SPC |
| Low capability | Process not centered, high variation | Adjust process mean, reduce variation |
| False alarms | Over-sensitive control limits | Verify control chart setup, check data |

### Data Transformation
```python
# Log transformation (for right-skewed data)
log_data = np.log(data)

# Square root transformation
sqrt_data = np.sqrt(data)

# Box-Cox transformation (automated)
transformed_data, lambda_param = stats.boxcox(data)

# Inverse transformation
inv_data = 1 / data
```

## Industry Standards & References

### Key Standards
- **SEMI E10**: Specification for Definition and Measurement of Equipment Reliability
- **JEDEC JESD94**: Application Specific Qualification Using Knowledge Based Test Methodology
- **ISO 13053**: Quantitative methods in process improvement - Six Sigma
- **ASTM E2281**: Standard Practice for Process Capability Studies

### Recommended Reading
- Montgomery, D.C. "Introduction to Statistical Quality Control"
- Wheeler, D.J. "Understanding Variation: The Key to Managing Chaos"
- NIST/SEMATECH e-Handbook of Statistical Methods

---

## Quick Reference Card

### Most Common Analyses
1. **Process Capability**: Cp, Cpk calculation
2. **Control Charts**: X̄-R charts with Western Electric rules
3. **Normality Test**: Shapiro-Wilk or Anderson-Darling
4. **Hypothesis Test**: One-sample or two-sample t-test
5. **Yield Calculation**: Current and predicted yield

### Critical Values (α = 0.05)
- **Z-critical**: ±1.96 (normal distribution)
- **t-critical**: Depends on degrees of freedom
- **Chi-square**: Depends on degrees of freedom
- **F-critical**: Depends on numerator and denominator df

### Emergency Contacts
- Process Engineering: For capability issues
- Quality Assurance: For control chart violations  
- Metrology: For measurement system problems
- Reliability: For distribution fitting and lifetime analysis

---

*This quick reference guide provides essential formulas, decision trees, and code snippets for statistical analysis in semiconductor manufacturing. Keep this handy for day-to-day analysis tasks.*
