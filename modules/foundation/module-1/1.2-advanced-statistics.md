# Module 1.2: Advanced Statistical Theory for Semiconductor Analysis

## Table of Contents

1. [Statistical Theory Foundations](#statistical-theory-foundations)
2. [Measurement System Analysis](#measurement-system-analysis)
3. [Advanced Statistical Methods](#advanced-statistical-methods)
4. [Process Control Theory](#process-control-theory)
5. [Distribution Theory in Semiconductors](#distribution-theory-in-semiconductors)
6. [Design of Experiments](#design-of-experiments)
7. [Multivariate Statistics](#multivariate-statistics)
8. [Time Series Analysis](#time-series-analysis)
9. [Reliability Statistics](#reliability-statistics)
10. [Implementation Guidelines](#implementation-guidelines)

---

## Statistical Theory Foundations

### Central Limit Theorem in Semiconductor Manufacturing

The Central Limit Theorem (CLT) is fundamental to semiconductor process control and capability analysis. It states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the underlying population distribution.

#### Mathematical Foundation

For a population with mean μ and finite variance σ², the sampling distribution of the sample mean x̄ has:

- Mean: E[x̄] = μ
- Variance: Var[x̄] = σ²/n
- Standard Error: SE[x̄] = σ/√n

#### Application in Semiconductor Testing

**Wafer-Level Sampling**: When measuring electrical parameters across a wafer:
- Individual die measurements may follow various distributions
- Average measurements across subgroups (typically 5-25 die) approach normality
- This enables the use of parametric statistical methods

**Example**: Critical Dimension Measurements
```python
# Demonstration of CLT with CD measurements
import numpy as np
from scipy.stats import lognorm, norm
import matplotlib.pyplot as plt

# Simulate individual die CD measurements (log-normal distribution)
np.random.seed(42)
shape = 0.1  # Shape parameter for log-normal
scale = 100  # Scale parameter (target CD)

# Generate population data
population_size = 10000
population_data = lognorm.rvs(s=shape, scale=scale, size=population_size)

# Sampling experiment
sample_sizes = [1, 5, 10, 25, 50]
n_samples = 1000

for n in sample_sizes:
    sample_means = []
    for _ in range(n_samples):
        sample = np.random.choice(population_data, size=n, replace=False)
        sample_means.append(np.mean(sample))

    # Test normality
    from scipy.stats import shapiro
    _, p_value = shapiro(sample_means[:500])  # Limit to 500 for shapiro test

    print(f"Sample size {n}: Shapiro p-value = {p_value:.4f}")
    print(f"  Mean: {np.mean(sample_means):.2f} nm")
    print(f"  Std:  {np.std(sample_means):.2f} nm")
    print(f"  Theoretical SE: {np.std(population_data)/np.sqrt(n):.2f} nm\n")
```

### Sampling Theory for Wafer Testing

Semiconductor manufacturing requires strategic sampling due to the high cost and time associated with comprehensive testing.

#### Sampling Strategies

**1. Random Sampling**
- Each die has equal probability of selection
- Provides unbiased estimates of population parameters
- Suitable for general process monitoring

**2. Systematic Sampling**
- Every kth die is selected
- Efficient for wafer-level patterns
- Risk: May miss periodic variation

**3. Stratified Sampling**
- Wafer divided into regions (center, edge, etc.)
- Ensures representation from all areas
- Improves precision for heterogeneous wafers

**4. Cluster Sampling**
- Groups of adjacent die are selected
- Reduces test time and cost
- May increase sampling error due to spatial correlation

#### Optimal Sample Size Determination

The required sample size depends on:
- Desired confidence level (typically 95%)
- Acceptable margin of error
- Population variance estimate

**Formula for mean estimation**:
```
n = (Z_{α/2} × σ / E)²
```

Where:
- Z_{α/2} = critical value for confidence level
- σ = population standard deviation
- E = desired margin of error

**Example Calculation**:
```python
from scipy.stats import norm

def calculate_sample_size(confidence_level, margin_error, std_dev):
    """Calculate required sample size for mean estimation"""
    alpha = 1 - confidence_level
    z_critical = norm.ppf(1 - alpha/2)

    n = (z_critical * std_dev / margin_error) ** 2
    return int(np.ceil(n))

# Example: Threshold voltage measurement
confidence = 0.95
margin_error = 2.0  # ±2 mV
estimated_std = 20.0  # 20 mV

required_n = calculate_sample_size(confidence, margin_error, estimated_std)
print(f"Required sample size: {required_n} die per wafer")
```

### Statistical Significance in Process Control

Understanding p-values and statistical significance is crucial for making correct decisions in process control.

#### Type I and Type II Errors

**Type I Error (α)**:
- False positive: Concluding process has changed when it hasn't
- Consequence: Unnecessary process adjustments, increased costs
- Typical α = 0.05 in semiconductor manufacturing

**Type II Error (β)**:
- False negative: Missing actual process changes
- Consequence: Defective products, yield loss
- Power = 1 - β (typically aim for power ≥ 0.80)

#### Power Analysis

Power analysis determines the probability of detecting a true process change of a specified magnitude.

```python
from scipy.stats import norm

def calculate_detection_power(effect_size, sample_size, alpha=0.05):
    """Calculate power to detect a process shift"""
    z_alpha = norm.ppf(1 - alpha/2)
    z_beta = z_alpha - effect_size * np.sqrt(sample_size)
    power = 1 - norm.cdf(z_beta)
    return power

# Example: Power to detect 1-sigma shift in process mean
effect_sizes = np.arange(0.5, 3.0, 0.1)
sample_sizes = [5, 10, 25, 50]

for n in sample_sizes:
    powers = [calculate_detection_power(es, n) for es in effect_sizes]
    plt.plot(effect_sizes, powers, label=f'n = {n}')

plt.xlabel('Effect Size (σ units)')
plt.ylabel('Power')
plt.title('Power Curves for Different Sample Sizes')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## Measurement System Analysis

Measurement System Analysis (MSA) ensures that measurement systems are adequate for their intended use and that measurement error is minimized relative to process variation.

### Components of Measurement Variation

Total observed variation consists of:
1. **Part-to-part variation** (actual process variation)
2. **Measurement system variation**
   - Repeatability (equipment variation)
   - Reproducibility (operator/environmental variation)
   - Bias (systematic error)
   - Linearity (consistency across range)
   - Stability (consistency over time)

### Gage R&R Studies

Gage Repeatability and Reproducibility studies quantify measurement system variation.

#### Mathematical Model

The total variance can be decomposed as:
```
σ²_total = σ²_part + σ²_measurement
σ²_measurement = σ²_repeatability + σ²_reproducibility
```

#### Implementation

```python
import pandas as pd
from scipy.stats import f

def gage_rr_analysis(data, parts, operators, measurements):
    """
    Perform Gage R&R analysis

    Parameters:
    data: DataFrame with columns ['Part', 'Operator', 'Measurement']
    parts: number of parts
    operators: number of operators
    measurements: number of repeat measurements
    """

    # Calculate means
    grand_mean = data['Measurement'].mean()
    part_means = data.groupby('Part')['Measurement'].mean()
    operator_means = data.groupby('Operator')['Measurement'].mean()

    # Calculate sum of squares
    total_ss = ((data['Measurement'] - grand_mean) ** 2).sum()

    part_ss = measurements * operators * ((part_means - grand_mean) ** 2).sum()
    operator_ss = measurements * parts * ((operator_means - grand_mean) ** 2).sum()

    # Calculate repeatability (within operator variation)
    repeatability_ss = 0
    for part in data['Part'].unique():
        for operator in data['Operator'].unique():
            subset = data[(data['Part'] == part) & (data['Operator'] == operator)]
            if len(subset) > 1:
                subset_mean = subset['Measurement'].mean()
                repeatability_ss += ((subset['Measurement'] - subset_mean) ** 2).sum()

    # Degrees of freedom
    df_total = len(data) - 1
    df_part = parts - 1
    df_operator = operators - 1
    df_repeatability = parts * operators * (measurements - 1)
    df_reproducibility = df_operator

    # Mean squares
    ms_part = part_ss / df_part
    ms_operator = operator_ss / df_operator
    ms_repeatability = repeatability_ss / df_repeatability

    # Variance components
    var_repeatability = ms_repeatability
    var_reproducibility = max(0, (ms_operator - ms_repeatability) / (parts * measurements))
    var_part = max(0, (ms_part - ms_repeatability) / (operators * measurements))

    var_measurement = var_repeatability + var_reproducibility
    var_total = var_part + var_measurement

    # Calculate %R&R
    if var_total > 0:
        percent_rr = 100 * var_measurement / var_total
        percent_repeatability = 100 * var_repeatability / var_total
        percent_reproducibility = 100 * var_reproducibility / var_total
        percent_part = 100 * var_part / var_total
    else:
        percent_rr = percent_repeatability = percent_reproducibility = percent_part = 0

    # Discrimination ratio
    ndc = 1.41 * np.sqrt(var_part) / np.sqrt(var_measurement) if var_measurement > 0 else float('inf')

    return {
        'percent_rr': percent_rr,
        'percent_repeatability': percent_repeatability,
        'percent_reproducibility': percent_reproducibility,
        'percent_part': percent_part,
        'ndc': ndc,
        'var_components': {
            'total': var_total,
            'part': var_part,
            'measurement': var_measurement,
            'repeatability': var_repeatability,
            'reproducibility': var_reproducibility
        }
    }

# Generate example data
np.random.seed(42)
parts = 10
operators = 3
measurements = 3

data_list = []
for part in range(1, parts + 1):
    part_true_value = np.random.normal(100, 5)  # True part dimension

    for operator in range(1, operators + 1):
        operator_bias = np.random.normal(0, 1)  # Operator effect

        for measurement in range(1, measurements + 1):
            measured_value = (part_true_value +
                            operator_bias +
                            np.random.normal(0, 0.5))  # Repeatability error

            data_list.append({
                'Part': part,
                'Operator': operator,
                'Measurement': measured_value
            })

gage_data = pd.DataFrame(data_list)
results = gage_rr_analysis(gage_data, parts, operators, measurements)

print("Gage R&R Analysis Results:")
print(f"Total R&R: {results['percent_rr']:.1f}%")
print(f"Repeatability: {results['percent_repeatability']:.1f}%")
print(f"Reproducibility: {results['percent_reproducibility']:.1f}%")
print(f"Part-to-Part: {results['percent_part']:.1f}%")
print(f"Number of Distinct Categories: {results['ndc']:.1f}")

# Interpretation guidelines
if results['percent_rr'] < 10:
    print("✅ Measurement system is acceptable")
elif results['percent_rr'] < 30:
    print("⚠️ Measurement system may be acceptable")
else:
    print("❌ Measurement system is not acceptable")
```

### Bias and Linearity Studies

#### Bias Study
Bias is the difference between observed average measurements and reference values.

```python
def bias_study(measured_values, reference_value, confidence_level=0.95):
    """Perform bias study analysis"""
    n = len(measured_values)
    mean_measured = np.mean(measured_values)
    std_measured = np.std(measured_values, ddof=1)

    bias = mean_measured - reference_value
    se_bias = std_measured / np.sqrt(n)

    # Confidence interval for bias
    alpha = 1 - confidence_level
    t_critical = stats.t.ppf(1 - alpha/2, n - 1)

    ci_lower = bias - t_critical * se_bias
    ci_upper = bias + t_critical * se_bias

    # Statistical significance test
    t_stat = bias / se_bias
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 1))

    return {
        'bias': bias,
        'bias_percent': 100 * bias / reference_value,
        'confidence_interval': (ci_lower, ci_upper),
        'p_value': p_value,
        'significant': p_value < (1 - confidence_level)
    }

# Example bias study
reference = 100.0  # nm
measurements = np.random.normal(100.2, 0.5, 30)  # Simulated measurements with bias

bias_results = bias_study(measurements, reference)
print(f"Bias: {bias_results['bias']:.3f} nm ({bias_results['bias_percent']:.2f}%)")
print(f"95% CI: [{bias_results['confidence_interval'][0]:.3f}, {bias_results['confidence_interval'][1]:.3f}]")
print(f"Significant bias: {'Yes' if bias_results['significant'] else 'No'} (p = {bias_results['p_value']:.4f})")
```

---

## Advanced Statistical Methods

### Non-Parametric Methods for Skewed Data

Semiconductor data often exhibits non-normal distributions, requiring non-parametric methods that don't assume specific distributions.

#### Mann-Whitney U Test

Alternative to two-sample t-test for non-normal data:

```python
from scipy.stats import mannwhitneyu

def mann_whitney_analysis(group1, group2, alternative='two-sided'):
    """Perform Mann-Whitney U test"""
    statistic, p_value = mannwhitneyu(group1, group2, alternative=alternative)

    # Effect size (rank-biserial correlation)
    n1, n2 = len(group1), len(group2)
    effect_size = 1 - (2 * statistic) / (n1 * n2)

    return {
        'statistic': statistic,
        'p_value': p_value,
        'effect_size': effect_size,
        'median_diff': np.median(group2) - np.median(group1)
    }

# Example: Compare leakage current between two tools
tool1_leakage = np.random.lognormal(np.log(10), 0.3, 50)
tool2_leakage = np.random.lognormal(np.log(12), 0.3, 50)

mw_results = mann_whitney_analysis(tool1_leakage, tool2_leakage)
print(f"Mann-Whitney U test:")
print(f"p-value: {mw_results['p_value']:.4f}")
print(f"Effect size: {mw_results['effect_size']:.4f}")
print(f"Median difference: {mw_results['median_diff']:.2f} nA")
```

#### Kruskal-Wallis Test

Non-parametric alternative to ANOVA:

```python
from scipy.stats import kruskal

def kruskal_wallis_analysis(*groups):
    """Perform Kruskal-Wallis test for multiple groups"""
    statistic, p_value = kruskal(*groups)

    # Calculate eta-squared (effect size)
    n_total = sum(len(group) for group in groups)
    eta_squared = (statistic - len(groups) + 1) / (n_total - len(groups))

    return {
        'statistic': statistic,
        'p_value': p_value,
        'eta_squared': eta_squared,
        'df': len(groups) - 1
    }

# Example: Compare yield across multiple lots
lot1_yield = np.random.beta(18, 2, 30) * 100
lot2_yield = np.random.beta(19, 2, 30) * 100
lot3_yield = np.random.beta(17, 2, 30) * 100

kw_results = kruskal_wallis_analysis(lot1_yield, lot2_yield, lot3_yield)
print(f"Kruskal-Wallis test:")
print(f"H-statistic: {kw_results['statistic']:.4f}")
print(f"p-value: {kw_results['p_value']:.4f}")
print(f"Effect size (η²): {kw_results['eta_squared']:.4f}")
```

### Bootstrap Methods for Small Sample Sizes

Bootstrap resampling provides robust confidence intervals and hypothesis tests for small samples or complex statistics.

```python
def bootstrap_confidence_interval(data, statistic_func, n_bootstrap=10000, confidence_level=0.95):
    """
    Calculate bootstrap confidence interval for any statistic

    Parameters:
    data: array-like, sample data
    statistic_func: function that calculates the statistic
    n_bootstrap: number of bootstrap samples
    confidence_level: confidence level for interval
    """
    n = len(data)
    bootstrap_stats = []

    np.random.seed(42)
    for _ in range(n_bootstrap):
        bootstrap_sample = np.random.choice(data, size=n, replace=True)
        bootstrap_stats.append(statistic_func(bootstrap_sample))

    bootstrap_stats = np.array(bootstrap_stats)

    alpha = 1 - confidence_level
    lower_percentile = 100 * (alpha / 2)
    upper_percentile = 100 * (1 - alpha / 2)

    ci_lower = np.percentile(bootstrap_stats, lower_percentile)
    ci_upper = np.percentile(bootstrap_stats, upper_percentile)

    return {
        'original_statistic': statistic_func(data),
        'bootstrap_mean': np.mean(bootstrap_stats),
        'bootstrap_std': np.std(bootstrap_stats),
        'confidence_interval': (ci_lower, ci_upper),
        'bootstrap_distribution': bootstrap_stats
    }

# Example: Bootstrap confidence interval for Cpk
def calculate_cpk(data, lsl=95, usl=105):
    """Calculate Cpk for given data and specifications"""
    mean = np.mean(data)
    std = np.std(data, ddof=1)
    cpu = (usl - mean) / (3 * std)
    cpl = (mean - lsl) / (3 * std)
    return min(cpu, cpl)

# Simulate small sample of CD measurements
cd_data = np.random.normal(100, 2, 20)

bootstrap_cpk = bootstrap_confidence_interval(
    cd_data,
    lambda x: calculate_cpk(x, 95, 105),
    n_bootstrap=10000
)

print(f"Bootstrap Analysis for Cpk:")
print(f"Original Cpk: {bootstrap_cpk['original_statistic']:.4f}")
print(f"Bootstrap mean: {bootstrap_cpk['bootstrap_mean']:.4f}")
print(f"Bootstrap std: {bootstrap_cpk['bootstrap_std']:.4f}")
print(f"95% CI: [{bootstrap_cpk['confidence_interval'][0]:.4f}, {bootstrap_cpk['confidence_interval'][1]:.4f}]")

# Test if Cpk > 1.33 (capability requirement)
capability_threshold = 1.33
p_value = np.mean(bootstrap_cpk['bootstrap_distribution'] <= capability_threshold)
print(f"P(Cpk ≤ 1.33): {p_value:.4f}")
```

---

## Process Control Theory

### Shewhart Control Chart Principles

Walter Shewhart's fundamental principles for statistical process control:

1. **Common Cause vs. Special Cause Variation**
   - Common cause: Natural, inherent process variation
   - Special cause: Assignable, unusual variation requiring investigation

2. **Control Limits vs. Specification Limits**
   - Control limits: ±3σ from process centerline (voice of the process)
   - Specification limits: Customer requirements (voice of the customer)

3. **Rational Subgrouping**
   - Minimize variation within subgroups
   - Maximize opportunity for variation between subgroups

#### Statistical Basis

For normally distributed data with known σ:
- P(point outside 3σ limits) = 0.0027
- Expected false alarm rate: 1 in 370 points

#### Control Chart Constants

For subgroup size n, the control chart constants are:

| n | A₂ | D₃ | D₄ | d₂ | c₄ |
|---|----|----|----|----|----|
| 2 | 1.880 | 0 | 3.267 | 1.128 | 0.7979 |
| 3 | 1.023 | 0 | 2.574 | 1.693 | 0.8862 |
| 4 | 0.729 | 0 | 2.282 | 2.059 | 0.9213 |
| 5 | 0.577 | 0 | 2.114 | 2.326 | 0.9400 |
| 10 | 0.308 | 0.223 | 1.777 | 3.078 | 0.9727 |

```python
def get_control_chart_constants(n):
    """Get control chart constants for subgroup size n"""
    constants = {
        2: {'A2': 1.880, 'D3': 0, 'D4': 3.267, 'd2': 1.128, 'c4': 0.7979},
        3: {'A2': 1.023, 'D3': 0, 'D4': 2.574, 'd2': 1.693, 'c4': 0.8862},
        4: {'A2': 0.729, 'D3': 0, 'D4': 2.282, 'd2': 2.059, 'c4': 0.9213},
        5: {'A2': 0.577, 'D3': 0, 'D4': 2.114, 'd2': 2.326, 'c4': 0.9400},
        10: {'A2': 0.308, 'D3': 0.223, 'D4': 1.777, 'd2': 3.078, 'c4': 0.9727}
    }
    return constants.get(n, None)

class ShewhartControlChart:
    """Implementation of Shewhart X-bar and R charts"""

    def __init__(self, subgroup_size):
        self.n = subgroup_size
        self.constants = get_control_chart_constants(subgroup_size)
        if self.constants is None:
            raise ValueError(f"Constants not available for subgroup size {subgroup_size}")

    def calculate_limits(self, data):
        """Calculate control limits from phase I data"""
        # Assuming data is a list of subgroups
        subgroup_means = [np.mean(subgroup) for subgroup in data]
        subgroup_ranges = [np.max(subgroup) - np.min(subgroup) for subgroup in data]

        # X-bar chart
        xbar_bar = np.mean(subgroup_means)
        r_bar = np.mean(subgroup_ranges)

        xbar_ucl = xbar_bar + self.constants['A2'] * r_bar
        xbar_lcl = xbar_bar - self.constants['A2'] * r_bar

        # R chart
        r_ucl = self.constants['D4'] * r_bar
        r_lcl = self.constants['D3'] * r_bar

        return {
            'xbar': {'center': xbar_bar, 'ucl': xbar_ucl, 'lcl': xbar_lcl},
            'r': {'center': r_bar, 'ucl': r_ucl, 'lcl': r_lcl}
        }

    def detect_out_of_control(self, values, limits):
        """Detect out-of-control points using Western Electric rules"""
        center = limits['center']
        ucl = limits['ucl']
        lcl = limits['lcl']

        # Calculate zones
        zone_a_upper = center + 2 * (ucl - center) / 3
        zone_a_lower = center - 2 * (center - lcl) / 3
        zone_b_upper = center + (ucl - center) / 3
        zone_b_lower = center - (center - lcl) / 3

        signals = []

        for i, value in enumerate(values):
            # Rule 1: Point beyond control limits
            if value > ucl or value < lcl:
                signals.append((i, 'Rule 1: Beyond control limits'))

            # Rule 2: 9 consecutive points on same side of center
            if i >= 8:
                if all(v > center for v in values[i-8:i+1]) or all(v < center for v in values[i-8:i+1]):
                    signals.append((i, 'Rule 2: 9 consecutive points on one side'))

            # Rule 3: 6 consecutive increasing or decreasing points
            if i >= 5:
                diffs = [values[j+1] - values[j] for j in range(i-4, i)]
                if all(d > 0 for d in diffs) or all(d < 0 for d in diffs):
                    signals.append((i, 'Rule 3: 6 consecutive trending points'))

            # Rule 4: 14 consecutive alternating points
            if i >= 13:
                alternating = True
                for j in range(i-12, i):
                    if (values[j+1] - center) * (values[j] - center) >= 0:
                        alternating = False
                        break
                if alternating:
                    signals.append((i, 'Rule 4: 14 consecutive alternating points'))

            # Rule 5: 2 of 3 consecutive points in Zone A
            if i >= 2:
                zone_a_count = sum(1 for v in values[i-2:i+1]
                                 if v > zone_a_upper or v < zone_a_lower)
                if zone_a_count >= 2:
                    signals.append((i, 'Rule 5: 2 of 3 points in Zone A'))

            # Rule 6: 4 of 5 consecutive points in Zone B or beyond
            if i >= 4:
                zone_b_count = sum(1 for v in values[i-4:i+1]
                                 if v > zone_b_upper or v < zone_b_lower)
                if zone_b_count >= 4:
                    signals.append((i, 'Rule 6: 4 of 5 points in Zone B or beyond'))

        return signals

# Example usage
np.random.seed(42)
# Generate phase I data (in-control)
phase1_data = []
for _ in range(25):
    subgroup = np.random.normal(100, 2, 5)
    phase1_data.append(subgroup)

chart = ShewhartControlChart(5)
limits = chart.calculate_limits(phase1_data)

print("Control Chart Limits:")
print(f"X-bar chart: {limits['xbar']['lcl']:.2f} - {limits['xbar']['center']:.2f} - {limits['xbar']['ucl']:.2f}")
print(f"R chart: {limits['r']['lcl']:.2f} - {limits['r']['center']:.2f} - {limits['r']['ucl']:.2f}")

# Generate phase II data with a process shift
phase2_means = []
for i in range(30):
    if i < 15:
        # In control
        subgroup = np.random.normal(100, 2, 5)
    else:
        # Process shift
        subgroup = np.random.normal(102, 2, 5)
    phase2_means.append(np.mean(subgroup))

# Detect out-of-control signals
signals = chart.detect_out_of_control(phase2_means, limits['xbar'])
print(f"\nOut-of-control signals detected: {len(signals)}")
for point, rule in signals:
    print(f"Point {point}: {rule}")
```

### CUSUM and EWMA Charts

#### CUSUM (Cumulative Sum) Charts

CUSUM charts are more sensitive to small process shifts than Shewhart charts.

```python
class CUSUMChart:
    """CUSUM chart implementation"""

    def __init__(self, target, sigma, h=5, k=0.5):
        """
        Initialize CUSUM chart

        Parameters:
        target: target value (μ₀)
        sigma: process standard deviation
        h: decision interval (typically 4-5)
        k: reference value (typically 0.5-1)
        """
        self.target = target
        self.sigma = sigma
        self.h = h
        self.k = k * sigma  # Convert to actual units
        self.ucl = h * sigma
        self.lcl = -h * sigma

    def calculate_cusum(self, data):
        """Calculate CUSUM statistics"""
        c_plus = [0]  # Upper CUSUM
        c_minus = [0]  # Lower CUSUM

        for i, x in enumerate(data):
            # Upper CUSUM (detects upward shifts)
            c_plus_i = max(0, c_plus[i] + (x - self.target) - self.k)
            c_plus.append(c_plus_i)

            # Lower CUSUM (detects downward shifts)
            c_minus_i = min(0, c_minus[i] + (x - self.target) + self.k)
            c_minus.append(c_minus_i)

        return c_plus[1:], c_minus[1:]  # Remove initial zeros

    def detect_signals(self, c_plus, c_minus):
        """Detect out-of-control signals"""
        signals = []

        for i, (cp, cm) in enumerate(zip(c_plus, c_minus)):
            if cp > self.ucl:
                signals.append((i, f'Upper CUSUM signal: {cp:.2f} > {self.ucl:.2f}'))
            if cm < self.lcl:
                signals.append((i, f'Lower CUSUM signal: {cm:.2f} < {self.lcl:.2f}'))

        return signals

# Example CUSUM implementation
target = 100
sigma = 2
cusum_chart = CUSUMChart(target, sigma)

# Generate data with small shift
np.random.seed(42)
data = []
for i in range(50):
    if i < 25:
        data.append(np.random.normal(100, 2))
    else:
        data.append(np.random.normal(101, 2))  # 0.5σ shift

c_plus, c_minus = cusum_chart.calculate_cusum(data)
signals = cusum_chart.detect_signals(c_plus, c_minus)

print(f"CUSUM Chart Analysis:")
print(f"Signals detected: {len(signals)}")
for point, message in signals:
    print(f"Point {point}: {message}")
```

#### EWMA (Exponentially Weighted Moving Average) Charts

```python
class EWMAChart:
    """EWMA chart implementation"""

    def __init__(self, target, sigma, lambda_param=0.2, L=3):
        """
        Initialize EWMA chart

        Parameters:
        target: target value
        sigma: process standard deviation
        lambda_param: smoothing parameter (0 < λ ≤ 1)
        L: width of control limits (typically 3)
        """
        self.target = target
        self.sigma = sigma
        self.lambda_param = lambda_param
        self.L = L

    def calculate_ewma(self, data):
        """Calculate EWMA statistics and control limits"""
        z = [self.target]  # Initialize with target
        sigmas = [0]  # Standard deviation of z_i

        for i, x in enumerate(data, 1):
            # EWMA statistic
            z_i = self.lambda_param * x + (1 - self.lambda_param) * z[i-1]
            z.append(z_i)

            # Standard deviation of z_i
            sigma_z = self.sigma * np.sqrt(
                self.lambda_param * (1 - (1 - self.lambda_param)**(2 * i)) /
                (2 - self.lambda_param)
            )
            sigmas.append(sigma_z)

        # Control limits (vary with time for first few points)
        ucl = [self.target + self.L * s for s in sigmas]
        lcl = [self.target - self.L * s for s in sigmas]

        return z[1:], ucl[1:], lcl[1:]  # Remove initial values

    def detect_signals(self, z, ucl, lcl):
        """Detect out-of-control signals"""
        signals = []

        for i, (zi, ucl_i, lcl_i) in enumerate(zip(z, ucl, lcl)):
            if zi > ucl_i or zi < lcl_i:
                signals.append((i, f'EWMA signal: {zi:.2f} outside [{lcl_i:.2f}, {ucl_i:.2f}]'))

        return signals

# Example EWMA implementation
ewma_chart = EWMAChart(target=100, sigma=2, lambda_param=0.2)
z, ucl, lcl = ewma_chart.calculate_ewma(data)
ewma_signals = ewma_chart.detect_signals(z, ucl, lcl)

print(f"\nEWMA Chart Analysis:")
print(f"Signals detected: {len(ewma_signals)}")
for point, message in ewma_signals:
    print(f"Point {point}: {message}")
```

---

## Distribution Theory in Semiconductors

### Common Distributions in Semiconductor Manufacturing

#### 1. Normal Distribution
**Applications**: Most parametric measurements when properly controlled
- Threshold voltages
- Critical dimensions (after process optimization)
- Resistance measurements

**Characteristics**:
- Symmetric, bell-shaped
- Completely described by mean (μ) and standard deviation (σ)
- 68-95-99.7 rule applies

#### 2. Log-Normal Distribution
**Applications**: Parameters that are products of many factors
- Leakage currents
- Contact resistance
- Particle sizes
- Reliability lifetimes

**Mathematical relationship**:
If ln(X) ~ N(μ, σ²), then X ~ LogNormal(μ, σ²)

```python
from scipy.stats import lognorm

def analyze_lognormal_data(data):
    """Analyze data assuming log-normal distribution"""
    # Fit log-normal distribution
    s, loc, scale = lognorm.fit(data, floc=0)  # floc=0 for 2-parameter fit

    # Parameters
    mu = np.log(scale)  # Mean of log(X)
    sigma = s  # Std dev of log(X)

    # Statistics
    geometric_mean = scale
    median = np.exp(mu)
    mode = np.exp(mu - sigma**2)
    arithmetic_mean = np.exp(mu + sigma**2/2)

    # Confidence intervals
    ci_95_lower = lognorm.ppf(0.025, s, scale=scale)
    ci_95_upper = lognorm.ppf(0.975, s, scale=scale)

    return {
        'parameters': {'mu': mu, 'sigma': sigma, 'scale': scale, 's': s},
        'statistics': {
            'geometric_mean': geometric_mean,
            'median': median,
            'mode': mode,
            'arithmetic_mean': arithmetic_mean
        },
        'ci_95': (ci_95_lower, ci_95_upper)
    }

# Example: Leakage current analysis
np.random.seed(42)
leakage_data = lognorm.rvs(s=0.5, scale=10, size=1000)

lognorm_analysis = analyze_lognormal_data(leakage_data)
print("Log-Normal Distribution Analysis:")
print(f"Geometric mean: {lognorm_analysis['statistics']['geometric_mean']:.2f} nA")
print(f"Median: {lognorm_analysis['statistics']['median']:.2f} nA")
print(f"Mode: {lognorm_analysis['statistics']['mode']:.2f} nA")
print(f"Arithmetic mean: {lognorm_analysis['statistics']['arithmetic_mean']:.2f} nA")
print(f"95% CI: [{lognorm_analysis['ci_95'][0]:.2f}, {lognorm_analysis['ci_95'][1]:.2f}] nA")
```

#### 3. Weibull Distribution
**Applications**: Reliability and lifetime analysis
- Time to failure
- Breakdown voltages
- Wear-out mechanisms

**Parameters**:
- β (shape): Failure rate behavior
  - β < 1: Decreasing failure rate (infant mortality)
  - β = 1: Constant failure rate (random failures)
  - β > 1: Increasing failure rate (wear-out)
- η (scale): Characteristic lifetime
- γ (location): Minimum lifetime (often 0)

```python
from scipy.stats import weibull_min

def weibull_reliability_analysis(failure_times):
    """Analyze failure data using Weibull distribution"""
    # Fit Weibull distribution
    c, loc, scale = weibull_min.fit(failure_times, floc=0)

    # Parameters
    beta = c  # Shape parameter
    eta = scale  # Scale parameter (characteristic life)

    # Key reliability metrics
    mttf = eta * stats.gamma(1 + 1/beta)  # Mean time to failure
    median_life = eta * (np.log(2))**(1/beta)

    # Reliability function
    def reliability(t):
        return np.exp(-(t/eta)**beta)

    # Failure rate function
    def hazard_rate(t):
        return (beta/eta) * (t/eta)**(beta-1)

    return {
        'parameters': {'beta': beta, 'eta': eta},
        'mttf': mttf,
        'median_life': median_life,
        'reliability_func': reliability,
        'hazard_func': hazard_rate
    }

# Example: Device lifetime analysis
np.random.seed(42)
failure_times = weibull_min.rvs(c=2, scale=1000, size=100)  # Hours

weibull_analysis = weibull_reliability_analysis(failure_times)
print(f"\nWeibull Reliability Analysis:")
print(f"Shape parameter (β): {weibull_analysis['parameters']['beta']:.2f}")
print(f"Scale parameter (η): {weibull_analysis['parameters']['eta']:.1f} hours")
print(f"MTTF: {weibull_analysis['mttf']:.1f} hours")
print(f"Median life: {weibull_analysis['median_life']:.1f} hours")
print(f"Reliability at 1000 hours: {weibull_analysis['reliability_func'](1000):.3f}")
```

#### 4. Poisson Distribution
**Applications**: Count data
- Defect counts per unit area
- Particle counts
- Failure counts in reliability testing

```python
from scipy.stats import poisson

def poisson_process_analysis(counts):
    """Analyze count data using Poisson distribution"""
    # Estimate lambda (mean count rate)
    lambda_est = np.mean(counts)

    # Confidence interval for lambda
    n = len(counts)
    total_counts = np.sum(counts)

    # Using chi-square distribution
    alpha = 0.05
    ci_lower = stats.chi2.ppf(alpha/2, 2*total_counts) / (2*n)
    ci_upper = stats.chi2.ppf(1-alpha/2, 2*total_counts + 2) / (2*n)

    # Goodness of fit test
    expected_probs = [poisson.pmf(k, lambda_est) for k in range(max(counts)+1)]
    observed_counts = [np.sum(counts == k) for k in range(max(counts)+1)]
    expected_counts = [p * n for p in expected_probs]

    # Chi-square test (combine low-frequency categories)
    valid_categories = [i for i, exp in enumerate(expected_counts) if exp >= 5]
    if len(valid_categories) < len(expected_counts):
        # Combine tail categories
        obs_combined = observed_counts[:valid_categories[-1]] + [sum(observed_counts[valid_categories[-1]:])]
        exp_combined = expected_counts[:valid_categories[-1]] + [sum(expected_counts[valid_categories[-1]:])]
    else:
        obs_combined = observed_counts
        exp_combined = expected_counts

    chi2_stat = sum((o - e)**2 / e for o, e in zip(obs_combined, exp_combined) if e > 0)
    df = len(obs_combined) - 1 - 1  # -1 for estimated parameter
    p_value = 1 - stats.chi2.cdf(chi2_stat, df)

    return {
        'lambda': lambda_est,
        'lambda_ci': (ci_lower, ci_upper),
        'goodness_of_fit': {
            'chi2_statistic': chi2_stat,
            'p_value': p_value,
            'df': df
        }
    }

# Example: Defect count analysis
np.random.seed(42)
defect_counts = poisson.rvs(mu=2.5, size=50)  # Defects per wafer

poisson_analysis = poisson_process_analysis(defect_counts)
print(f"\nPoisson Process Analysis:")
print(f"Estimated rate (λ): {poisson_analysis['lambda']:.2f} defects/unit")
print(f"95% CI for λ: [{poisson_analysis['lambda_ci'][0]:.2f}, {poisson_analysis['lambda_ci'][1]:.2f}]")
print(f"Goodness of fit: χ² = {poisson_analysis['goodness_of_fit']['chi2_statistic']:.2f}, p = {poisson_analysis['goodness_of_fit']['p_value']:.4f}")
```

### Distribution Selection and Testing

```python
def select_best_distribution(data, distributions=None):
    """
    Select best distribution for data using multiple criteria

    distributions: list of scipy.stats distribution objects to test
    """
    if distributions is None:
        distributions = [
            stats.norm,
            stats.lognorm,
            stats.weibull_min,
            stats.gamma,
            stats.beta,
            stats.exponweib
        ]

    results = []

    for dist in distributions:
        try:
            # Fit distribution
            params = dist.fit(data)

            # Kolmogorov-Smirnov test
            ks_stat, ks_p = stats.kstest(data, lambda x: dist.cdf(x, *params))

            # Anderson-Darling test (if available)
            try:
                ad_stat, ad_critical, ad_significance = stats.anderson(data, dist.name)
                ad_p = None  # A-D test doesn't provide p-value directly
            except:
                ad_stat = ad_p = None

            # Akaike Information Criterion
            log_likelihood = np.sum(dist.logpdf(data, *params))
            k = len(params)  # number of parameters
            n = len(data)
            aic = 2*k - 2*log_likelihood
            bic = k*np.log(n) - 2*log_likelihood

            results.append({
                'distribution': dist.name,
                'parameters': params,
                'ks_statistic': ks_stat,
                'ks_p_value': ks_p,
                'ad_statistic': ad_stat,
                'aic': aic,
                'bic': bic,
                'log_likelihood': log_likelihood
            })

        except Exception as e:
            print(f"Failed to fit {dist.name}: {e}")

    # Sort by AIC (lower is better)
    results.sort(key=lambda x: x['aic'])

    return results

# Example: Find best distribution for threshold voltage data
np.random.seed(42)
# Generate mixed data (slightly skewed)
vth_data = np.concatenate([
    np.random.normal(650, 20, 800),
    np.random.normal(680, 15, 200)
])

dist_results = select_best_distribution(vth_data)

print("Distribution Selection Results (sorted by AIC):")
print("-" * 60)
for i, result in enumerate(dist_results[:5]):  # Show top 5
    print(f"{i+1}. {result['distribution'].title()}")
    print(f"   AIC: {result['aic']:.2f}, BIC: {result['bic']:.2f}")
    print(f"   K-S test: D = {result['ks_statistic']:.4f}, p = {result['ks_p_value']:.4f}")
    print(f"   Parameters: {[f'{p:.3f}' for p in result['parameters']]}")
    print()
```

---

## Implementation Guidelines

### Software Architecture for Statistical Analysis

#### Class Structure

```python
from abc import ABC, abstractmethod
import logging

class StatisticalAnalyzer(ABC):
    """Abstract base class for statistical analyzers"""

    def __init__(self, name, confidence_level=0.95):
        self.name = name
        self.confidence_level = confidence_level
        self.logger = logging.getLogger(f"StatAnalyzer.{name}")

    @abstractmethod
    def analyze(self, data, **kwargs):
        """Perform statistical analysis"""
        pass

    @abstractmethod
    def interpret_results(self, results):
        """Provide interpretation of results"""
        pass

    def validate_data(self, data):
        """Validate input data"""
        if len(data) == 0:
            raise ValueError("Data cannot be empty")

        if not all(np.isfinite(data)):
            self.logger.warning("Data contains infinite or NaN values")
            return np.array(data)[np.isfinite(data)]

        return np.array(data)

class ProcessCapabilityAnalyzer(StatisticalAnalyzer):
    """Process capability analysis implementation"""

    def __init__(self, confidence_level=0.95):
        super().__init__("ProcessCapability", confidence_level)

    def analyze(self, data, lsl, usl, target=None):
        """Perform process capability analysis"""
        data = self.validate_data(data)

        if target is None:
            target = (lsl + usl) / 2

        mean = np.mean(data)
        std = np.std(data, ddof=1)
        n = len(data)

        # Basic capability indices
        cp = (usl - lsl) / (6 * std)
        cpu = (usl - mean) / (3 * std)
        cpl = (mean - lsl) / (3 * std)
        cpk = min(cpu, cpl)

        # Confidence intervals for Cpk
        # Using approximation for large samples
        se_cpk = cpk * np.sqrt((1/(9*n*cpk**2)) + (1/(2*(n-1))))
        alpha = 1 - self.confidence_level
        t_critical = stats.t.ppf(1 - alpha/2, n - 1)

        cpk_ci_lower = cpk - t_critical * se_cpk
        cpk_ci_upper = cpk + t_critical * se_cpk

        # Process performance indices (same as capability for this example)
        pp = cp
        ppk = cpk

        # Predicted yield
        yield_pred = (stats.norm.cdf(usl, mean, std) -
                     stats.norm.cdf(lsl, mean, std)) * 100

        results = {
            'sample_size': n,
            'mean': mean,
            'std_dev': std,
            'specifications': {'lsl': lsl, 'usl': usl, 'target': target},
            'indices': {
                'cp': cp,
                'cpu': cpu,
                'cpl': cpl,
                'cpk': cpk,
                'pp': pp,
                'ppk': ppk
            },
            'cpk_confidence_interval': (cpk_ci_lower, cpk_ci_upper),
            'predicted_yield': yield_pred,
            'process_centered': abs(mean - target) < (usl - lsl) * 0.05
        }

        return results

    def interpret_results(self, results):
        """Interpret capability analysis results"""
        cpk = results['indices']['cpk']

        if cpk >= 1.33:
            capability_rating = "Excellent (Class 1)"
            recommendation = "Process is highly capable. Continue monitoring."
        elif cpk >= 1.0:
            capability_rating = "Adequate (Class 2)"
            recommendation = "Process is marginally capable. Consider improvement."
        elif cpk >= 0.67:
            capability_rating = "Poor (Class 3)"
            recommendation = "Process is not capable. Immediate improvement required."
        else:
            capability_rating = "Unacceptable (Class 4)"
            recommendation = "Process is completely inadequate. Major overhaul needed."

        interpretation = {
            'capability_rating': capability_rating,
            'recommendation': recommendation,
            'key_findings': []
        }

        # Add specific findings
        if not results['process_centered']:
            interpretation['key_findings'].append(
                f"Process is off-center (mean = {results['mean']:.2f}, "
                f"target = {results['specifications']['target']:.2f})"
            )

        if results['indices']['cp'] > results['indices']['cpk']:
            interpretation['key_findings'].append(
                "Process variation is acceptable but centering needs improvement"
            )

        yield_pred = results['predicted_yield']
        if yield_pred < 99:
            interpretation['key_findings'].append(
                f"Predicted yield is {yield_pred:.2f}%, below typical targets"
            )

        return interpretation

# Example usage
analyzer = ProcessCapabilityAnalyzer()

# Generate example data
np.random.seed(42)
process_data = np.random.normal(100, 1.5, 100)

# Analyze capability
results = analyzer.analyze(process_data, lsl=95, usl=105, target=100)
interpretation = analyzer.interpret_results(results)

print("Process Capability Analysis:")
print(f"Cpk: {results['indices']['cpk']:.3f}")
print(f"Cp:  {results['indices']['cp']:.3f}")
print(f"Rating: {interpretation['capability_rating']}")
print(f"Recommendation: {interpretation['recommendation']}")

if interpretation['key_findings']:
    print("Key Findings:")
    for finding in interpretation['key_findings']:
        print(f"  • {finding}")
```

### Quality Assurance and Validation

#### Automated Testing Framework

```python
import unittest
from unittest.mock import patch

class TestStatisticalMethods(unittest.TestCase):
    """Test suite for statistical analysis methods"""

    def setUp(self):
        """Set up test fixtures"""
        np.random.seed(42)
        self.normal_data = np.random.normal(100, 5, 1000)
        self.analyzer = ProcessCapabilityAnalyzer()

    def test_capability_calculation_known_values(self):
        """Test capability calculation with known values"""
        # Perfect normal data with known parameters
        data = np.random.normal(100, 1, 1000)
        results = self.analyzer.analyze(data, lsl=94, usl=106)

        # Theoretical Cp should be (106-94)/(6*1) = 2.0
        expected_cp = 2.0
        self.assertAlmostEqual(results['indices']['cp'], expected_cp, places=1)

    def test_empty_data_handling(self):
        """Test handling of empty data"""
        with self.assertRaises(ValueError):
            self.analyzer.analyze([], lsl=95, usl=105)

    def test_invalid_specifications(self):
        """Test handling of invalid specifications"""
        with self.assertRaises(ValueError):
            self.analyzer.analyze(self.normal_data, lsl=105, usl=95)  # LSL > USL

    def test_confidence_interval_coverage(self):
        """Test confidence interval coverage using simulation"""
        true_cpk = 1.33
        coverages = []

        for _ in range(100):
            # Generate data with known Cpk
            data = np.random.normal(100, 1.5, 50)
            results = self.analyzer.analyze(data, lsl=93, usl=107)

            ci_lower, ci_upper = results['cpk_confidence_interval']
            coverage = ci_lower <= true_cpk <= ci_upper
            coverages.append(coverage)

        actual_coverage = np.mean(coverages)
        expected_coverage = 0.95

        # Allow for some variability in simulation
        self.assertGreater(actual_coverage, expected_coverage - 0.1)

    def test_process_centering_detection(self):
        """Test detection of process centering issues"""
        # Off-center process
        off_center_data = np.random.normal(98, 2, 100)
        results = self.analyzer.analyze(off_center_data, lsl=94, usl=106, target=100)

        self.assertFalse(results['process_centered'])

        # Centered process
        centered_data = np.random.normal(100, 2, 100)
        results = self.analyzer.analyze(centered_data, lsl=94, usl=106, target=100)

        self.assertTrue(results['process_centered'])

# Run tests
if __name__ == '__main__':
    unittest.main(verbosity=2)
```

### Performance Optimization

#### Efficient Computation for Large Datasets

```python
import numba
from numba import jit

@jit(nopython=True)
def fast_capability_indices(data, lsl, usl):
    """Optimized capability calculation using Numba JIT compilation"""
    n = len(data)
    mean = 0.0

    # Calculate mean
    for i in range(n):
        mean += data[i]
    mean /= n

    # Calculate standard deviation
    var_sum = 0.0
    for i in range(n):
        diff = data[i] - mean
        var_sum += diff * diff

    std = np.sqrt(var_sum / (n - 1))

    # Calculate capability indices
    cp = (usl - lsl) / (6 * std)
    cpu = (usl - mean) / (3 * std)
    cpl = (mean - lsl) / (3 * std)
    cpk = min(cpu, cpl)

    return cp, cpk, cpu, cpl

# Benchmark performance
import time

# Large dataset
large_data = np.random.normal(100, 5, 1000000)

# Standard calculation
start_time = time.time()
mean = np.mean(large_data)
std = np.std(large_data, ddof=1)
cp_standard = (105 - 95) / (6 * std)
cpk_standard = min((105 - mean) / (3 * std), (mean - 95) / (3 * std))
standard_time = time.time() - start_time

# Optimized calculation
start_time = time.time()
cp_fast, cpk_fast, _, _ = fast_capability_indices(large_data, 95, 105)
fast_time = time.time() - start_time

print(f"Standard calculation: {standard_time:.4f} seconds")
print(f"Optimized calculation: {fast_time:.4f} seconds")
print(f"Speedup: {standard_time/fast_time:.2f}x")
print(f"Results match: Cp {abs(cp_standard - cp_fast) < 1e-10}, Cpk {abs(cpk_standard - cpk_fast) < 1e-10}")
```

---

## Conclusion

This comprehensive guide covers the advanced statistical theory and methods essential for semiconductor analysis. The combination of theoretical understanding, practical implementation, and quality assurance ensures robust and reliable statistical analysis capabilities.

### Key Takeaways

1. **Statistical foundations** provide the theoretical basis for all analysis methods
2. **Measurement system analysis** ensures data quality and reliability
3. **Advanced methods** handle non-normal data and complex scenarios
4. **Process control theory** enables effective monitoring and improvement
5. **Distribution theory** guides proper model selection and interpretation
6. **Implementation guidelines** ensure robust, efficient, and maintainable code

### Best Practices

- Always validate assumptions before applying statistical methods
- Use appropriate sample sizes for desired confidence and power
- Implement comprehensive testing and validation procedures
- Document all assumptions, limitations, and interpretations
- Regularly review and update methods based on new research and standards

### Future Developments

- Integration with machine learning methods
- Real-time statistical monitoring systems
- Advanced multivariate techniques
- Bayesian statistical approaches
- Uncertainty quantification methods

This foundation enables the development of sophisticated statistical analysis capabilities that meet the demanding requirements of modern semiconductor manufacturing and development.
