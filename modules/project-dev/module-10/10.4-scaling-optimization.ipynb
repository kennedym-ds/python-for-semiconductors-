{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10.4: Scaling & Optimization for Semiconductor ML\n",
    "\n",
    "This notebook demonstrates practical techniques for scaling and optimizing machine learning pipelines for semiconductor manufacturing workloads.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master vectorization with NumPy/Pandas to avoid Python loops\n",
    "- Implement parallel processing with joblib for batch operations\n",
    "- Apply memory and time profiling for performance optimization\n",
    "- Use caching strategies with joblib for computational efficiency\n",
    "- Understand incremental/partial fit patterns for large datasets\n",
    "\n",
    "## Topics Covered\n",
    "1. **Vectorization**: NumPy/Pandas operations vs Python loops\n",
    "2. **Parallel Processing**: Batch operations with joblib\n",
    "3. **Profiling**: Memory and time profiling techniques\n",
    "4. **Caching**: Computational caching with joblib.Memory\n",
    "5. **Incremental Learning**: Partial fit patterns for streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tracemalloc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, Memory\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation for Optimization Demonstrations\n",
    "\n",
    "Let's create synthetic wafer processing data that's complex enough to demonstrate optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wafer_process_data(n=5000, seed=RANDOM_SEED):\n",
    "    \"\"\"Generate synthetic wafer processing data for optimization demonstrations.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Process parameters\n",
    "    temperature = rng.normal(450, 15, n)\n",
    "    pressure = rng.normal(2.5, 0.3, n) \n",
    "    flow_rate = rng.normal(120, 10, n)\n",
    "    time_duration = rng.normal(60, 5, n)\n",
    "    chamber_id = rng.integers(1, 9, n)  # 8 chambers\n",
    "    \n",
    "    # Additional features for complexity\n",
    "    humidity = rng.normal(45, 5, n)\n",
    "    gas_concentration = rng.normal(0.85, 0.05, n)\n",
    "    power_consumption = rng.normal(2000, 200, n)\n",
    "    \n",
    "    # Complex yield calculation with interactions\n",
    "    base_yield = (70 + \n",
    "                  0.05*(temperature-450) - \n",
    "                  1.5*(pressure-2.5)**2 + \n",
    "                  0.04*flow_rate + \n",
    "                  0.2*time_duration +\n",
    "                  0.0005*(temperature-450)*(flow_rate-120) -\n",
    "                  0.1*(humidity-45)**2 +\n",
    "                  10*(gas_concentration-0.85) -\n",
    "                  0.001*(power_consumption-2000))\n",
    "    \n",
    "    # Add chamber effects\n",
    "    chamber_effects = np.array([0, -2, 1, -1, 2, 0, 1, -1])[chamber_id - 1]\n",
    "    \n",
    "    # Add noise\n",
    "    noise = rng.normal(0, 3, n)\n",
    "    yield_pct = np.clip(base_yield + chamber_effects + noise, 0, 100)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'temperature': temperature,\n",
    "        'pressure': pressure,\n",
    "        'flow_rate': flow_rate,\n",
    "        'time_duration': time_duration,\n",
    "        'chamber_id': chamber_id,\n",
    "        'humidity': humidity,\n",
    "        'gas_concentration': gas_concentration,\n",
    "        'power_consumption': power_consumption,\n",
    "        'yield_pct': yield_pct\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate datasets of different sizes for demonstrations\n",
    "small_data = generate_wafer_process_data(n=1000)\n",
    "medium_data = generate_wafer_process_data(n=10000)\n",
    "large_data = generate_wafer_process_data(n=50000)\n",
    "\n",
    "print(f\"Small dataset: {small_data.shape}\")\n",
    "print(f\"Medium dataset: {medium_data.shape}\")\n",
    "print(f\"Large dataset: {large_data.shape}\")\n",
    "\n",
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "small_data.plot(x='temperature', y='yield_pct', kind='scatter', ax=axes[0,0], alpha=0.6)\n",
    "axes[0,0].set_title('Temperature vs Yield')\n",
    "\n",
    "small_data.plot(x='pressure', y='yield_pct', kind='scatter', ax=axes[0,1], alpha=0.6)\n",
    "axes[0,1].set_title('Pressure vs Yield')\n",
    "\n",
    "small_data.boxplot(column='yield_pct', by='chamber_id', ax=axes[1,0])\n",
    "axes[1,0].set_title('Yield by Chamber')\n",
    "\n",
    "small_data['yield_pct'].hist(bins=30, ax=axes[1,1])\n",
    "axes[1,1].set_title('Yield Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorization: NumPy/Pandas vs Python Loops\n",
    "\n",
    "One of the most important optimizations is using vectorized operations instead of Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_feature_engineering(df):\n",
    "    \"\"\"Vectorized feature engineering using NumPy operations.\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Vectorized operations - all at once\n",
    "    df_new['temp_centered'] = df['temperature'] - df['temperature'].mean()\n",
    "    df_new['pressure_sq'] = df['pressure'] ** 2\n",
    "    df_new['flow_temp_inter'] = df['flow_rate'] * df['temperature']\n",
    "    df_new['power_efficiency'] = df['power_consumption'] / df['power_consumption'].max()\n",
    "    df_new['normalized_time'] = df['time_duration'] / df['time_duration'].max()\n",
    "    \n",
    "    # Complex vectorized calculations\n",
    "    df_new['stability_index'] = np.sqrt(\n",
    "        (df['temperature'] - df['temperature'].mean())**2 + \n",
    "        (df['pressure'] - df['pressure'].mean())**2\n",
    "    )\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def loop_based_feature_engineering(df):\n",
    "    \"\"\"Non-vectorized feature engineering using Python loops (slow).\"\"\"\n",
    "    df_new = df.copy()\n",
    "    n = len(df)\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df_new['temp_centered'] = 0.0\n",
    "    df_new['pressure_sq'] = 0.0\n",
    "    df_new['flow_temp_inter'] = 0.0\n",
    "    df_new['power_efficiency'] = 0.0\n",
    "    df_new['normalized_time'] = 0.0\n",
    "    df_new['stability_index'] = 0.0\n",
    "    \n",
    "    temp_mean = df['temperature'].mean()\n",
    "    pressure_mean = df['pressure'].mean()\n",
    "    time_max = df['time_duration'].max()\n",
    "    power_max = df['power_consumption'].max()\n",
    "    \n",
    "    # Loop-based operations (inefficient)\n",
    "    for i in range(n):\n",
    "        df_new.iloc[i, df_new.columns.get_loc('temp_centered')] = df.iloc[i]['temperature'] - temp_mean\n",
    "        df_new.iloc[i, df_new.columns.get_loc('pressure_sq')] = df.iloc[i]['pressure'] ** 2\n",
    "        df_new.iloc[i, df_new.columns.get_loc('flow_temp_inter')] = df.iloc[i]['flow_rate'] * df.iloc[i]['temperature']\n",
    "        df_new.iloc[i, df_new.columns.get_loc('power_efficiency')] = df.iloc[i]['power_consumption'] / power_max\n",
    "        df_new.iloc[i, df_new.columns.get_loc('normalized_time')] = df.iloc[i]['time_duration'] / time_max\n",
    "        df_new.iloc[i, df_new.columns.get_loc('stability_index')] = np.sqrt(\n",
    "            (df.iloc[i]['temperature'] - temp_mean)**2 + \n",
    "            (df.iloc[i]['pressure'] - pressure_mean)**2\n",
    "        )\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Benchmark vectorized vs loop operations\n",
    "print(\"Benchmarking Vectorized vs Loop Operations\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_data = small_data.copy()\n",
    "\n",
    "# Vectorized approach\n",
    "start_time = time.perf_counter()\n",
    "vec_result = vectorized_feature_engineering(test_data)\n",
    "vec_time = time.perf_counter() - start_time\n",
    "\n",
    "# Loop-based approach  \n",
    "start_time = time.perf_counter()\n",
    "loop_result = loop_based_feature_engineering(test_data)\n",
    "loop_time = time.perf_counter() - start_time\n",
    "\n",
    "speedup = loop_time / vec_time\n",
    "\n",
    "print(f\"Dataset size: {len(test_data):,} rows\")\n",
    "print(f\"Vectorized time: {vec_time:.4f} seconds\")\n",
    "print(f\"Loop-based time: {loop_time:.4f} seconds\")\n",
    "print(f\"Speedup: {speedup:.1f}x faster with vectorization\")\n",
    "\n",
    "# Verify results are the same\n",
    "print(f\"\\nResults match: {np.allclose(vec_result['temp_centered'], loop_result['temp_centered'])}\")\n",
    "\n",
    "# Show feature engineering results\n",
    "print(f\"\\nNew features created: {len(vec_result.columns) - len(test_data.columns)}\")\n",
    "print(f\"Final dataset shape: {vec_result.shape}\")\n",
    "vec_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel Processing with joblib\n",
    "\n",
    "For CPU-intensive tasks, parallel processing can provide significant speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wafer_batch(batch_data):\n",
    "    \"\"\"Simulate complex wafer processing calculations.\"\"\"\n",
    "    # Simulate complex mathematical operations\n",
    "    result = np.sqrt(np.abs(batch_data)) + np.log1p(np.abs(batch_data))\n",
    "    return np.mean(result, axis=1)  # Return batch statistics\n",
    "\n",
    "def parallel_batch_processing(data, batch_size=1000, n_jobs=-1):\n",
    "    \"\"\"Process data in parallel batches.\"\"\"\n",
    "    # Split data into batches\n",
    "    batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_wafer_batch)(batch) for batch in batches)\n",
    "    \n",
    "    return np.concatenate(results)\n",
    "\n",
    "def serial_batch_processing(data, batch_size=1000):\n",
    "    \"\"\"Process data serially for comparison.\"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        results.append(process_wafer_batch(batch))\n",
    "    return np.concatenate(results)\n",
    "\n",
    "# Benchmark parallel vs serial processing\n",
    "print(\"Benchmarking Parallel vs Serial Processing\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use medium dataset for demonstration\n",
    "test_array = medium_data[['temperature', 'pressure', 'flow_rate', 'time_duration']].values\n",
    "\n",
    "# Serial processing\n",
    "start_time = time.perf_counter()\n",
    "serial_result = serial_batch_processing(test_array)\n",
    "serial_time = time.perf_counter() - start_time\n",
    "\n",
    "# Parallel processing\n",
    "start_time = time.perf_counter()\n",
    "parallel_result = parallel_batch_processing(test_array)\n",
    "parallel_time = time.perf_counter() - start_time\n",
    "\n",
    "speedup = serial_time / parallel_time\n",
    "\n",
    "print(f\"Dataset size: {len(test_array):,} rows\")\n",
    "print(f\"Serial time: {serial_time:.4f} seconds\")\n",
    "print(f\"Parallel time: {parallel_time:.4f} seconds\")\n",
    "print(f\"Speedup: {speedup:.1f}x faster with parallelization\")\n",
    "\n",
    "# Verify results are the same\n",
    "print(f\"Results match: {np.allclose(serial_result, parallel_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory and Time Profiling\n",
    "\n",
    "Understanding memory usage and execution time is crucial for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceProfiler:\n",
    "    \"\"\"Simple profiler for timing and memory usage.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.times = {}\n",
    "        self.memory_usage = {}\n",
    "    \n",
    "    def profile_function(self, name, func, *args, **kwargs):\n",
    "        \"\"\"Profile both time and memory usage of a function.\"\"\"\n",
    "        # Start memory tracking\n",
    "        tracemalloc.start()\n",
    "        \n",
    "        # Time the function\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        # Get memory usage\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        \n",
    "        # Store results\n",
    "        self.times[name] = end_time - start_time\n",
    "        self.memory_usage[name] = {\n",
    "            'current_mb': current / 1024 / 1024,\n",
    "            'peak_mb': peak / 1024 / 1024\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate performance report.\"\"\"\n",
    "        print(\"Performance Profile Report\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"{'Function':<20} {'Time (s)':<10} {'Peak RAM (MB)':<15}\")\n",
    "        print(\"-\"*45)\n",
    "        \n",
    "        for name in self.times:\n",
    "            time_val = self.times[name]\n",
    "            memory_val = self.memory_usage[name]['peak_mb']\n",
    "            print(f\"{name:<20} {time_val:<10.4f} {memory_val:<15.2f}\")\n",
    "\n",
    "# Profile different operations\n",
    "profiler = PerformanceProfiler()\n",
    "\n",
    "# Profile data generation\n",
    "profiler.profile_function('data_generation', generate_wafer_process_data, n=10000)\n",
    "\n",
    "# Profile feature engineering\n",
    "test_data = medium_data.copy()\n",
    "profiler.profile_function('vectorized_features', vectorized_feature_engineering, test_data)\n",
    "\n",
    "# Profile model training\n",
    "X = test_data.drop(['yield_pct'], axis=1)\n",
    "y = test_data['yield_pct']\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=RANDOM_SEED)\n",
    "profiler.profile_function('model_training', model.fit, X, y)\n",
    "\n",
    "# Profile predictions\n",
    "profiler.profile_function('predictions', model.predict, X)\n",
    "\n",
    "# Generate report\n",
    "profiler.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Caching with joblib.Memory\n",
    "\n",
    "Caching expensive computations can dramatically improve performance for repeated operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup caching directory\n",
    "cache_dir = Path('/tmp/semiconductor_cache')\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "memory = Memory(cache_dir, verbose=1)\n",
    "\n",
    "# Create cached versions of expensive functions\n",
    "@memory.cache\n",
    "def cached_feature_engineering(df_hash, df):\n",
    "    \"\"\"Cached version of feature engineering.\"\"\"\n",
    "    print(\"Computing features (not cached)...\")\n",
    "    return vectorized_feature_engineering(df)\n",
    "\n",
    "@memory.cache\n",
    "def cached_model_training(X_hash, y_hash, X, y):\n",
    "    \"\"\"Cached version of model training.\"\"\"\n",
    "    print(\"Training model (not cached)...\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Demonstrate caching benefits\n",
    "print(\"Demonstrating Caching Benefits\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "test_data = medium_data.copy()\n",
    "data_hash = hash(str(test_data.values.tobytes()))\n",
    "\n",
    "# First run (no cache)\n",
    "print(\"\\n1st run (computing from scratch):\")\n",
    "start_time = time.perf_counter()\n",
    "features_1 = cached_feature_engineering(data_hash, test_data)\n",
    "time_1 = time.perf_counter() - start_time\n",
    "print(f\"Time: {time_1:.4f} seconds\")\n",
    "\n",
    "# Second run (cached)\n",
    "print(\"\\n2nd run (using cache):\")\n",
    "start_time = time.perf_counter()\n",
    "features_2 = cached_feature_engineering(data_hash, test_data)\n",
    "time_2 = time.perf_counter() - start_time\n",
    "print(f\"Time: {time_2:.4f} seconds\")\n",
    "\n",
    "speedup = time_1 / time_2\n",
    "print(f\"\\nSpeedup from caching: {speedup:.1f}x\")\n",
    "\n",
    "# Test model caching\n",
    "X = features_1.drop(['yield_pct'], axis=1)\n",
    "y = features_1['yield_pct']\n",
    "X_hash = hash(str(X.values.tobytes()))\n",
    "y_hash = hash(str(y.values.tobytes()))\n",
    "\n",
    "print(\"\\nModel training (1st time):\")\n",
    "start_time = time.perf_counter()\n",
    "model_1 = cached_model_training(X_hash, y_hash, X, y)\n",
    "model_time_1 = time.perf_counter() - start_time\n",
    "print(f\"Time: {model_time_1:.4f} seconds\")\n",
    "\n",
    "print(\"\\nModel training (cached):\")\n",
    "start_time = time.perf_counter()\n",
    "model_2 = cached_model_training(X_hash, y_hash, X, y)\n",
    "model_time_2 = time.perf_counter() - start_time\n",
    "print(f\"Time: {model_time_2:.4f} seconds\")\n",
    "\n",
    "model_speedup = model_time_1 / model_time_2\n",
    "print(f\"\\nModel caching speedup: {model_speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Incremental Learning for Large Datasets\n",
    "\n",
    "For very large datasets that don't fit in memory, incremental learning allows us to train models in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def incremental_training_demo(data, batch_size=1000):\n",
    "    \"\"\"Demonstrate incremental learning with SGD.\"\"\"\n",
    "    \n",
    "    # Prepare features and target\n",
    "    features = vectorized_feature_engineering(data)\n",
    "    X = features.drop(['yield_pct'], axis=1)\n",
    "    y = features['yield_pct']\n",
    "    \n",
    "    # Initialize scaler and model for incremental learning\n",
    "    scaler = StandardScaler()\n",
    "    model = SGDRegressor(random_state=RANDOM_SEED, max_iter=1000)\n",
    "    \n",
    "    # First batch to initialize scaler\n",
    "    first_batch_X = X.iloc[:batch_size]\n",
    "    first_batch_y = y.iloc[:batch_size]\n",
    "    \n",
    "    # Fit scaler on first batch (in practice, use a representative sample)\n",
    "    X_scaled = scaler.fit_transform(first_batch_X)\n",
    "    model.partial_fit(X_scaled, first_batch_y)\n",
    "    \n",
    "    batch_scores = []\n",
    "    \n",
    "    # Process remaining batches incrementally\n",
    "    for i in range(batch_size, len(X), batch_size):\n",
    "        end_idx = min(i + batch_size, len(X))\n",
    "        \n",
    "        batch_X = X.iloc[i:end_idx]\n",
    "        batch_y = y.iloc[i:end_idx]\n",
    "        \n",
    "        # Transform and train\n",
    "        X_batch_scaled = scaler.transform(batch_X)\n",
    "        model.partial_fit(X_batch_scaled, batch_y)\n",
    "        \n",
    "        # Evaluate on current batch\n",
    "        pred = model.predict(X_batch_scaled)\n",
    "        score = r2_score(batch_y, pred)\n",
    "        batch_scores.append(score)\n",
    "        \n",
    "        if len(batch_scores) % 5 == 0:\n",
    "            print(f\"Processed {end_idx:,} samples, R² score: {score:.4f}\")\n",
    "    \n",
    "    return model, scaler, batch_scores\n",
    "\n",
    "# Compare batch vs incremental learning\n",
    "print(\"Incremental Learning Demonstration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use large dataset\n",
    "large_sample = large_data.sample(n=20000, random_state=RANDOM_SEED)\n",
    "\n",
    "# Incremental learning\n",
    "print(\"\\nIncremental learning:\")\n",
    "start_time = time.perf_counter()\n",
    "inc_model, inc_scaler, scores = incremental_training_demo(large_sample, batch_size=2000)\n",
    "inc_time = time.perf_counter() - start_time\n",
    "print(f\"Incremental training time: {inc_time:.4f} seconds\")\n",
    "\n",
    "# Batch learning for comparison\n",
    "print(\"\\nBatch learning:\")\n",
    "start_time = time.perf_counter()\n",
    "features_batch = vectorized_feature_engineering(large_sample)\n",
    "X_batch = features_batch.drop(['yield_pct'], axis=1)\n",
    "y_batch = features_batch['yield_pct']\n",
    "\n",
    "batch_scaler = StandardScaler()\n",
    "X_batch_scaled = batch_scaler.fit_transform(X_batch)\n",
    "batch_model = SGDRegressor(random_state=RANDOM_SEED, max_iter=1000)\n",
    "batch_model.fit(X_batch_scaled, y_batch)\n",
    "batch_time = time.perf_counter() - start_time\n",
    "print(f\"Batch training time: {batch_time:.4f} seconds\")\n",
    "\n",
    "# Compare final performance\n",
    "test_sample = large_data.sample(n=1000, random_state=42)\n",
    "test_features = vectorized_feature_engineering(test_sample)\n",
    "X_test = test_features.drop(['yield_pct'], axis=1)\n",
    "y_test = test_features['yield_pct']\n",
    "\n",
    "# Incremental model performance\n",
    "X_test_inc = inc_scaler.transform(X_test)\n",
    "inc_pred = inc_model.predict(X_test_inc)\n",
    "inc_score = r2_score(y_test, inc_pred)\n",
    "\n",
    "# Batch model performance\n",
    "X_test_batch = batch_scaler.transform(X_test)\n",
    "batch_pred = batch_model.predict(X_test_batch)\n",
    "batch_score = r2_score(y_test, batch_pred)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"Incremental model R²: {inc_score:.4f}\")\n",
    "print(f\"Batch model R²: {batch_score:.4f}\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(scores)+1), scores, 'b-', linewidth=2)\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Incremental Learning Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Optimization Pipeline\n",
    "\n",
    "Let's put it all together in a comprehensive optimization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMLPipeline:\n",
    "    \"\"\"Optimized ML pipeline with all techniques combined.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_caching=True, cache_dir='/tmp/ml_cache', \n",
    "                 use_parallel=True, n_jobs=-1):\n",
    "        self.use_caching = use_caching\n",
    "        self.use_parallel = use_parallel\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        # Setup caching\n",
    "        if use_caching:\n",
    "            cache_path = Path(cache_dir)\n",
    "            cache_path.mkdir(exist_ok=True)\n",
    "            self.memory = Memory(cache_path, verbose=0)\n",
    "        else:\n",
    "            self.memory = None\n",
    "            \n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def _get_cached_or_compute(self, func, *args, **kwargs):\n",
    "        \"\"\"Use cached computation if available.\"\"\"\n",
    "        if self.memory:\n",
    "            cached_func = self.memory.cache(func)\n",
    "            return cached_func(*args, **kwargs)\n",
    "        return func(*args, **kwargs)\n",
    "    \n",
    "    def preprocess_features(self, data, data_hash=None):\n",
    "        \"\"\"Vectorized feature preprocessing with optional caching.\"\"\"\n",
    "        if data_hash is None:\n",
    "            data_hash = hash(str(data.values.tobytes()))\n",
    "            \n",
    "        return self._get_cached_or_compute(\n",
    "            vectorized_feature_engineering, data_hash, data\n",
    "        )\n",
    "    \n",
    "    def fit(self, data, target_col='yield_pct', use_incremental=False, batch_size=5000):\n",
    "        \"\"\"Fit the optimized pipeline.\"\"\"\n",
    "        print(f\"Training optimized pipeline (parallel={self.use_parallel}, \"\n",
    "              f\"caching={self.use_caching}, incremental={use_incremental})\")\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Feature engineering with caching\n",
    "        features = self.preprocess_features(data)\n",
    "        \n",
    "        # Prepare data\n",
    "        X = features.drop([target_col], axis=1)\n",
    "        y = features[target_col]\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        # Scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        if use_incremental:\n",
    "            # Incremental learning\n",
    "            self.model = SGDRegressor(random_state=RANDOM_SEED, max_iter=1000)\n",
    "            \n",
    "            # Fit scaler on first batch\n",
    "            first_batch = X.iloc[:batch_size]\n",
    "            self.scaler.fit(first_batch)\n",
    "            \n",
    "            # Train incrementally\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                end_idx = min(i + batch_size, len(X))\n",
    "                batch_X = X.iloc[i:end_idx]\n",
    "                batch_y = y.iloc[i:end_idx]\n",
    "                \n",
    "                X_scaled = self.scaler.transform(batch_X)\n",
    "                \n",
    "                if i == 0:\n",
    "                    self.model.partial_fit(X_scaled, batch_y)\n",
    "                else:\n",
    "                    self.model.partial_fit(X_scaled, batch_y)\n",
    "        else:\n",
    "            # Batch learning with parallelization\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            self.model = RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                random_state=RANDOM_SEED,\n",
    "                n_jobs=self.n_jobs if self.use_parallel else 1\n",
    "            )\n",
    "            self.model.fit(X_scaled, y)\n",
    "        \n",
    "        training_time = time.perf_counter() - start_time\n",
    "        print(f\"Training completed in {training_time:.4f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Make predictions with the optimized pipeline.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "            \n",
    "        # Feature engineering\n",
    "        features = self.preprocess_features(data)\n",
    "        X = features[self.feature_names]\n",
    "        \n",
    "        # Scale and predict\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def evaluate(self, data, target_col='yield_pct'):\n",
    "        \"\"\"Evaluate the pipeline.\"\"\"\n",
    "        features = self.preprocess_features(data)\n",
    "        y_true = features[target_col]\n",
    "        y_pred = self.predict(data)\n",
    "        \n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'R2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Demonstrate the complete optimized pipeline\n",
    "print(\"Complete Optimized Pipeline Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create train/test split\n",
    "train_data = large_data.sample(n=15000, random_state=42)\n",
    "test_data = large_data.sample(n=3000, random_state=123)\n",
    "\n",
    "# Compare different configurations\n",
    "configs = [\n",
    "    {'name': 'Basic', 'use_caching': False, 'use_parallel': False},\n",
    "    {'name': 'Parallel', 'use_caching': False, 'use_parallel': True},\n",
    "    {'name': 'Cached', 'use_caching': True, 'use_parallel': False},\n",
    "    {'name': 'Optimized', 'use_caching': True, 'use_parallel': True},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting {config['name']} configuration...\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = OptimizedMLPipeline(\n",
    "        use_caching=config['use_caching'],\n",
    "        use_parallel=config['use_parallel']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.perf_counter()\n",
    "    pipeline.fit(train_data)\n",
    "    train_time = time.perf_counter() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.perf_counter()\n",
    "    metrics = pipeline.evaluate(test_data)\n",
    "    eval_time = time.perf_counter() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        'config': config['name'],\n",
    "        'train_time': train_time,\n",
    "        'eval_time': eval_time,\n",
    "        **metrics\n",
    "    })\n",
    "    \n",
    "    print(f\"Train time: {train_time:.4f}s, Eval time: {eval_time:.4f}s, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training time comparison\n",
    "ax1.bar(results_df['config'], results_df['train_time'])\n",
    "ax1.set_title('Training Time Comparison')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² score comparison\n",
    "ax2.bar(results_df['config'], results_df['R2'])\n",
    "ax2.set_title('Model Performance (R²)')\n",
    "ax2.set_ylabel('R² Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate speedups\n",
    "baseline_time = results_df[results_df['config'] == 'Basic']['train_time'].iloc[0]\n",
    "optimized_time = results_df[results_df['config'] == 'Optimized']['train_time'].iloc[0]\n",
    "speedup = baseline_time / optimized_time\n",
    "\n",
    "print(f\"\\nOptimization Summary:\")\n",
    "print(f\"Overall speedup: {speedup:.1f}x faster\")\n",
    "print(f\"Time saved: {baseline_time - optimized_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways and Best Practices\n",
    "\n",
    "### Optimization Strategies for Semiconductor ML:\n",
    "\n",
    "1. **Vectorization**: Always prefer NumPy/Pandas operations over Python loops\n",
    "2. **Parallel Processing**: Use joblib for CPU-intensive tasks with multiple cores\n",
    "3. **Caching**: Cache expensive computations that are likely to be repeated\n",
    "4. **Incremental Learning**: Use partial_fit for datasets too large for memory\n",
    "5. **Profiling**: Regularly profile code to identify bottlenecks\n",
    "\n",
    "### Manufacturing-Specific Considerations:\n",
    "\n",
    "- **Real-time constraints**: Optimize for inference speed in production\n",
    "- **Memory limitations**: Consider edge computing constraints\n",
    "- **Batch processing**: Optimize for wafer-level batch operations\n",
    "- **Streaming data**: Prepare for continuous process monitoring\n",
    "\n",
    "### Performance Metrics to Track:\n",
    "\n",
    "- **Training time**: How long to retrain models\n",
    "- **Inference time**: Latency for real-time predictions\n",
    "- **Memory usage**: Peak and average memory consumption\n",
    "- **Throughput**: Samples processed per second\n",
    "- **Scalability**: Performance with increasing data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final optimization checklist for your projects\n",
    "print(\"Optimization Checklist for Semiconductor ML:\")\n",
    "print(\"=\"*50)\n",
    "checklist = [\n",
    "    \"✓ Use vectorized operations instead of loops\",\n",
    "    \"✓ Implement parallel processing for CPU-bound tasks\", \n",
    "    \"✓ Cache expensive computations\",\n",
    "    \"✓ Profile memory and time usage regularly\",\n",
    "    \"✓ Consider incremental learning for large datasets\",\n",
    "    \"✓ Optimize data loading and preprocessing\",\n",
    "    \"✓ Use appropriate batch sizes\",\n",
    "    \"✓ Monitor resource utilization in production\",\n",
    "    \"✓ Test performance with realistic data volumes\",\n",
    "    \"✓ Document performance characteristics\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)\n",
    "\n",
    "print(\"\\nRemember: Premature optimization is the root of all evil.\")\n",
    "print(\"Always profile first, then optimize the actual bottlenecks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}