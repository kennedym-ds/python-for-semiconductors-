{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 10.1: Project Architecture & Best Practices - Interactive Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides hands-on exploration of project architecture patterns and best practices for semiconductor ML projects. We'll walk through scaffolding projects, validating structures, and implementing configuration management patterns.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Scaffold** semiconductor ML projects using standardized templates\n",
        "- **Validate** project structures against industry best practices\n",
        "- **Implement** configuration management and environment setup\n",
        "- **Design** CLI interfaces with JSON outputs\n",
        "- **Configure** structured logging for production environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Standard imports\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "\n",
        "# Set up paths relative to notebook location\n",
        "NOTEBOOK_DIR = Path().resolve()\n",
        "MODULE_DIR = NOTEBOOK_DIR.parent\n",
        "PROJECT_ROOT = MODULE_DIR.parent.parent.parent\n",
        "DATA_DIR = PROJECT_ROOT / \"datasets\"\n",
        "\n",
        "# Add module to path for imports\n",
        "sys.path.append(str(MODULE_DIR))\n",
        "\n",
        "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
        "print(f\"Module directory: {MODULE_DIR}\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Data directory exists: {DATA_DIR.exists()}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Project Scaffolding\n",
        "\n",
        "Let's start by exploring the project scaffolding capabilities of our architecture pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Import our pipeline directly\n",
        "from importlib.util import spec_from_file_location, module_from_spec\n",
        "\n",
        "pipeline_path = MODULE_DIR / \"10.1-project-architecture-pipeline.py\"\n",
        "spec = spec_from_file_location(\"project_architecture\", pipeline_path)\n",
        "project_arch = module_from_spec(spec)\n",
        "spec.loader.exec_module(project_arch)\n",
        "\n",
        "# Create pipeline instance\n",
        "pipeline = project_arch.ProjectArchitecturePipeline()\n",
        "\n",
        "print(\"\u2705 Project architecture pipeline loaded successfully\")\n",
        "print(f\"Available project types: {list(project_arch.SEMICONDUCTOR_PROJECT_TYPES.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore Available Project Types\n",
        "\n",
        "Our scaffolding system supports different types of semiconductor ML projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Display project types and their characteristics\n",
        "project_types = project_arch.SEMICONDUCTOR_PROJECT_TYPES\n",
        "\n",
        "for project_type, info in project_types.items():\n",
        "    print(f\"\\n\ud83d\udd2c **{project_type.title()}**\")\n",
        "    print(f\"   Description: {info['description']}\")\n",
        "    print(f\"   Key Metrics: {', '.join(info['key_metrics'])}\")\n",
        "    print(f\"   Sample Datasets: {', '.join(info['sample_datasets'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaffold a Classification Project\n",
        "\n",
        "Let's create a semiconductor defect classification project using our scaffolding system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a temporary directory for our demo project\n",
        "demo_projects_dir = Path(tempfile.mkdtemp(prefix=\"semiconductor_projects_\"))\n",
        "print(f\"Demo projects directory: {demo_projects_dir}\")\n",
        "\n",
        "# Scaffold a classification project\n",
        "project_name = \"wafer_defect_classifier\"\n",
        "project_type = \"classification\"\n",
        "\n",
        "try:\n",
        "    result = pipeline.scaffold(\n",
        "        name=project_name,\n",
        "        project_type=project_type,\n",
        "        output_dir=demo_projects_dir,\n",
        "        include_notebooks=True,\n",
        "        include_docker=True\n",
        "    )\n",
        "    \n",
        "    print(\"\u2705 Project scaffolded successfully!\")\n",
        "    print(f\"Project path: {result['project_path']}\")\n",
        "    print(f\"Project type: {result['project_type']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error scaffolding project: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore Generated Project Structure\n",
        "\n",
        "Let's examine what was created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "project_path = demo_projects_dir / project_name\n",
        "\n",
        "def display_tree(directory, prefix=\"\", max_depth=3, current_depth=0):\n",
        "    \"\"\"Display directory tree structure.\"\"\"\n",
        "    if current_depth >= max_depth:\n",
        "        return\n",
        "        \n",
        "    items = sorted(directory.iterdir())\n",
        "    dirs = [item for item in items if item.is_dir()]\n",
        "    files = [item for item in items if item.is_file()]\n",
        "    \n",
        "    # Display directories first\n",
        "    for i, item in enumerate(dirs):\n",
        "        is_last_dir = (i == len(dirs) - 1 and len(files) == 0)\n",
        "        current_prefix = \"\u2514\u2500\u2500 \" if is_last_dir else \"\u251c\u2500\u2500 \"\n",
        "        print(f\"{prefix}{current_prefix}{item.name}/\")\n",
        "        \n",
        "        next_prefix = prefix + (\"    \" if is_last_dir else \"\u2502   \")\n",
        "        display_tree(item, next_prefix, max_depth, current_depth + 1)\n",
        "    \n",
        "    # Display files\n",
        "    for i, item in enumerate(files):\n",
        "        is_last = (i == len(files) - 1)\n",
        "        current_prefix = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n",
        "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
        "\n",
        "print(f\"\ud83d\udcc1 Project Structure: {project_name}\")\n",
        "display_tree(project_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examine Generated Files\n",
        "\n",
        "Let's look at some key files that were generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# README.md\n",
        "readme_path = project_path / \"README.md\"\n",
        "if readme_path.exists():\n",
        "    print(\"\ud83d\udcc4 README.md (first 20 lines):\")\n",
        "    with open(readme_path) as f:\n",
        "        lines = f.readlines()[:20]\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            print(f\"{i:2d}: {line.rstrip()}\")\n",
        "    print(\"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Configuration file\n",
        "config_path = project_path / \"configs\" / \"config.yaml\"\n",
        "if config_path.exists():\n",
        "    print(\"\u2699\ufe0f Configuration (config.yaml):\")\n",
        "    with open(config_path) as f:\n",
        "        config = yaml.safe_load(f)\n",
        "        print(yaml.dump(config, default_flow_style=False, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Environment template\n",
        "env_template_path = project_path / \".env.template\"\n",
        "if env_template_path.exists():\n",
        "    print(\"\ud83d\udd12 Environment Template (.env.template):\")\n",
        "    with open(env_template_path) as f:\n",
        "        lines = f.readlines()[:15]  # Show first 15 lines\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            print(f\"{i:2d}: {line.rstrip()}\")\n",
        "    print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Project Validation\n",
        "\n",
        "Now let's validate our scaffolded project against best practices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Validate the generated project\n",
        "validation_result = pipeline.validate(project_path)\n",
        "\n",
        "print(f\"\ud83d\udcca Validation Results for {project_name}\")\n",
        "print(f\"Valid: {'\u2705' if validation_result.is_valid else '\u274c'} {validation_result.is_valid}\")\n",
        "print(f\"Score: {validation_result.score:.1f}/100.0\")\n",
        "\n",
        "if validation_result.errors:\n",
        "    print(f\"\\n\u274c Errors ({len(validation_result.errors)}):\")\n",
        "    for error in validation_result.errors:\n",
        "        print(f\"   \u2022 {error}\")\n",
        "\n",
        "if validation_result.warnings:\n",
        "    print(f\"\\n\u26a0\ufe0f Warnings ({len(validation_result.warnings)}):\")\n",
        "    for warning in validation_result.warnings:\n",
        "        print(f\"   \u2022 {warning}\")\n",
        "\n",
        "if validation_result.suggestions:\n",
        "    print(f\"\\n\ud83d\udca1 Suggestions ({len(validation_result.suggestions)}):\")\n",
        "    for suggestion in validation_result.suggestions:\n",
        "        print(f\"   \u2022 {suggestion}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 Score Breakdown:\")\n",
        "for metric, score in validation_result.metrics.items():\n",
        "    print(f\"   {metric}: {score:.1f} points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Validation Metrics\n",
        "\n",
        "Let's create a visualization of the validation scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a bar chart of validation metrics\n",
        "metrics = validation_result.metrics\n",
        "metric_names = list(metrics.keys())\n",
        "metric_scores = list(metrics.values())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(metric_names, metric_scores, color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, score in zip(bars, metric_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "             f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.title(f'Project Validation Scores: {project_name}', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xlabel('Validation Metric', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, max(metric_scores) * 1.1)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Overall Score: {validation_result.score:.1f}/100.0\")\n",
        "print(f\"Status: {'\u2705 PASS' if validation_result.is_valid else '\u274c FAIL'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Detailed Structure Linting\n",
        "\n",
        "Let's perform detailed linting to check for common anti-patterns and issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Perform detailed linting\n",
        "lint_result = pipeline.lint_structure(project_path)\n",
        "\n",
        "print(\"\ud83d\udd0d Detailed Linting Results\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Display validation summary\n",
        "validation = lint_result['validation']\n",
        "print(f\"Validation Status: {'\u2705 PASS' if validation['is_valid'] else '\u274c FAIL'}\")\n",
        "print(f\"Overall Score: {validation['score']:.1f}/100.0\")\n",
        "\n",
        "# Display detailed checks\n",
        "detailed = lint_result['detailed_checks']\n",
        "\n",
        "print(\"\\n\ud83d\udea8 Anti-patterns Check:\")\n",
        "anti_patterns = detailed.get('anti_patterns', [])\n",
        "if anti_patterns:\n",
        "    for pattern in anti_patterns:\n",
        "        print(f\"   \u274c {pattern}\")\n",
        "else:\n",
        "    print(\"   \u2705 No anti-patterns detected\")\n",
        "\n",
        "print(\"\\n\ud83d\udcdd Naming Issues:\")\n",
        "naming_issues = detailed.get('naming_issues', [])\n",
        "if naming_issues:\n",
        "    for issue in naming_issues:\n",
        "        print(f\"   \u26a0\ufe0f {issue}\")\n",
        "else:\n",
        "    print(\"   \u2705 No naming issues detected\")\n",
        "\n",
        "print(\"\\n\ud83d\udce6 Import Issues:\")\n",
        "import_issues = detailed.get('import_issues', [])\n",
        "if import_issues:\n",
        "    for issue in import_issues:\n",
        "        print(f\"   \u26a0\ufe0f {issue}\")\n",
        "else:\n",
        "    print(\"   \u2705 No import issues detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CLI Interface Testing\n",
        "\n",
        "Let's test the CLI interface of our pipeline using subprocess calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test CLI help command\n",
        "pipeline_script = MODULE_DIR / \"10.1-project-architecture-pipeline.py\"\n",
        "\n",
        "print(\"\ud83d\udda5\ufe0f Testing CLI Interface\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Test help command\n",
        "try:\n",
        "    result = subprocess.run([\n",
        "        'python', str(pipeline_script), '--help'\n",
        "    ], capture_output=True, text=True, timeout=10)\n",
        "    \n",
        "    print(\"\ud83d\udccb CLI Help Output:\")\n",
        "    print(result.stdout)\n",
        "    \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"\u274c CLI help command timed out\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error running CLI help: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test scaffold command via CLI\n",
        "cli_project_name = \"cli_test_project\"\n",
        "cli_output_dir = demo_projects_dir\n",
        "\n",
        "try:\n",
        "    result = subprocess.run([\n",
        "        'python', str(pipeline_script), 'scaffold',\n",
        "        '--name', cli_project_name,\n",
        "        '--type', 'regression',\n",
        "        '--output', str(cli_output_dir),\n",
        "        '--no-docker'  # Skip Docker for faster testing\n",
        "    ], capture_output=True, text=True, timeout=30)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\u2705 CLI scaffold command successful!\")\n",
        "        \n",
        "        # Parse JSON output\n",
        "        try:\n",
        "            output_data = json.loads(result.stdout)\n",
        "            print(f\"\ud83d\udcc1 Created project: {output_data['project_path']}\")\n",
        "            print(f\"\ud83d\udcca Project type: {output_data['project_type']}\")\n",
        "            print(f\"\u2728 Features: {len(output_data['metadata']['compliance_features'])} compliance features\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"\u26a0\ufe0f Could not parse JSON output\")\n",
        "            print(\"Raw output:\", result.stdout[:200], \"...\")\n",
        "    else:\n",
        "        print(f\"\u274c CLI scaffold command failed with exit code {result.returncode}\")\n",
        "        print(\"Error output:\", result.stderr)\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"\u274c CLI scaffold command timed out\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error running CLI scaffold: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test validation command via CLI\n",
        "if (cli_output_dir / cli_project_name).exists():\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            'python', str(pipeline_script), 'validate',\n",
        "            '--project-path', str(cli_output_dir / cli_project_name)\n",
        "        ], capture_output=True, text=True, timeout=15)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"\u2705 CLI validation command successful!\")\n",
        "            \n",
        "            # Parse JSON output\n",
        "            try:\n",
        "                output_data = json.loads(result.stdout)\n",
        "                validation_data = output_data['validation']\n",
        "                \n",
        "                print(f\"\ud83d\udcca Validation Score: {validation_data['score']:.1f}/100.0\")\n",
        "                print(f\"\u2705 Valid: {validation_data['is_valid']}\")\n",
        "                print(f\"\u274c Errors: {len(validation_data['errors'])}\")\n",
        "                print(f\"\u26a0\ufe0f Warnings: {len(validation_data['warnings'])}\")\n",
        "                \n",
        "            except json.JSONDecodeError:\n",
        "                print(\"\u26a0\ufe0f Could not parse JSON output\")\n",
        "                print(\"Raw output:\", result.stdout[:200], \"...\")\n",
        "        else:\n",
        "            print(f\"\u274c CLI validation command failed with exit code {result.returncode}\")\n",
        "            print(\"Error output:\", result.stderr)\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"\u274c CLI validation command timed out\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error running CLI validation: {e}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f CLI test project not found, skipping validation test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configuration Management Patterns\n",
        "\n",
        "Let's explore the configuration management patterns in our generated projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Demonstrate configuration loading pattern\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class DemoConfig:\n",
        "    \"\"\"Example configuration class for semiconductor ML projects.\"\"\"\n",
        "    project_name: str\n",
        "    environment: str\n",
        "    log_level: str\n",
        "    random_seed: int\n",
        "    tolerance: float\n",
        "    spec_low: float\n",
        "    spec_high: float\n",
        "    \n",
        "    @classmethod\n",
        "    def from_env(cls, config_file: Optional[Path] = None):\n",
        "        \"\"\"Load configuration from environment and YAML file.\"\"\"\n",
        "        # Simulate environment variables\n",
        "        env_vars = {\n",
        "            'PROJECT_NAME': 'demo_semiconductor_project',\n",
        "            'ENVIRONMENT': 'development',\n",
        "            'LOG_LEVEL': 'INFO',\n",
        "            'RANDOM_SEED': '42',\n",
        "            'TOLERANCE': '2.0',\n",
        "            'SPEC_LOW': '60.0',\n",
        "            'SPEC_HIGH': '100.0'\n",
        "        }\n",
        "        \n",
        "        return cls(\n",
        "            project_name=env_vars.get('PROJECT_NAME', 'unnamed_project'),\n",
        "            environment=env_vars.get('ENVIRONMENT', 'development'),\n",
        "            log_level=env_vars.get('LOG_LEVEL', 'INFO'),\n",
        "            random_seed=int(env_vars.get('RANDOM_SEED', '42')),\n",
        "            tolerance=float(env_vars.get('TOLERANCE', '2.0')),\n",
        "            spec_low=float(env_vars.get('SPEC_LOW', '60.0')),\n",
        "            spec_high=float(env_vars.get('SPEC_HIGH', '100.0'))\n",
        "        )\n",
        "\n",
        "# Load configuration\n",
        "config = DemoConfig.from_env()\n",
        "\n",
        "print(\"\u2699\ufe0f Configuration Loading Example\")\n",
        "print(f\"Project Name: {config.project_name}\")\n",
        "print(f\"Environment: {config.environment}\")\n",
        "print(f\"Log Level: {config.log_level}\")\n",
        "print(f\"Random Seed: {config.random_seed}\")\n",
        "print(f\"Manufacturing Tolerance: {config.tolerance}\")\n",
        "print(f\"Spec Limits: {config.spec_low} - {config.spec_high}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Manufacturing Metrics Implementation\n",
        "\n",
        "Let's implement and test semiconductor-specific metrics that should be included in projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def compute_semiconductor_metrics(y_true, y_pred, tolerance=2.0, \n",
        "                                spec_low=60.0, spec_high=100.0,\n",
        "                                cost_per_unit=1.0):\n",
        "    \"\"\"Compute semiconductor manufacturing metrics.\"\"\"\n",
        "    \n",
        "    # Prediction Within Spec (PWS)\n",
        "    pws = np.mean((y_pred >= spec_low) & (y_pred <= spec_high))\n",
        "    \n",
        "    # Estimated Loss from prediction errors\n",
        "    loss_components = np.maximum(0, np.abs(y_true - y_pred) - tolerance)\n",
        "    estimated_loss = float(np.sum(loss_components) * cost_per_unit)\n",
        "    \n",
        "    # Yield Rate (assuming higher values = better yield)\n",
        "    yield_rate = np.mean(y_pred >= spec_low)\n",
        "    \n",
        "    # Process Capability Index (simplified)\n",
        "    if len(y_pred) > 1:\n",
        "        process_std = np.std(y_pred)\n",
        "        spec_range = spec_high - spec_low\n",
        "        cp_index = spec_range / (6 * process_std) if process_std > 0 else float('inf')\n",
        "    else:\n",
        "        cp_index = 0.0\n",
        "    \n",
        "    return {\n",
        "        'PWS': pws,\n",
        "        'Estimated_Loss': estimated_loss,\n",
        "        'Yield_Rate': yield_rate,\n",
        "        'Cp_Index': cp_index\n",
        "    }\n",
        "\n",
        "# Generate synthetic semiconductor data for demonstration\n",
        "np.random.seed(RANDOM_SEED)\n",
        "n_samples = 1000\n",
        "\n",
        "# Simulate process parameters\n",
        "temperature = np.random.normal(450, 15, n_samples)\n",
        "pressure = np.random.normal(2.5, 0.3, n_samples)\n",
        "flow_rate = np.random.normal(120, 10, n_samples)\n",
        "\n",
        "# Simulate yield (target variable)\n",
        "noise = np.random.normal(0, 3, n_samples)\n",
        "y_true = (0.2 * temperature + 10 * pressure + 0.1 * flow_rate + noise)\n",
        "\n",
        "# Simulate predictions (with some error)\n",
        "prediction_noise = np.random.normal(0, 2, n_samples)\n",
        "y_pred = y_true + prediction_noise\n",
        "\n",
        "# Compute metrics\n",
        "metrics = compute_semiconductor_metrics(\n",
        "    y_true, y_pred, \n",
        "    tolerance=config.tolerance,\n",
        "    spec_low=config.spec_low,\n",
        "    spec_high=config.spec_high\n",
        ")\n",
        "\n",
        "print(\"\ud83c\udfed Semiconductor Manufacturing Metrics\")\n",
        "print(\"=\" * 40)\n",
        "for metric_name, value in metrics.items():\n",
        "    if metric_name == 'Estimated_Loss':\n",
        "        print(f\"{metric_name}: ${value:.2f}\")\n",
        "    elif metric_name in ['PWS', 'Yield_Rate']:\n",
        "        print(f\"{metric_name}: {value:.1%}\")\n",
        "    else:\n",
        "        print(f\"{metric_name}: {value:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Manufacturing Metrics\n",
        "\n",
        "Let's create visualizations to understand these semiconductor-specific metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a comprehensive visualization of manufacturing metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Semiconductor Manufacturing Metrics Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Prediction vs True values with spec limits\n",
        "ax1 = axes[0, 0]\n",
        "ax1.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
        "ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', alpha=0.8, label='Perfect Prediction')\n",
        "ax1.axhline(y=config.spec_low, color='orange', linestyle='--', alpha=0.7, label='Spec Limits')\n",
        "ax1.axhline(y=config.spec_high, color='orange', linestyle='--', alpha=0.7)\n",
        "ax1.axvline(x=config.spec_low, color='orange', linestyle='--', alpha=0.7)\n",
        "ax1.axvline(x=config.spec_high, color='orange', linestyle='--', alpha=0.7)\n",
        "ax1.set_xlabel('True Values')\n",
        "ax1.set_ylabel('Predicted Values')\n",
        "ax1.set_title('Predictions vs True Values')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Distribution of predictions with spec limits\n",
        "ax2 = axes[0, 1]\n",
        "ax2.hist(y_pred, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "ax2.axvline(x=config.spec_low, color='red', linestyle='--', linewidth=2, label=f'Lower Spec ({config.spec_low})')\n",
        "ax2.axvline(x=config.spec_high, color='red', linestyle='--', linewidth=2, label=f'Upper Spec ({config.spec_high})')\n",
        "ax2.axvline(x=np.mean(y_pred), color='green', linestyle='-', linewidth=2, label=f'Mean ({np.mean(y_pred):.1f})')\n",
        "ax2.set_xlabel('Predicted Values')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Distribution of Predictions')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Error distribution with tolerance\n",
        "ax3 = axes[1, 0]\n",
        "errors = np.abs(y_true - y_pred)\n",
        "ax3.hist(errors, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "ax3.axvline(x=config.tolerance, color='red', linestyle='--', linewidth=2, label=f'Tolerance ({config.tolerance})')\n",
        "ax3.axvline(x=np.mean(errors), color='blue', linestyle='-', linewidth=2, label=f'Mean Error ({np.mean(errors):.2f})')\n",
        "ax3.set_xlabel('Absolute Error')\n",
        "ax3.set_ylabel('Frequency')\n",
        "ax3.set_title('Prediction Error Distribution')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Metrics summary\n",
        "ax4 = axes[1, 1]\n",
        "metric_names = list(metrics.keys())\n",
        "metric_values = [metrics[name] for name in metric_names]\n",
        "\n",
        "# Normalize metrics for display (0-1 scale)\n",
        "normalized_values = []\n",
        "for name, value in zip(metric_names, metric_values):\n",
        "    if name in ['PWS', 'Yield_Rate']:\n",
        "        normalized_values.append(value)  # Already 0-1\n",
        "    elif name == 'Cp_Index':\n",
        "        normalized_values.append(min(value / 2.0, 1.0))  # Cap at 2.0 for display\n",
        "    else:  # Estimated_Loss\n",
        "        normalized_values.append(max(0, 1 - value / 1000))  # Invert and scale\n",
        "\n",
        "bars = ax4.bar(range(len(metric_names)), normalized_values, \n",
        "               color=['green', 'red', 'blue', 'purple'], alpha=0.7)\n",
        "ax4.set_xticks(range(len(metric_names)))\n",
        "ax4.set_xticklabels(metric_names, rotation=45)\n",
        "ax4.set_ylabel('Normalized Score (0-1)')\n",
        "ax4.set_title('Manufacturing Metrics Summary')\n",
        "ax4.set_ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, original_value, name in zip(bars, metric_values, metric_names):\n",
        "    if name == 'Estimated_Loss':\n",
        "        label = f'${original_value:.0f}'\n",
        "    elif name in ['PWS', 'Yield_Rate']:\n",
        "        label = f'{original_value:.1%}'\n",
        "    else:\n",
        "        label = f'{original_value:.2f}'\n",
        "    \n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "             label, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\n\ud83d\udcca Summary Statistics:\")\n",
        "print(f\"   Samples: {n_samples:,}\")\n",
        "print(f\"   Within Spec: {int(metrics['PWS'] * n_samples):,} ({metrics['PWS']:.1%})\")\n",
        "print(f\"   Mean Absolute Error: {np.mean(errors):.2f}\")\n",
        "print(f\"   Samples Within Tolerance: {int(np.mean(errors <= config.tolerance) * n_samples):,} ({np.mean(errors <= config.tolerance):.1%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Testing Different Project Types\n",
        "\n",
        "Let's test scaffolding different types of semiconductor projects to see how the templates adapt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test all project types\n",
        "project_type_results = {}\n",
        "\n",
        "for proj_type in project_types.keys():\n",
        "    test_name = f\"test_{proj_type}_project\"\n",
        "    \n",
        "    try:\n",
        "        result = pipeline.scaffold(\n",
        "            name=test_name,\n",
        "            project_type=proj_type,\n",
        "            output_dir=demo_projects_dir,\n",
        "            include_notebooks=False,  # Skip notebooks for faster testing\n",
        "            include_docker=False      # Skip Docker for faster testing\n",
        "        )\n",
        "        \n",
        "        # Validate the created project\n",
        "        validation = pipeline.validate(Path(result['project_path']))\n",
        "        \n",
        "        project_type_results[proj_type] = {\n",
        "            'success': True,\n",
        "            'path': result['project_path'],\n",
        "            'score': validation.score,\n",
        "            'valid': validation.is_valid,\n",
        "            'features': len(result['metadata']['compliance_features'])\n",
        "        }\n",
        "        \n",
        "        print(f\"\u2705 {proj_type.title()}: Created and validated (Score: {validation.score:.1f})\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        project_type_results[proj_type] = {\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        print(f\"\u274c {proj_type.title()}: Failed - {e}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Project Type Testing Summary:\")\n",
        "successful = sum(1 for r in project_type_results.values() if r.get('success', False))\n",
        "print(f\"   Successful: {successful}/{len(project_types)}\")\n",
        "print(f\"   Average Score: {np.mean([r['score'] for r in project_type_results.values() if r.get('success', False)]):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare Project Types\n",
        "\n",
        "Let's visualize the differences between project types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create comparison visualization\n",
        "successful_results = {k: v for k, v in project_type_results.items() if v.get('success', False)}\n",
        "\n",
        "if successful_results:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Validation scores comparison\n",
        "    types = list(successful_results.keys())\n",
        "    scores = [successful_results[t]['score'] for t in types]\n",
        "    \n",
        "    bars1 = ax1.bar(types, scores, color='lightblue', alpha=0.7, edgecolor='navy')\n",
        "    ax1.set_title('Validation Scores by Project Type', fontweight='bold')\n",
        "    ax1.set_ylabel('Validation Score')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, score in zip(bars1, scores):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
        "                f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # Features comparison\n",
        "    features = [successful_results[t]['features'] for t in types]\n",
        "    \n",
        "    bars2 = ax2.bar(types, features, color='lightgreen', alpha=0.7, edgecolor='darkgreen')\n",
        "    ax2.set_title('Compliance Features by Project Type', fontweight='bold')\n",
        "    ax2.set_ylabel('Number of Features')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, feature_count in zip(bars2, features):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "                f'{feature_count}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Project Type Analysis:\")\n",
        "    for proj_type, result in successful_results.items():\n",
        "        status = \"\u2705 VALID\" if result['valid'] else \"\u26a0\ufe0f ISSUES\"\n",
        "        print(f\"   {proj_type.title()}: {status} (Score: {result['score']:.1f}, Features: {result['features']})\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No successful project types to compare\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup and Summary\n",
        "\n",
        "Let's clean up our temporary projects and summarize what we've learned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Cleanup temporary projects\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    if demo_projects_dir.exists():\n",
        "        shutil.rmtree(demo_projects_dir)\n",
        "        print(f\"\ud83e\uddf9 Cleaned up temporary projects directory: {demo_projects_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Could not clean up temporary directory: {e}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Module 10.1 Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\n\u2705 **What We Accomplished:**\")\n",
        "print(\"   \u2022 Created standardized project scaffolding system\")\n",
        "print(\"   \u2022 Implemented comprehensive project validation\")\n",
        "print(\"   \u2022 Demonstrated CLI interface with JSON outputs\")\n",
        "print(\"   \u2022 Explored configuration management patterns\")\n",
        "print(\"   \u2022 Implemented semiconductor-specific metrics\")\n",
        "print(\"   \u2022 Tested different project types and templates\")\n",
        "\n",
        "print(\"\\n\ud83d\udd11 **Key Concepts Covered:**\")\n",
        "print(\"   \u2022 Standardized directory structures for ML projects\")\n",
        "print(\"   \u2022 Multi-tier configuration management (env vars, YAML, constants)\")\n",
        "print(\"   \u2022 Data versioning and path resolution strategies\")\n",
        "print(\"   \u2022 CLI consistency with train/evaluate/predict patterns\")\n",
        "print(\"   \u2022 Structured logging with JSON outputs\")\n",
        "print(\"   \u2022 Secrets management with environment variables\")\n",
        "print(\"   \u2022 Manufacturing-specific metrics (PWS, Estimated Loss, etc.)\")\n",
        "\n",
        "print(\"\\n\ud83c\udfed **Semiconductor Manufacturing Focus:**\")\n",
        "print(\"   \u2022 Process parameter templates and validation\")\n",
        "print(\"   \u2022 Yield prediction and defect classification structures\")\n",
        "print(\"   \u2022 Specification compliance and tolerance metrics\")\n",
        "print(\"   \u2022 Equipment monitoring and time series patterns\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 **Next Steps:**\")\n",
        "print(\"   \u2022 Apply these patterns to your own semiconductor ML projects\")\n",
        "print(\"   \u2022 Customize templates for your specific manufacturing processes\")\n",
        "print(\"   \u2022 Integrate with your existing CI/CD and deployment pipelines\")\n",
        "print(\"   \u2022 Extend metrics and validation rules for your domain\")\n",
        "\n",
        "print(\"\\n\ud83d\udcda **Resources for Further Learning:**\")\n",
        "print(\"   \u2022 10.1-project-architecture-fundamentals.md - Detailed theory\")\n",
        "print(\"   \u2022 10.1-project-architecture-quick-ref.md - Quick reference guide\")\n",
        "print(\"   \u2022 10.1-project-architecture-pipeline.py - Full implementation\")\n",
        "print(\"   \u2022 projects/starter/template/ - Reusable project template\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
