{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 10.2: Testing & Quality Assurance for Semiconductor ML Pipelines\n",
        "\n",
        "This notebook demonstrates comprehensive testing strategies, coverage measurement, and quality gates for production ML systems in semiconductor manufacturing.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand testing patterns for CLI-based ML pipelines\n",
        "- Implement coverage measurement with pytest-cov\n",
        "- Apply code quality checks with flake8 and black\n",
        "- Validate dataset paths and system health\n",
        "- Create quality gates for production deployments\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set paths\n",
        "MODULE_DIR = Path('.').resolve()\n",
        "PROJECT_ROOT = MODULE_DIR.parent.parent.parent\n",
        "PIPELINE_SCRIPT = MODULE_DIR / '10.2-testing-qa-pipeline.py'\n",
        "\n",
        "print(f\"Module directory: {MODULE_DIR}\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Pipeline script: {PIPELINE_SCRIPT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CLI Testing Fundamentals\n",
        "\n",
        "### 1.1 Basic CLI Interaction\n",
        "\n",
        "First, let's explore the QA pipeline CLI interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_pipeline_cmd(args, capture_output=True):\n",
        "    \"\"\"Execute the QA pipeline CLI and return results.\"\"\"\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, str(PIPELINE_SCRIPT)] + args,\n",
        "        capture_output=capture_output,\n",
        "        text=True,\n",
        "        cwd=PROJECT_ROOT\n",
        "    )\n",
        "    if capture_output:\n",
        "        try:\n",
        "            return json.loads(result.stdout)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"stdout\": result.stdout, \"stderr\": result.stderr, \"returncode\": result.returncode}\n",
        "    return result\n",
        "\n",
        "# Test help command\n",
        "help_result = run_pipeline_cmd(['--help'], capture_output=False)\n",
        "print(f\"Help command exit code: {help_result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 System Health Check\n",
        "\n",
        "Let's start with a basic system health check using the `predict` command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run system health check\n",
        "health_check = run_pipeline_cmd(['predict'])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SYSTEM HEALTH CHECK RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Status: {health_check['status']}\")\n",
        "print(f\"Overall Health: {health_check['recommendation'].upper()}\")\n",
        "print(f\"Target: {health_check['target']}\")\n",
        "\n",
        "# Display import test results\n",
        "print(\"\\n\ud83d\udce6 Package Import Tests:\")\n",
        "import_tests = health_check['health_check']['import_tests']\n",
        "for package, result in import_tests.items():\n",
        "    status_icon = \"\u2705\" if result['status'] == 'pass' else \"\u274c\"\n",
        "    print(f\"  {status_icon} {package}: {result['status']}\")\n",
        "    if result['error']:\n",
        "        print(f\"    Error: {result['error']}\")\n",
        "\n",
        "# Display functionality tests\n",
        "print(\"\\n\ud83d\udd27 Basic Functionality Tests:\")\n",
        "func_tests = health_check['health_check']['basic_functionality']\n",
        "for test, result in func_tests.items():\n",
        "    status_icon = \"\u2705\" if result['status'] == 'pass' else \"\u274c\"\n",
        "    print(f\"  {status_icon} {test}: {result['status']}\")\n",
        "    if 'shape' in result:\n",
        "        print(f\"    Data shape: {result['shape']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Path Validation\n",
        "\n",
        "### 2.1 Validate Dataset Accessibility\n",
        "\n",
        "One of the most common issues in ML pipelines is incorrect dataset paths. Let's validate our dataset structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run dataset path validation\n",
        "path_validation = run_pipeline_cmd(['evaluate', '--check-type', 'paths'])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET PATH VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset_info = path_validation['checks']['dataset_paths']\n",
        "print(f\"Dataset root: {dataset_info['dataset_root']}\")\n",
        "print(f\"All datasets valid: {dataset_info['all_valid']}\\n\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "datasets_data = []\n",
        "for name, info in dataset_info['datasets'].items():\n",
        "    datasets_data.append({\n",
        "        'Dataset': name,\n",
        "        'Type': info['type'],\n",
        "        'Exists': info['exists'],\n",
        "        'Path': info['path']\n",
        "    })\n",
        "\n",
        "df_datasets = pd.DataFrame(datasets_data)\n",
        "print(\"Dataset Status Summary:\")\n",
        "print(df_datasets.to_string(index=False))\n",
        "\n",
        "# Visualize dataset availability\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "# Count dataset types and availability\n",
        "availability_counts = df_datasets['Exists'].value_counts()\n",
        "type_counts = df_datasets['Type'].value_counts()\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Dataset availability pie chart\n",
        "ax1.pie(availability_counts.values, labels=['Available', 'Missing'], \n",
        "        autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'lightcoral'])\n",
        "ax1.set_title('Dataset Availability')\n",
        "\n",
        "# Dataset types bar chart\n",
        "ax2.bar(type_counts.index, type_counts.values, color=['skyblue', 'lightblue'])\n",
        "ax2.set_title('Dataset Types')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Cross-Platform Path Compatibility Test\n",
        "\n",
        "Let's demonstrate how to handle different path formats across platforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def test_path_normalization():\n",
        "    \"\"\"Test path handling across different platforms.\"\"\"\n",
        "    test_cases = [\n",
        "        \"~/datasets/secom/secom.data\",          # Home directory\n",
        "        \"../../../datasets/secom/secom.data\",   # Relative paths  \n",
        "        \"/tmp/test_dataset.csv\",                 # Absolute paths\n",
        "        \"datasets\\\\secom\\\\secom.data\",            # Windows-style paths\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    for path_str in test_cases:\n",
        "        try:\n",
        "            normalized = Path(path_str).expanduser().resolve()\n",
        "            results.append({\n",
        "                'Original': path_str,\n",
        "                'Normalized': str(normalized),\n",
        "                'Is_Absolute': normalized.is_absolute(),\n",
        "                'Platform': os.name\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'Original': path_str,\n",
        "                'Normalized': f\"Error: {e}\",\n",
        "                'Is_Absolute': False,\n",
        "                'Platform': os.name\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "path_test_df = test_path_normalization()\n",
        "print(\"Path Normalization Test Results:\")\n",
        "print(path_test_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Code Quality Assessment\n",
        "\n",
        "### 3.1 Linting and Formatting Checks\n",
        "\n",
        "Let's run code quality checks on our module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run code quality assessment\n",
        "quality_check = run_pipeline_cmd(['evaluate', '--check-type', 'lint', '--target-path', str(MODULE_DIR)])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CODE QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code_quality = quality_check['checks']['code_quality']\n",
        "\n",
        "# Display flake8 results\n",
        "print(\"\ud83d\udd0d Flake8 Linting Results:\")\n",
        "flake8_results = code_quality['flake8']\n",
        "print(f\"  Violations: {flake8_results['violations']}\")\n",
        "print(f\"  Compliant: {flake8_results['compliant']}\")\n",
        "if flake8_results['violations'] > 0:\n",
        "    print(f\"  Output: {flake8_results['output']}\")\n",
        "\n",
        "# Display black results\n",
        "print(\"\\n\ud83c\udfa8 Black Formatting Results:\")\n",
        "black_results = code_quality['black']\n",
        "print(f\"  Compliant: {black_results['compliant']}\")\n",
        "print(f\"  Action: {black_results['action']}\")\n",
        "\n",
        "# Display overall score\n",
        "print(f\"\\n\ud83d\udcca Overall Quality Score: {code_quality['overall_score']:.1f}/100\")\n",
        "\n",
        "# Visualize quality metrics\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "# Create quality metrics\n",
        "metrics = {\n",
        "    'Flake8 Score': max(0, 100 - flake8_results['violations'] * 5),\n",
        "    'Black Score': 100.0 if black_results['compliant'] else 70.0,\n",
        "    'Overall Score': code_quality['overall_score']\n",
        "}\n",
        "\n",
        "# Create bar chart\n",
        "bars = ax.bar(metrics.keys(), metrics.values(), \n",
        "              color=['lightblue', 'lightgreen', 'orange'])\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Code Quality Metrics')\n",
        "ax.set_ylim(0, 100)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, metrics.values()):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "            f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "# Add quality threshold line\n",
        "ax.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Quality Threshold')\n",
        "ax.legend()\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Automatic Code Formatting\n",
        "\n",
        "If there are formatting issues, we can automatically fix them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we need to fix formatting issues\n",
        "if not black_results['compliant']:\n",
        "    print(\"\ud83d\udd27 Fixing formatting issues with Black...\")\n",
        "    fix_result = run_pipeline_cmd([\n",
        "        'evaluate', '--check-type', 'lint', \n",
        "        '--target-path', str(MODULE_DIR),\n",
        "        '--fix-issues'\n",
        "    ])\n",
        "    \n",
        "    fixed_black = fix_result['checks']['code_quality']['black']\n",
        "    print(f\"Formatting fixed: {fixed_black['action']}\")\n",
        "    print(f\"Now compliant: {fixed_black['compliant']}\")\n",
        "else:\n",
        "    print(\"\u2705 Code is already properly formatted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Performance Benchmarking\n",
        "\n",
        "### 4.1 Pipeline Performance Assessment\n",
        "\n",
        "Let's benchmark key operations in our ML pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run performance benchmarks\n",
        "perf_check = run_pipeline_cmd(['evaluate', '--check-type', 'performance'])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PERFORMANCE BENCHMARKING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "performance_data = perf_check['checks']['performance']\n",
        "benchmarks = performance_data['benchmarks']\n",
        "\n",
        "print(f\"Overall Performance Score: {performance_data['overall_score']:.1f}/100\\n\")\n",
        "\n",
        "# Create performance summary\n",
        "perf_summary = []\n",
        "for operation, metrics in benchmarks.items():\n",
        "    perf_summary.append({\n",
        "        'Operation': operation.replace('_', ' ').title(),\n",
        "        'Execution Time (s)': f\"{metrics['execution_time']:.4f}\",\n",
        "        'Status': metrics['status'],\n",
        "        'Details': metrics.get('data_shape', metrics.get('r2_score', metrics.get('prediction', 'N/A')))\n",
        "    })\n",
        "\n",
        "df_performance = pd.DataFrame(perf_summary)\n",
        "print(\"Performance Benchmark Results:\")\n",
        "print(df_performance.to_string(index=False))\n",
        "\n",
        "# Visualize performance metrics\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Execution times bar chart\n",
        "operations = [op.replace('_', ' ').title() for op in benchmarks.keys()]\n",
        "times = [metrics['execution_time'] for metrics in benchmarks.values()]\n",
        "\n",
        "bars1 = ax1.bar(operations, times, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "ax1.set_ylabel('Execution Time (seconds)')\n",
        "ax1.set_title('Operation Execution Times')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add value labels\n",
        "for bar, time in zip(bars1, times):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "             f'{time:.3f}s', ha='center', va='bottom')\n",
        "\n",
        "# Performance score gauge\n",
        "score = performance_data['overall_score']\n",
        "ax2.pie([score, 100-score], labels=['Performance Score', ''], \n",
        "        colors=['lightgreen' if score > 70 else 'orange' if score > 50 else 'lightcoral', 'lightgray'],\n",
        "        startangle=90, counterclock=False)\n",
        "ax2.set_title(f'Performance Score: {score:.1f}/100')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Performance Trends Analysis\n",
        "\n",
        "Let's run multiple benchmarks to analyze performance consistency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run multiple performance benchmarks to check consistency\n",
        "print(\"Running multiple performance benchmarks...\")\n",
        "\n",
        "benchmark_results = []\n",
        "for i in range(5):\n",
        "    perf_result = run_pipeline_cmd(['evaluate', '--check-type', 'performance'])\n",
        "    benchmarks = perf_result['checks']['performance']['benchmarks']\n",
        "    \n",
        "    run_data = {'run': i + 1}\n",
        "    for operation, metrics in benchmarks.items():\n",
        "        run_data[f'{operation}_time'] = metrics['execution_time']\n",
        "    benchmark_results.append(run_data)\n",
        "\n",
        "df_trends = pd.DataFrame(benchmark_results)\n",
        "\n",
        "# Plot performance trends\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "operations = ['data_generation_time', 'model_training_time', 'prediction_time']\n",
        "labels = ['Data Generation', 'Model Training', 'Prediction']\n",
        "\n",
        "for op, label in zip(operations, labels):\n",
        "    ax.plot(df_trends['run'], df_trends[op], marker='o', label=label)\n",
        "\n",
        "ax.set_xlabel('Benchmark Run')\n",
        "ax.set_ylabel('Execution Time (seconds)')\n",
        "ax.set_title('Performance Consistency Analysis')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate performance statistics\n",
        "print(\"\\nPerformance Statistics:\")\n",
        "for op, label in zip(operations, labels):\n",
        "    times = df_trends[op]\n",
        "    print(f\"{label}:\")\n",
        "    print(f\"  Mean: {times.mean():.4f}s\")\n",
        "    print(f\"  Std:  {times.std():.4f}s\")\n",
        "    print(f\"  Min:  {times.min():.4f}s\")\n",
        "    print(f\"  Max:  {times.max():.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comprehensive Quality Assessment\n",
        "\n",
        "### 5.1 Full QA Test Suite\n",
        "\n",
        "Let's run a comprehensive quality assessment using the train command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive QA test suite\n",
        "print(\"\ud83d\ude80 Running comprehensive QA test suite...\\n\")\n",
        "\n",
        "qa_result = run_pipeline_cmd([\n",
        "    'train', '--test-suite', 'smoke',\n",
        "    '--coverage-threshold', '75',\n",
        "    '--lint-threshold', '85',\n",
        "    '--performance-threshold', '30'\n",
        "])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPREHENSIVE QA ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"Status: {qa_result['status']}\")\n",
        "\n",
        "# Display metrics\n",
        "metrics = qa_result['metrics']\n",
        "print(f\"\\n\ud83d\udcca Test Metrics:\")\n",
        "print(f\"  Test Pass Rate: {metrics['test_pass_rate']:.1f}%\")\n",
        "print(f\"  Coverage: {metrics['coverage_percentage']:.1f}%\")\n",
        "print(f\"  Lint Score: {metrics['lint_score']:.1f}\")\n",
        "print(f\"  Performance Score: {metrics['performance_score']:.1f}\")\n",
        "print(f\"  Total Tests: {metrics['total_tests']}\")\n",
        "print(f\"  Failed Tests: {metrics['failed_tests']}\")\n",
        "print(f\"  Execution Time: {metrics['execution_time']:.2f}s\")\n",
        "\n",
        "# Display quality gates\n",
        "gates = qa_result['quality_gates']\n",
        "print(f\"\\n\ud83d\udeaa Quality Gates:\")\n",
        "for gate, passed in gates.items():\n",
        "    status_icon = \"\u2705\" if passed else \"\u274c\"\n",
        "    print(f\"  {status_icon} {gate.replace('_', ' ').title()}: {passed}\")\n",
        "\n",
        "# Visualize QA metrics\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Test results pie chart\n",
        "passed_tests = metrics['total_tests'] - metrics['failed_tests']\n",
        "ax1.pie([passed_tests, metrics['failed_tests']], \n",
        "        labels=['Passed', 'Failed'],\n",
        "        colors=['lightgreen', 'lightcoral'],\n",
        "        autopct='%1.1f%%')\n",
        "ax1.set_title('Test Results')\n",
        "\n",
        "# Quality metrics radar chart simulation (bar chart)\n",
        "quality_metrics = {\n",
        "    'Test Pass Rate': metrics['test_pass_rate'],\n",
        "    'Coverage': metrics['coverage_percentage'], \n",
        "    'Lint Score': metrics['lint_score'],\n",
        "    'Performance': metrics['performance_score']\n",
        "}\n",
        "\n",
        "bars2 = ax2.bar(quality_metrics.keys(), quality_metrics.values(),\n",
        "                color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.set_title('Quality Metrics')\n",
        "ax2.set_ylim(0, 100)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add threshold line\n",
        "ax2.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Target Threshold')\n",
        "ax2.legend()\n",
        "\n",
        "# Quality gates status\n",
        "gate_names = list(gates.keys())\n",
        "gate_status = [1 if gates[gate] else 0 for gate in gate_names]\n",
        "colors = ['lightgreen' if status else 'lightcoral' for status in gate_status]\n",
        "\n",
        "ax3.bar([name.replace('_', '\\n') for name in gate_names], gate_status, color=colors)\n",
        "ax3.set_ylabel('Passed (1) / Failed (0)')\n",
        "ax3.set_title('Quality Gates Status')\n",
        "ax3.set_ylim(0, 1.2)\n",
        "\n",
        "# Execution time trend (simulated)\n",
        "time_data = [metrics['execution_time']] * 5  # Simulate 5 runs\n",
        "ax4.plot(range(1, 6), time_data, marker='o', color='purple')\n",
        "ax4.set_xlabel('Run Number')\n",
        "ax4.set_ylabel('Execution Time (s)')\n",
        "ax4.set_title('Execution Time Trend')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Quality Trends Dashboard\n",
        "\n",
        "Let's create a comprehensive quality dashboard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive quality dashboard\n",
        "def create_quality_dashboard(qa_metrics, path_validation, performance_data):\n",
        "    \"\"\"Create a comprehensive quality dashboard.\"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1])\n",
        "    \n",
        "    # 1. Overall Quality Score (large gauge)\n",
        "    ax1 = fig.add_subplot(gs[0, :2])\n",
        "    overall_score = (qa_metrics['test_pass_rate'] + qa_metrics['coverage_percentage'] + \n",
        "                    qa_metrics['lint_score'] + qa_metrics['performance_score']) / 4\n",
        "    \n",
        "    # Create gauge chart\n",
        "    theta = np.linspace(0, np.pi, 100)\n",
        "    r = np.ones_like(theta)\n",
        "    \n",
        "    ax1.plot(theta, r, 'k-', linewidth=3)\n",
        "    \n",
        "    # Color segments\n",
        "    colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
        "    segments = np.linspace(0, np.pi, 6)\n",
        "    \n",
        "    for i in range(5):\n",
        "        mask = (theta >= segments[i]) & (theta < segments[i+1])\n",
        "        ax1.fill_between(theta[mask], 0, r[mask], color=colors[i], alpha=0.3)\n",
        "    \n",
        "    # Add needle\n",
        "    needle_angle = np.pi * (1 - overall_score / 100)\n",
        "    ax1.plot([needle_angle, needle_angle], [0, 0.8], 'r-', linewidth=4)\n",
        "    ax1.plot(needle_angle, 0, 'ro', markersize=10)\n",
        "    \n",
        "    ax1.set_ylim(0, 1.2)\n",
        "    ax1.set_xlim(0, np.pi)\n",
        "    ax1.set_title(f'Overall Quality Score: {overall_score:.1f}/100', fontsize=16, fontweight='bold')\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # 2. Test Results Summary\n",
        "    ax2 = fig.add_subplot(gs[0, 2])\n",
        "    passed = qa_metrics['total_tests'] - qa_metrics['failed_tests']\n",
        "    ax2.pie([passed, qa_metrics['failed_tests']], \n",
        "            labels=['Passed', 'Failed'],\n",
        "            colors=['lightgreen', 'lightcoral'],\n",
        "            autopct='%1.0f')\n",
        "    ax2.set_title('Test Results')\n",
        "    \n",
        "    # 3. Dataset Status\n",
        "    ax3 = fig.add_subplot(gs[0, 3])\n",
        "    datasets = path_validation['datasets']\n",
        "    available = sum(1 for d in datasets.values() if d['exists'])\n",
        "    total = len(datasets)\n",
        "    ax3.pie([available, total - available],\n",
        "            labels=['Available', 'Missing'],\n",
        "            colors=['lightblue', 'lightcoral'],\n",
        "            autopct='%1.0f')\n",
        "    ax3.set_title('Dataset Status')\n",
        "    \n",
        "    # 4. Quality Metrics Comparison\n",
        "    ax4 = fig.add_subplot(gs[1, :])\n",
        "    metrics_data = {\n",
        "        'Test Pass Rate': qa_metrics['test_pass_rate'],\n",
        "        'Coverage': qa_metrics['coverage_percentage'],\n",
        "        'Lint Score': qa_metrics['lint_score'],\n",
        "        'Performance': qa_metrics['performance_score']\n",
        "    }\n",
        "    \n",
        "    x_pos = np.arange(len(metrics_data))\n",
        "    bars = ax4.bar(x_pos, metrics_data.values(), \n",
        "                   color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
        "    \n",
        "    # Add threshold lines\n",
        "    thresholds = [95, 75, 85, 70]  # Different thresholds for different metrics\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        ax4.axhline(y=threshold, xmin=i/len(metrics_data), xmax=(i+1)/len(metrics_data),\n",
        "                   color='red', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    ax4.set_ylabel('Score')\n",
        "    ax4.set_title('Quality Metrics vs Thresholds')\n",
        "    ax4.set_xticks(x_pos)\n",
        "    ax4.set_xticklabels(metrics_data.keys(), rotation=45)\n",
        "    ax4.set_ylim(0, 100)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, metrics_data.values()):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{value:.1f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 5. Performance Breakdown\n",
        "    ax5 = fig.add_subplot(gs[2, :2])\n",
        "    perf_ops = list(performance_data['benchmarks'].keys())\n",
        "    perf_times = [performance_data['benchmarks'][op]['execution_time'] for op in perf_ops]\n",
        "    \n",
        "    ax5.barh(perf_ops, perf_times, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "    ax5.set_xlabel('Execution Time (s)')\n",
        "    ax5.set_title('Performance Breakdown')\n",
        "    \n",
        "    # 6. Quality Trend (simulated)\n",
        "    ax6 = fig.add_subplot(gs[2, 2:])\n",
        "    days = list(range(1, 8))\n",
        "    quality_trend = [overall_score + np.random.normal(0, 2) for _ in days]\n",
        "    \n",
        "    ax6.plot(days, quality_trend, marker='o', linewidth=2, color='purple')\n",
        "    ax6.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='Target (80%)')\n",
        "    ax6.set_xlabel('Day')\n",
        "    ax6.set_ylabel('Quality Score')\n",
        "    ax6.set_title('Quality Trend (7 days)')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Create dashboard with current data\n",
        "dashboard_fig = create_quality_dashboard(\n",
        "    qa_result['metrics'],\n",
        "    path_validation['checks']['dataset_paths'],\n",
        "    perf_check['checks']['performance']\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Automation and CI Integration\n",
        "\n",
        "### 6.1 Simulated CI Pipeline\n",
        "\n",
        "Let's simulate a continuous integration pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_ci_pipeline():\n",
        "    \"\"\"Simulate a complete CI pipeline with quality gates.\"\"\"\n",
        "    \n",
        "    print(\"\ud83d\ude80 Starting CI Pipeline Simulation...\\n\")\n",
        "    \n",
        "    pipeline_steps = [\n",
        "        (\"Code Checkout\", lambda: {\"status\": \"success\", \"message\": \"Code checked out successfully\"}),\n",
        "        (\"Dependency Installation\", lambda: {\"status\": \"success\", \"message\": \"Dependencies installed\"}),\n",
        "        (\"Linting (Flake8)\", lambda: run_pipeline_cmd(['evaluate', '--check-type', 'lint'])),\n",
        "        (\"Code Quality (Black)\", lambda: {\"status\": \"success\", \"message\": \"Code formatting verified\"}),\n",
        "        (\"Dataset Validation\", lambda: run_pipeline_cmd(['evaluate', '--check-type', 'paths'])),\n",
        "        (\"Smoke Tests\", lambda: run_pipeline_cmd(['evaluate', '--check-type', 'smoke'])),\n",
        "        (\"Performance Tests\", lambda: run_pipeline_cmd(['evaluate', '--check-type', 'performance'])),\n",
        "        (\"Full Test Suite\", lambda: run_pipeline_cmd(['train', '--test-suite', 'smoke'])),\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for step_name, step_func in pipeline_steps:\n",
        "        print(f\"\ud83d\udccb Running: {step_name}...\")\n",
        "        \n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            result = step_func()\n",
        "            execution_time = time.time() - start_time\n",
        "            \n",
        "            # Determine success based on result\n",
        "            if isinstance(result, dict):\n",
        "                if 'status' in result:\n",
        "                    success = result['status'] in ['success', 'trained', 'evaluated', 'predicted']\n",
        "                else:\n",
        "                    success = True  # Assume success if no status field\n",
        "            else:\n",
        "                success = True\n",
        "            \n",
        "            results.append({\n",
        "                'Step': step_name,\n",
        "                'Status': 'PASS' if success else 'FAIL',\n",
        "                'Duration': f\"{execution_time:.2f}s\",\n",
        "                'Details': result.get('message', 'Completed successfully')\n",
        "            })\n",
        "            \n",
        "            status_icon = \"\u2705\" if success else \"\u274c\"\n",
        "            print(f\"  {status_icon} {step_name}: {'PASS' if success else 'FAIL'} ({execution_time:.2f}s)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'Step': step_name,\n",
        "                'Status': 'ERROR',\n",
        "                'Duration': 'N/A',\n",
        "                'Details': str(e)\n",
        "            })\n",
        "            print(f\"  \u274c {step_name}: ERROR - {str(e)}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "import time\n",
        "ci_results = simulate_ci_pipeline()\n",
        "\n",
        "# Display CI results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CI PIPELINE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df_ci = pd.DataFrame(ci_results)\n",
        "print(df_ci.to_string(index=False))\n",
        "\n",
        "# Summary statistics\n",
        "total_steps = len(ci_results)\n",
        "passed_steps = sum(1 for r in ci_results if r['Status'] == 'PASS')\n",
        "failed_steps = sum(1 for r in ci_results if r['Status'] == 'FAIL')\n",
        "error_steps = sum(1 for r in ci_results if r['Status'] == 'ERROR')\n",
        "\n",
        "print(f\"\\n\ud83d\udcca CI Pipeline Summary:\")\n",
        "print(f\"  Total Steps: {total_steps}\")\n",
        "print(f\"  Passed: {passed_steps}\")\n",
        "print(f\"  Failed: {failed_steps}\")\n",
        "print(f\"  Errors: {error_steps}\")\n",
        "print(f\"  Success Rate: {(passed_steps/total_steps)*100:.1f}%\")\n",
        "\n",
        "# Visualize CI results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# CI step status\n",
        "status_counts = df_ci['Status'].value_counts()\n",
        "colors = {'PASS': 'lightgreen', 'FAIL': 'lightcoral', 'ERROR': 'orange'}\n",
        "status_colors = [colors.get(status, 'gray') for status in status_counts.index]\n",
        "\n",
        "ax1.pie(status_counts.values, labels=status_counts.index, colors=status_colors, autopct='%1.0f')\n",
        "ax1.set_title('CI Pipeline Status Distribution')\n",
        "\n",
        "# Step execution timeline\n",
        "step_names = [step[:15] + '...' if len(step) > 15 else step for step in df_ci['Step']]\n",
        "y_pos = np.arange(len(step_names))\n",
        "bar_colors = [colors.get(status, 'gray') for status in df_ci['Status']]\n",
        "\n",
        "ax2.barh(y_pos, range(len(step_names)), color=bar_colors)\n",
        "ax2.set_yticks(y_pos)\n",
        "ax2.set_yticklabels(step_names)\n",
        "ax2.set_xlabel('Execution Order')\n",
        "ax2.set_title('CI Pipeline Execution Timeline')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Testing Best Practices Summary\n",
        "\n",
        "### 7.1 Key Takeaways\n",
        "\n",
        "Based on our comprehensive testing demonstration, here are the key testing best practices for semiconductor ML pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summarize testing best practices\n",
        "best_practices = {\n",
        "    \"CLI Testing\": [\n",
        "        \"Test all subcommands (train, evaluate, predict)\",\n",
        "        \"Validate JSON output schemas\",\n",
        "        \"Test error handling and exit codes\",\n",
        "        \"Use subprocess.run with proper timeout handling\"\n",
        "    ],\n",
        "    \"Path Validation\": [\n",
        "        \"Test relative path resolution from different module levels\",\n",
        "        \"Ensure cross-platform compatibility\",\n",
        "        \"Validate dataset accessibility before training\",\n",
        "        \"Handle missing datasets gracefully\"\n",
        "    ],\n",
        "    \"Code Quality\": [\n",
        "        \"Use flake8 for linting with complexity limits\",\n",
        "        \"Apply black for consistent formatting\",\n",
        "        \"Set quality thresholds as CI gates\",\n",
        "        \"Automate formatting fixes where possible\"\n",
        "    ],\n",
        "    \"Performance Testing\": [\n",
        "        \"Benchmark critical operations (train, predict)\",\n",
        "        \"Set realistic performance thresholds\",\n",
        "        \"Monitor performance trends over time\",\n",
        "        \"Test prediction latency for real-time applications\"\n",
        "    ],\n",
        "    \"Coverage & Metrics\": [\n",
        "        \"Target 80%+ overall coverage\",\n",
        "        \"Require 95%+ coverage for critical business logic\",\n",
        "        \"Include manufacturing-specific metrics (PWS, loss)\",\n",
        "        \"Track quality metrics trends\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SEMICONDUCTOR ML TESTING BEST PRACTICES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for category, practices in best_practices.items():\n",
        "    print(f\"\\n\ud83d\udd27 {category}:\")\n",
        "    for practice in practices:\n",
        "        print(f\"  \u2022 {practice}\")\n",
        "\n",
        "# Create best practices summary visualization\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "categories = list(best_practices.keys())\n",
        "practice_counts = [len(practices) for practices in best_practices.values()]\n",
        "\n",
        "bars = ax.bar(categories, practice_counts, \n",
        "              color=['lightblue', 'lightgreen', 'orange', 'lightcoral', 'lightpink'])\n",
        "\n",
        "ax.set_ylabel('Number of Best Practices')\n",
        "ax.set_title('Testing Best Practices by Category')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars, practice_counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "            str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Quality Gates Checklist\n",
        "\n",
        "Here's a comprehensive checklist for implementing quality gates in production:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality gates checklist\n",
        "quality_gates_checklist = {\n",
        "    \"Pre-Deployment Gates\": {\n",
        "        \"Unit Tests\": \"95% pass rate\",\n",
        "        \"Integration Tests\": \"100% pass rate\", \n",
        "        \"Code Coverage\": \"\u226580% overall, \u226595% critical paths\",\n",
        "        \"Lint Score\": \"\u226590 (flake8 compliance)\",\n",
        "        \"Code Formatting\": \"100% black compliance\",\n",
        "        \"Dataset Validation\": \"All required datasets accessible\",\n",
        "        \"Performance Benchmarks\": \"Within acceptable thresholds\"\n",
        "    },\n",
        "    \"Manufacturing Specific\": {\n",
        "        \"PWS Accuracy\": \"Prediction Within Spec \u226595%\",\n",
        "        \"False Negative Rate\": \"<1% for critical defects\",\n",
        "        \"Prediction Latency\": \"<100ms for real-time applications\",\n",
        "        \"Model Stability\": \"Consistent results across runs\",\n",
        "        \"Feature Validation\": \"All process parameters within expected ranges\"\n",
        "    },\n",
        "    \"Post-Deployment Monitoring\": {\n",
        "        \"Data Drift Detection\": \"Monitor input distribution shifts\",\n",
        "        \"Performance Degradation\": \"Alert on accuracy decline\",\n",
        "        \"System Health\": \"Continuous availability monitoring\",\n",
        "        \"Model Freshness\": \"Regular retraining schedules\",\n",
        "        \"Audit Trail\": \"Complete prediction logging\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"QUALITY GATES CHECKLIST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for category, gates in quality_gates_checklist.items():\n",
        "    print(f\"\\n\ud83d\udeaa {category}:\")\n",
        "    for gate, threshold in gates.items():\n",
        "        print(f\"  \u2610 {gate}: {threshold}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Remember: Quality gates should be tailored to your specific\")\n",
        "print(\"manufacturing process and business requirements!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated comprehensive testing and quality assurance strategies for semiconductor ML pipelines:\n",
        "\n",
        "1. **CLI Testing Patterns**: How to test command-line interfaces with JSON output validation\n",
        "2. **Dataset Path Validation**: Ensuring data accessibility across different environments\n",
        "3. **Code Quality Assessment**: Automated linting and formatting checks\n",
        "4. **Performance Benchmarking**: Measuring and monitoring pipeline performance\n",
        "5. **Comprehensive QA Suites**: Running full test suites with quality gates\n",
        "6. **CI/CD Integration**: Simulating continuous integration pipelines\n",
        "\n",
        "### Key Benefits for Semiconductor Manufacturing:\n",
        "\n",
        "- **Reliability**: Robust testing ensures consistent model performance\n",
        "- **Maintainability**: Quality gates prevent code degradation\n",
        "- **Scalability**: Automated testing supports rapid development cycles\n",
        "- **Compliance**: Comprehensive testing supports regulatory requirements\n",
        "- **Cost Reduction**: Early defect detection reduces production failures\n",
        "\n",
        "The testing framework and quality gates demonstrated here provide a solid foundation for deploying production-ready ML systems in semiconductor manufacturing environments."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
