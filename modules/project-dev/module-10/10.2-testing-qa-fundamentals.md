# Module 10.2 – Testing & Quality Assurance for Semiconductor ML Pipelines

## 1. Introduction: Quality Gates in Production ML

### Why Testing Matters in Semiconductor Manufacturing

In semiconductor manufacturing, ML models directly impact:
- **Yield prediction accuracy** → $Millions in revenue impact
- **Defect detection reliability** → Product quality and customer satisfaction  
- **Process optimization decisions** → Manufacturing efficiency and cost control
- **Real-time process control** → Equipment downtime and throughput

A single false negative in critical defect detection or inaccurate yield prediction can cascade into significant financial losses. Therefore, comprehensive testing and quality assurance become mission-critical components of any production ML system.

### Testing Strategy Hierarchy

```
├── Unit Tests               # Individual function validation
├── Integration Tests        # Pipeline component interaction
├── CLI/API Tests           # User interface contracts
├── Model Performance Tests  # Statistical validation
├── Data Validation Tests   # Input/output schema compliance
└── Production Monitoring   # Drift detection & alerting
```

## 2. Core Testing Patterns for ML Pipelines

### 2.1 CLI Testing Pattern

For semiconductor ML pipelines following the standard `train/evaluate/predict` CLI pattern:

```python
import subprocess
import json
import sys
from pathlib import Path

def run_pipeline_cmd(script_path: Path, args: List[str]) -> Dict:
    """Execute pipeline CLI command and return JSON response."""
    result = subprocess.run(
        [sys.executable, str(script_path)] + args,
        capture_output=True,
        text=True,
        check=True
    )
    return json.loads(result.stdout)

def test_train_roundtrip():
    """Test complete train -> save -> load -> evaluate cycle."""
    # Train and save model
    train_result = run_pipeline_cmd(PIPELINE_SCRIPT, [
        'train', '--dataset', 'synthetic_yield',
        '--model', 'ridge', '--save', str(model_path)
    ])
    assert train_result['status'] == 'trained'
    assert model_path.exists()

    # Load and evaluate
    eval_result = run_pipeline_cmd(PIPELINE_SCRIPT, [
        'evaluate', '--model-path', str(model_path),
        '--dataset', 'synthetic_yield'
    ])
    assert eval_result['status'] == 'evaluated'
    assert 'r2_score' in eval_result['metrics']
```

### 2.2 JSON Output Validation

All pipeline commands must emit structured JSON for programmatic validation:

```python
def validate_json_schema(response: Dict, expected_keys: Set[str]):
    """Validate JSON response contains required keys."""
    assert isinstance(response, dict), "Response must be valid JSON object"
    missing_keys = expected_keys - response.keys()
    assert not missing_keys, f"Missing required keys: {missing_keys}"

def test_train_output_schema():
    """Validate training output contains required metrics."""
    result = run_pipeline_cmd(SCRIPT, ['train', '--dataset', 'synthetic'])
    validate_json_schema(result, {'status', 'metrics', 'metadata'})

    # Manufacturing-specific metrics validation
    metrics = result['metrics']
    required_metrics = {'mae', 'rmse', 'r2_score', 'pws', 'estimated_loss'}
    validate_json_schema(metrics, required_metrics)
```

### 2.3 Error Handling & Exit Codes

Test both success and failure scenarios:

```python
def test_invalid_model_path():
    """Test graceful handling of missing model files."""
    with pytest.raises(subprocess.CalledProcessError) as exc_info:
        run_pipeline_cmd(SCRIPT, [
            'evaluate', '--model-path', '/nonexistent/model.joblib'
        ])
    assert exc_info.value.returncode != 0

def test_malformed_input_json():
    """Test handling of invalid prediction inputs."""
    with pytest.raises(subprocess.CalledProcessError):
        run_pipeline_cmd(SCRIPT, [
            'predict', '--model-path', str(valid_model),
            '--input-json', '{"invalid": "missing_required_features"}'
        ])
```

## 3. Dataset Path Validation Strategy

### 3.1 Relative Path Resolution

Semiconductor ML projects often have complex dataset hierarchies. Implement robust path validation:

```python
def validate_dataset_paths():
    """Validate all dataset paths are accessible from pipeline locations."""

    # Standard dataset root resolution
    def resolve_data_dir(module_level: int) -> Path:
        """Resolve dataset directory relative to module location."""
        script_dir = Path(__file__).parent
        data_dir = script_dir / ('../' * module_level) / 'datasets'
        return data_dir.resolve()

    # Test path resolution from different module levels
    datasets = {
        'secom': 'secom/secom.data',
        'steel_plates': 'steel-plates/faults.csv',
        'synthetic': None  # Generated on-demand
    }

    for name, path in datasets.items():
        if path:  # Skip synthetic datasets
            full_path = resolve_data_dir(3) / path  # Module 3 level
            assert full_path.exists(), f"Dataset {name} not found at {full_path}"

def test_dataset_loading_from_pipeline():
    """Test dataset loading works from pipeline script location."""
    result = run_pipeline_cmd(SCRIPT, ['train', '--dataset', 'secom'])
    assert result['status'] == 'trained'
    assert result['metadata']['dataset'] == 'secom'
```

### 3.2 Cross-Platform Path Compatibility

Ensure paths work across development environments:

```python
import os
from pathlib import Path

def normalize_dataset_path(path_str: str) -> Path:
    """Convert path string to normalized Path object."""
    return Path(path_str).expanduser().resolve()

def test_path_normalization():
    """Test path handling across different platforms."""
    test_cases = [
        "~/datasets/secom/secom.data",          # Home directory
        "../../../datasets/secom/secom.data",   # Relative paths
        "/absolute/path/to/dataset.csv"         # Absolute paths
    ]

    for path_str in test_cases:
        normalized = normalize_dataset_path(path_str)
        assert isinstance(normalized, Path)
        assert normalized.is_absolute()
```

## 4. Coverage Measurement with pytest-cov

### 4.1 Basic Coverage Setup

```bash
# Install coverage tools
pip install pytest-cov

# Run tests with coverage
pytest --cov=modules/project-dev/module-10 --cov-report=html --cov-report=term

# Generate detailed HTML report
pytest --cov=. --cov-report=html --cov-report=term-missing
```

### 4.2 Coverage Configuration (.coveragerc)

```ini
[run]
source = modules/
omit =
    */test_*.py
    */tests/*
    */venv/*
    */env/*
    setup.py

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:

[html]
directory = htmlcov
```

### 4.3 Coverage Quality Gates

Establish minimum coverage thresholds for production code:

```python
# pytest.ini or pyproject.toml
[tool:pytest]
addopts = --cov=modules --cov-report=term-missing --cov-fail-under=80
```

Target coverage levels:
- **Pipeline core logic**: 95%+ (critical business logic)
- **CLI argument parsing**: 85%+ (user-facing interfaces)  
- **Data loading utilities**: 90%+ (data integrity critical)
- **Overall project**: 80%+ (sustainable maintenance threshold)

## 5. Linting & Code Quality with Flake8 and Black

### 5.1 Flake8 Configuration (.flake8)

```ini
[flake8]
max-line-length = 120
max-complexity = 12
select = E9,F63,F7,F82
ignore =
    E203,  # whitespace before ':'
    E501,  # line too long (handled by black)
    W503,  # line break before binary operator
exclude =
    .git,
    __pycache__,
    build,
    dist,
    *.egg-info,
    venv,
    env
```

### 5.2 Black Configuration (pyproject.toml)

```toml
[tool.black]
line-length = 120
target-version = ['py38', 'py39', 'py310', 'py311']
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''
```

### 5.3 Automated Quality Checks

```python
def test_code_formatting():
    """Test that all Python files are properly formatted."""
    import subprocess

    # Check Black formatting
    result = subprocess.run(['black', '--check', '.'], capture_output=True)
    assert result.returncode == 0, "Code not formatted with Black"

    # Check Flake8 compliance
    result = subprocess.run(['flake8', '.'], capture_output=True)
    assert result.returncode == 0, f"Flake8 violations: {result.stdout.decode()}"
```

## 6. CI/CD Integration Patterns

### 6.1 GitHub Actions Quality Gates

```yaml
name: Quality Assurance
on: [push, pull_request]

jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-basic.txt

      - name: Lint with flake8
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=12 --max-line-length=120 --statistics

      - name: Format check with Black
        run: black --check .

      - name: Test with pytest
        run: |
          pytest --cov=modules --cov-report=xml --cov-fail-under=80

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
```

### 6.2 Pre-commit Hooks (Optional)

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.7.0
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8

  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: pytest
        language: system
        pass_filenames: false
        always_run: true
```

## 7. Smoke Testing Strategy

### 7.1 Import Validation

Test that all modules can be imported without errors:

```python
def test_import_smoke():
    """Smoke test: verify all key modules import successfully."""
    import modules.foundation.module_1.wafer_analysis
    import modules.foundation.module_2.data_quality_framework
    import modules.foundation.module_3.regression_pipeline
    import modules.intermediate.module_4.ensemble_pipeline
    # Add other critical imports

def test_dependency_availability():
    """Test that all required dependencies are available."""
    critical_packages = [
        'numpy', 'pandas', 'scikit-learn', 'matplotlib',
        'jupyter', 'pytest', 'black', 'flake8'
    ]

    for package in critical_packages:
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            pytest.fail(f"Critical package {package} not available")
```

### 7.2 End-to-End Pipeline Testing

```python
def test_complete_pipeline_smoke(tmp_path):
    """Smoke test: full train->evaluate->predict cycle."""
    model_path = tmp_path / 'smoke_model.joblib'

    # Train
    train_result = run_pipeline_cmd(SCRIPT, [
        'train', '--dataset', 'synthetic_yield',
        '--model', 'ridge', '--save', str(model_path)
    ])
    assert train_result['status'] == 'trained'

    # Evaluate
    eval_result = run_pipeline_cmd(SCRIPT, [
        'evaluate', '--model-path', str(model_path),
        '--dataset', 'synthetic_yield'
    ])
    assert eval_result['status'] == 'evaluated'

    # Predict
    pred_result = run_pipeline_cmd(SCRIPT, [
        'predict', '--model-path', str(model_path),
        '--input-json', '{"temperature":450,"pressure":2.5,"flow":120,"time":60}'
    ])
    assert 'prediction' in pred_result
```

## 8. Documentation Testing

### 8.1 Docstring Validation

```python
def test_docstring_coverage():
    """Test that all public functions have docstrings."""
    import inspect
    from modules.project_dev.module_10.testing_qa_pipeline import TestingQAPipeline

    public_methods = [
        method for method in dir(TestingQAPipeline)
        if not method.startswith('_') and callable(getattr(TestingQAPipeline, method))
    ]

    for method_name in public_methods:
        method = getattr(TestingQAPipeline, method_name)
        assert method.__doc__, f"Method {method_name} lacks docstring"
```

### 8.2 Example Code Validation

Test that code examples in documentation actually work:

```python
def test_documentation_examples():
    """Test that code examples in documentation execute successfully."""
    # Extract and test code blocks from markdown files
    import re

    doc_file = Path(__file__).parent / "10.2-testing-qa-fundamentals.md"
    content = doc_file.read_text()

    # Find Python code blocks
    code_blocks = re.findall(r'```python\n(.*?)\n```', content, re.DOTALL)

    for i, code in enumerate(code_blocks):
        try:
            exec(code)
        except Exception as e:
            pytest.fail(f"Documentation example {i+1} failed: {e}")
```

## 9. Performance Testing for ML Pipelines

### 9.1 Training Time Benchmarks

```python
import time

def test_training_performance():
    """Test that training completes within acceptable time limits."""
    start_time = time.time()

    result = run_pipeline_cmd(SCRIPT, [
        'train', '--dataset', 'synthetic_yield', '--model', 'ridge'
    ])

    training_time = time.time() - start_time

    assert result['status'] == 'trained'
    assert training_time < 30, f"Training took {training_time:.2f}s, expected <30s"

def test_prediction_latency():
    """Test prediction response time for real-time applications."""
    # Pre-train model
    model_path = train_test_model()

    start_time = time.time()
    result = run_pipeline_cmd(SCRIPT, [
        'predict', '--model-path', str(model_path),
        '--input-json', '{"temperature":450,"pressure":2.5,"flow":120,"time":60}'
    ])
    prediction_time = time.time() - start_time

    assert 'prediction' in result
    assert prediction_time < 1.0, f"Prediction took {prediction_time:.2f}s, expected <1s"
```

## 10. Quality Metrics and Manufacturing KPIs

### 10.1 Manufacturing-Specific Test Validation

```python
def test_manufacturing_metrics_validity():
    """Test that manufacturing KPIs are calculated correctly."""
    result = run_pipeline_cmd(SCRIPT, ['train', '--dataset', 'synthetic_yield'])
    metrics = result['metrics']

    # Prediction Within Spec (PWS) validation
    pws = metrics.get('pws')
    assert pws is not None, "PWS metric missing"
    assert 0 <= pws <= 100, f"PWS {pws} outside valid range [0,100]"

    # Estimated Loss validation
    est_loss = metrics.get('estimated_loss')
    assert est_loss is not None, "Estimated loss metric missing"
    assert est_loss >= 0, f"Estimated loss {est_loss} cannot be negative"

    # Standard ML metrics
    assert 'mae' in metrics and metrics['mae'] >= 0
    assert 'rmse' in metrics and metrics['rmse'] >= 0
    assert 'r2_score' in metrics and -1 <= metrics['r2_score'] <= 1
```

## 11. Test Organization and Best Practices

### 11.1 Test File Structure

```
modules/project-dev/module-10/
├── 10.2-testing-qa-pipeline.py      # Main pipeline script
├── test_testing_qa_pipeline.py      # CLI and integration tests
├── tests/                           # Additional test modules
│   ├── __init__.py
│   ├── test_unit_functions.py       # Unit tests
│   ├── test_data_validation.py      # Data validation tests
│   └── test_performance.py          # Performance benchmarks
└── fixtures/                        # Test data and fixtures
    ├── sample_data.csv
    └── expected_outputs.json
```

### 11.2 Test Naming Conventions

- `test_*_success` - Happy path scenarios
- `test_*_failure` - Error handling scenarios  
- `test_*_edge_cases` - Boundary conditions
- `test_*_performance` - Speed/memory benchmarks
- `test_*_integration` - Multi-component interactions

### 11.3 Test Data Management

```python
@pytest.fixture
def sample_semiconductor_data():
    """Generate consistent test data for semiconductor manufacturing."""
    np.random.seed(42)
    return pd.DataFrame({
        'temperature': np.random.normal(450, 15, 100),
        'pressure': np.random.normal(2.5, 0.3, 100),
        'flow': np.random.normal(120, 10, 100),
        'time': np.random.normal(60, 5, 100),
        'yield': np.random.normal(85, 8, 100)
    })

@pytest.fixture
def trained_model(tmp_path, sample_semiconductor_data):
    """Provide a pre-trained model for testing."""
    model_path = tmp_path / 'test_model.joblib'
    # Train and save model
    # Return path for use in tests
    return model_path
```

## 12. Continuous Quality Improvement

### 12.1 Test Metrics Tracking

Track test suite health over time:
- Test execution time trends
- Coverage percentage changes
- Flaky test identification
- Test failure root cause analysis

### 12.2 Quality Gate Evolution

As the codebase matures, progressively tighten quality gates:
- **Phase 1**: Basic functionality testing
- **Phase 2**: Add performance benchmarks  
- **Phase 3**: Increase coverage requirements
- **Phase 4**: Add security and compliance testing

## 13. Next Steps and Advanced Topics

1. **Mutation Testing**: Use tools like `mutmut` to test test quality
2. **Property-Based Testing**: Use `hypothesis` for edge case discovery
3. **Contract Testing**: Validate API contracts between pipeline components
4. **Load Testing**: Simulate production workloads for stress testing
5. **Security Testing**: Validate input sanitization and access controls

## References

- [pytest Documentation](https://docs.pytest.org/)
- [pytest-cov Coverage Testing](https://pytest-cov.readthedocs.io/)
- [Black Code Formatter](https://black.readthedocs.io/)
- [Flake8 Linting Tool](https://flake8.pycqa.org/)
- [GitHub Actions CI/CD](https://docs.github.com/en/actions)
- [Pre-commit Hooks](https://pre-commit.com/)
