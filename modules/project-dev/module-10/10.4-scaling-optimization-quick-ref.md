# Module 10.4 Quick Reference: Scaling & Optimization

## ⚡ Vectorization Cheat Sheet

### Replace Python loops with NumPy operations
```python
# ❌ Slow: Python loop
result = []
for i in range(len(data)):
    result.append(data[i] ** 2)

# ✅ Fast: Vectorized
result = data ** 2
```

### Common vectorization patterns
```python
# Element-wise operations
np.sqrt(pressures)
np.log(temperatures)
temperatures * flow_rates

# Boolean operations
high_yield = yields > 95
defective = np.where(quality < threshold, 'fail', 'pass')

# Statistical operations
np.mean(data, axis=0)
np.std(data, axis=1)
```

## 🔄 Parallel Processing

### joblib for CPU-intensive tasks
```python
from joblib import Parallel, delayed

# Parallel processing
results = Parallel(n_jobs=-1)(
    delayed(process_wafer)(wafer) 
    for wafer in wafer_batch
)

# Parallel with progress
from tqdm import tqdm
results = Parallel(n_jobs=-1)(
    delayed(process_wafer)(wafer) 
    for wafer in tqdm(wafer_batch)
)
```

### Batch processing pattern
```python
def parallel_batch_processing(data, batch_size=1000, n_jobs=-1):
    batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
    results = Parallel(n_jobs=n_jobs)(
        delayed(process_batch)(batch) for batch in batches
    )
    return np.concatenate(results)
```

## 📊 Profiling Quick Commands

### Time profiling
```python
import time

# Basic timing
start = time.perf_counter()
result = function()
elapsed = time.perf_counter() - start

# Context manager
from contextlib import contextmanager

@contextmanager
def timer(name="Operation"):
    start = time.perf_counter()
    yield
    print(f"{name}: {time.perf_counter() - start:.4f}s")

with timer("Feature engineering"):
    features = engineer_features(data)
```

### Memory profiling
```python
import tracemalloc

# Basic memory tracking
tracemalloc.start()
result = memory_intensive_function()
current, peak = tracemalloc.get_traced_memory()
print(f"Peak: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

### Built-in profiler
```python
import cProfile
import pstats

# Profile code
pr = cProfile.Profile()
pr.enable()
your_function()
pr.disable()

# Show results
stats = pstats.Stats(pr)
stats.sort_stats('cumulative').print_stats(10)
```

## 💾 Caching with joblib.Memory

### Basic caching setup
```python
from joblib import Memory

memory = Memory('/tmp/cache', verbose=1)

@memory.cache
def expensive_function(data_hash, data):
    # Expensive computation
    return result

# Use with hash for cache key
data_hash = hash(str(data.values.tobytes()))
result = expensive_function(data_hash, data)
```

### Cache management
```python
# Clear all cache
memory.clear()

# Clear specific function
expensive_function.clear()

# Check cache size
size_mb = memory.bytes_used() / 1024 / 1024
print(f"Cache size: {size_mb:.1f} MB")
```

## 🔄 Incremental Learning

### SGD with partial_fit
```python
from sklearn.linear_model import SGDRegressor

model = SGDRegressor(random_state=42)
scaler = StandardScaler()

# First batch: fit scaler
X_scaled = scaler.fit_transform(X_first_batch)
model.partial_fit(X_scaled, y_first_batch)

# Subsequent batches
for X_batch, y_batch in data_batches:
    X_scaled = scaler.transform(X_batch)
    model.partial_fit(X_scaled, y_batch)
```

### Online learning class template
```python
class OnlineML:
    def __init__(self):
        self.model = SGDRegressor()
        self.scaler = StandardScaler()
        self.fitted = False
    
    def partial_fit(self, X, y):
        if not self.fitted:
            X_scaled = self.scaler.fit_transform(X)
            self.model.partial_fit(X_scaled, y)
            self.fitted = True
        else:
            X_scaled = self.scaler.transform(X)
            self.model.partial_fit(X_scaled, y)
```

## 🛠️ Optimization Patterns

### Data type optimization
```python
# Use smaller data types
df['chamber_id'] = df['chamber_id'].astype('int8')
df['yield'] = df['yield'].astype('float32')
df['process_step'] = df['process_step'].astype('category')
```

### Memory-efficient iteration
```python
# Process large files in chunks
for chunk in pd.read_csv('large_file.csv', chunksize=10000):
    process_chunk(chunk)

# Memory mapping for huge arrays
data = np.memmap('huge_data.dat', dtype='float32', mode='r')
```

### Optimal batch sizes
```python
# Find optimal batch size
def find_optimal_batch_size(data_size, memory_limit_mb=1000):
    bytes_per_sample = 1024  # Estimate
    max_samples = (memory_limit_mb * 1024 * 1024) // bytes_per_sample
    return min(max_samples, data_size)
```

## 📈 Performance Monitoring

### Simple performance tracker
```python
class PerfTracker:
    def __init__(self):
        self.times = {}
        self.memory = {}
    
    def time_it(self, name, func, *args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        self.times[name] = time.perf_counter() - start
        return result
    
    def report(self):
        for name, duration in self.times.items():
            print(f"{name}: {duration:.4f}s")
```

### Resource monitoring
```python
import psutil

# Current memory usage
memory_mb = psutil.Process().memory_info().rss / 1024 / 1024

# CPU usage
cpu_percent = psutil.cpu_percent(interval=1)

# Available memory
available_gb = psutil.virtual_memory().available / 1024**3
```

## ⚡ Manufacturing-Specific Optimizations

### Real-time inference optimization
```python
# Precompute static features
@memory.cache
def precompute_chamber_stats(chamber_data):
    return chamber_data.groupby('chamber_id').agg({
        'temperature': ['mean', 'std'],
        'pressure': ['mean', 'std']
    })

# Use lighter models for real-time
from sklearn.tree import DecisionTreeRegressor
fast_model = DecisionTreeRegressor(max_depth=5)  # Limited depth
```

### Edge computing patterns
```python
# Memory-efficient preprocessing
class EdgePreprocessor:
    def __init__(self):
        self.mins = None
        self.ranges = None
    
    def fit(self, X):
        self.mins = X.min(axis=0)
        self.ranges = X.max(axis=0) - self.mins
    
    def transform(self, X):
        return (X - self.mins) / self.ranges  # Simpler than StandardScaler
```

## 🎯 Optimization Decision Tree

```
Performance Issue?
├── High latency?
│   ├── Use vectorization
│   ├── Reduce model complexity
│   └── Precompute features
├── High memory usage?
│   ├── Use smaller data types
│   ├── Process in chunks
│   └── Use memory mapping
├── Repeated computations?
│   ├── Add caching
│   └── Precompute static features
└── Large datasets?
    ├── Use incremental learning
    ├── Parallel processing
    └── Optimize I/O
```

## 📋 Optimization Checklist

**Before optimizing:**
- [ ] Profile to identify actual bottlenecks
- [ ] Measure baseline performance
- [ ] Set specific performance targets

**Vectorization:**
- [ ] Replace Python loops with NumPy operations
- [ ] Use pandas vectorized string/datetime operations
- [ ] Leverage broadcasting for array operations

**Parallelization:**
- [ ] Identify CPU-intensive, independent tasks
- [ ] Use joblib for embarrassingly parallel problems
- [ ] Consider batch size vs. overhead trade-offs

**Memory optimization:**
- [ ] Use appropriate data types (int8, float32, category)
- [ ] Process data in chunks for large datasets
- [ ] Monitor memory usage in production

**Caching:**
- [ ] Cache expensive computations
- [ ] Use stable cache keys (hashes)
- [ ] Monitor and manage cache size

**Incremental learning:**
- [ ] Use partial_fit for streaming data
- [ ] Fit preprocessing on representative sample
- [ ] Monitor for concept drift

**Production readiness:**
- [ ] Monitor latency, throughput, memory
- [ ] Set up automated performance alerts
- [ ] Have fallback strategies for performance issues

## 🚀 Quick Performance Wins

1. **Replace loops**: `for i in range(len(data))` → `np.vectorize` or direct operations
2. **Use proper dtypes**: `float64` → `float32`, `object` → `category`
3. **Cache expensive ops**: Add `@memory.cache` to slow functions
4. **Parallel processing**: Use `joblib.Parallel` for CPU-bound tasks
5. **Chunk large data**: `pd.read_csv(chunksize=10000)` for big files
6. **Profile first**: Use `cProfile` to find actual bottlenecks

## 📚 Key Libraries Reference

```python
# Core optimization
import numpy as np
import pandas as pd
from joblib import Parallel, delayed, Memory

# Profiling
import time
import tracemalloc
import cProfile
import psutil

# Incremental learning
from sklearn.linear_model import SGDRegressor, SGDClassifier
from sklearn.preprocessing import StandardScaler

# Memory mapping
data = np.memmap('file.dat', dtype='float32', mode='r')
```

## 🎯 When to Use What

| Situation | Solution |
|-----------|----------|
| Python loops on arrays | NumPy vectorization |
| CPU-intensive tasks | joblib.Parallel |
| Repeated expensive computations | joblib.Memory caching |
| Large datasets (> RAM) | Incremental learning |
| Real-time constraints | Model simplification + precomputation |
| Memory constraints | Data type optimization + chunking |
| Unknown bottlenecks | Profile with cProfile first |

Remember: **Measure first, optimize second!** 🎯