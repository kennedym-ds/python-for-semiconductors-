# Module 10.2 Quick Reference: Testing & QA for Semiconductor ML

## Essential CLI Commands

### QA Pipeline Script Usage
```bash
# Run comprehensive test suite
python 10.2-testing-qa-pipeline.py train --test-suite all --coverage-threshold 80

# Code quality check with auto-fix
python 10.2-testing-qa-pipeline.py evaluate --check-type lint --fix-issues

# Dataset path validation
python 10.2-testing-qa-pipeline.py evaluate --check-type paths

# System health check
python 10.2-testing-qa-pipeline.py predict

# Performance benchmarking
python 10.2-testing-qa-pipeline.py evaluate --check-type performance
```

### Standard Test Commands
```bash
# Run tests with coverage
pytest --cov=modules --cov-report=html --cov-report=term-missing

# Code formatting
black --check .
black .  # Auto-fix

# Linting
flake8 . --count --statistics
flake8 . --max-complexity=12 --max-line-length=120
```

## Testing Patterns

### 1. CLI Testing Template
```python
import subprocess
import json
import sys
from pathlib import Path

def run_cmd(script_path, args):
    result = subprocess.run(
        [sys.executable, str(script_path)] + args,
        capture_output=True, text=True, check=True
    )
    return json.loads(result.stdout)

def test_train_roundtrip(tmp_path):
    model_path = tmp_path / 'model.joblib'
    
    # Train and save
    train_out = run_cmd(SCRIPT, [
        'train', '--dataset', 'synthetic',
        '--save', str(model_path)
    ])
    assert train_out['status'] == 'trained'
    assert model_path.exists()
    
    # Load and evaluate
    eval_out = run_cmd(SCRIPT, [
        'evaluate', '--model-path', str(model_path)
    ])
    assert eval_out['status'] == 'evaluated'
```

### 2. Dataset Path Validation
```python
def validate_dataset_paths(module_level=3):
    script_dir = Path(__file__).parent
    data_dir = script_dir / ('../' * module_level) / 'datasets'
    data_dir = data_dir.resolve()
    
    datasets = {
        'secom': 'secom/secom.data',
        'steel_plates': 'steel-plates/faults.csv'
    }
    
    for name, path in datasets.items():
        full_path = data_dir / path
        assert full_path.exists(), f"Dataset {name} not found"
```

### 3. JSON Schema Validation
```python
def validate_json_schema(response, expected_keys):
    assert isinstance(response, dict)
    missing_keys = expected_keys - response.keys()
    assert not missing_keys, f"Missing keys: {missing_keys}"

def test_output_schema():
    result = run_cmd(SCRIPT, ['train'])
    validate_json_schema(result, {'status', 'metrics', 'metadata'})
    
    # Manufacturing metrics validation
    metrics = result['metrics']
    required_metrics = {'mae', 'rmse', 'r2_score', 'pws', 'estimated_loss'}
    validate_json_schema(metrics, required_metrics)
```

### 4. Error Handling Tests
```python
def test_invalid_inputs():
    # Missing model file
    with pytest.raises(subprocess.CalledProcessError):
        run_cmd(SCRIPT, ['evaluate', '--model-path', '/nonexistent/model.joblib'])
    
    # Invalid JSON input
    with pytest.raises(subprocess.CalledProcessError):
        run_cmd(SCRIPT, ['predict', '--input-json', 'invalid_json'])
```

## Configuration Files

### 1. .flake8 Configuration
```ini
[flake8]
max-line-length = 120
max-complexity = 12
select = E9,F63,F7,F82
ignore = E203,E501,W503
exclude = .git,__pycache__,build,dist,*.egg-info,venv
```

### 2. pytest.ini / pyproject.toml
```ini
[tool:pytest]
addopts = --cov=modules --cov-report=term-missing --cov-fail-under=80
testpaths = modules/
python_files = test_*.py
python_functions = test_*
```

### 3. .coveragerc
```ini
[run]
source = modules/
omit = */test_*.py, */tests/*, setup.py

[report]
exclude_lines = 
    pragma: no cover
    def __repr__
    raise NotImplementedError

[html]
directory = htmlcov
```

### 4. Black Configuration (pyproject.toml)
```toml
[tool.black]
line-length = 120
target-version = ['py38', 'py39', 'py310', 'py311']
include = '\.pyi?$'
exclude = '''
/(\.git|\.hg|\.mypy_cache|\.tox|\.venv|_build|buck-out|build|dist)/
'''
```

## Quality Gates Checklist

### Pre-Deployment Gates
- [ ] **Unit Tests**: 95%+ pass rate
- [ ] **Integration Tests**: 100% pass rate  
- [ ] **Code Coverage**: ≥80% overall, ≥95% critical paths
- [ ] **Lint Score**: ≥90 (flake8 compliance)
- [ ] **Code Formatting**: 100% black compliance
- [ ] **Dataset Validation**: All datasets accessible
- [ ] **Performance**: Within thresholds

### Manufacturing-Specific Gates
- [ ] **PWS Accuracy**: Prediction Within Spec ≥95%
- [ ] **False Negative Rate**: <1% for critical defects
- [ ] **Prediction Latency**: <100ms for real-time
- [ ] **Model Stability**: Consistent across runs
- [ ] **Feature Validation**: Process parameters in range

### CI/CD Pipeline Steps
1. **Code Checkout** → Clone repository
2. **Dependencies** → Install requirements-basic.txt
3. **Linting** → flake8 compliance check
4. **Formatting** → black formatting verification
5. **Path Validation** → Dataset accessibility
6. **Smoke Tests** → Basic functionality
7. **Performance** → Benchmark critical operations
8. **Full Suite** → Comprehensive test execution

## Common Test Fixtures

### Synthetic Data Generator
```python
@pytest.fixture
def sample_semiconductor_data():
    np.random.seed(42)
    return pd.DataFrame({
        'temperature': np.random.normal(450, 15, 100),
        'pressure': np.random.normal(2.5, 0.3, 100),
        'flow': np.random.normal(120, 10, 100),
        'time': np.random.normal(60, 5, 100),
        'yield': np.random.normal(85, 8, 100)
    })
```

### Trained Model Fixture
```python
@pytest.fixture
def trained_model(tmp_path, sample_semiconductor_data):
    from sklearn.linear_model import Ridge
    
    model_path = tmp_path / 'test_model.joblib'
    X = sample_semiconductor_data.drop('yield', axis=1)
    y = sample_semiconductor_data['yield']
    
    model = Ridge(random_state=42)
    model.fit(X, y)
    
    joblib.dump(model, model_path)
    return model_path
```

## Performance Benchmarks

### Target Thresholds
| Operation | Threshold | Critical |
|-----------|-----------|----------|
| Data Generation | <1s | No |
| Model Training | <30s | Yes |
| Single Prediction | <100ms | Yes |
| Batch Prediction | <5s/1000 | No |
| Model Loading | <2s | Yes |

### Performance Test Example
```python
def test_prediction_latency():
    model_path = train_test_model()
    
    start_time = time.time()
    result = run_cmd(SCRIPT, [
        'predict', '--model-path', str(model_path),
        '--input-json', '{"temperature":450,"pressure":2.5}'
    ])
    latency = time.time() - start_time
    
    assert 'prediction' in result
    assert latency < 0.1, f"Latency {latency:.3f}s exceeds 100ms threshold"
```

## Coverage Targets

### By Component Type
- **Pipeline Core Logic**: 95%+ (business critical)
- **CLI Argument Parsing**: 85%+ (user interface)
- **Data Loading Utilities**: 90%+ (data integrity)
- **Error Handling**: 100% (robustness)
- **Overall Project**: 80%+ (maintainability)

### Coverage Commands
```bash
# Generate HTML coverage report
pytest --cov=modules --cov-report=html

# Terminal coverage with missing lines
pytest --cov=modules --cov-report=term-missing

# Fail if coverage below threshold
pytest --cov=modules --cov-fail-under=80

# Coverage for specific module
pytest --cov=modules.foundation.module_3 --cov-report=term
```

## Manufacturing Metrics Validation

### Required Metrics for Semiconductor ML
```python
def validate_manufacturing_metrics(metrics):
    """Validate semiconductor-specific metrics."""
    
    # Standard ML metrics
    assert 'mae' in metrics and metrics['mae'] >= 0
    assert 'rmse' in metrics and metrics['rmse'] >= 0  
    assert 'r2_score' in metrics and -1 <= metrics['r2_score'] <= 1
    
    # Manufacturing-specific metrics
    pws = metrics.get('pws')  # Prediction Within Spec
    assert pws is not None and 0 <= pws <= 100
    
    est_loss = metrics.get('estimated_loss')
    assert est_loss is not None and est_loss >= 0
```

### Metric Calculation Examples
```python
# Prediction Within Spec (PWS)
def calculate_pws(y_true, y_pred, lower_spec, upper_spec):
    within_spec = ((y_pred >= lower_spec) & (y_pred <= upper_spec)).mean()
    return within_spec * 100

# Estimated Manufacturing Loss
def calculate_estimated_loss(y_true, y_pred, cost_per_unit=1000):
    mae = np.mean(np.abs(y_true - y_pred))
    return mae * cost_per_unit
```

## Troubleshooting Guide

### Common Issues

#### Test Failures
```bash
# Debug specific test
pytest -v -s test_specific_function

# Run only failed tests
pytest --lf

# Stop on first failure
pytest -x
```

#### Coverage Issues
```bash
# See which lines are missing coverage
pytest --cov=modules --cov-report=term-missing

# Generate detailed HTML report
pytest --cov=modules --cov-report=html
# Open htmlcov/index.html in browser
```

#### Path Resolution Errors
```python
# Debug path resolution
def debug_paths():
    script_dir = Path(__file__).parent
    print(f"Script dir: {script_dir}")
    print(f"Resolved: {script_dir.resolve()}")
    print(f"Parent: {script_dir.parent}")
    print(f"Datasets: {script_dir / '../../../datasets'}")
```

#### Import Errors
```python
# Check import availability
def check_imports():
    required = ['numpy', 'pandas', 'sklearn', 'pytest']
    for pkg in required:
        try:
            __import__(pkg)
            print(f"✅ {pkg}")
        except ImportError as e:
            print(f"❌ {pkg}: {e}")
```

## Quick Commands Reference

```bash
# Full QA pipeline
python 10.2-testing-qa-pipeline.py train --test-suite all

# Just smoke tests
python 10.2-testing-qa-pipeline.py train --test-suite smoke

# Check code quality
python 10.2-testing-qa-pipeline.py evaluate --check-type lint

# Validate datasets
python 10.2-testing-qa-pipeline.py evaluate --check-type paths

# Performance benchmark
python 10.2-testing-qa-pipeline.py evaluate --check-type performance

# System health
python 10.2-testing-qa-pipeline.py predict

# Help for any command
python 10.2-testing-qa-pipeline.py --help
python 10.2-testing-qa-pipeline.py train --help
```

Remember: Adapt quality thresholds and metrics to your specific semiconductor manufacturing requirements!