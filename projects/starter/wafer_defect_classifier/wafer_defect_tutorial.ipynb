{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wafer Defect Classification Tutorial\n",
        "\n",
        "This tutorial demonstrates how to build and deploy a production-ready wafer defect classification system using classical machine learning approaches.\n",
        "\n",
        "## Business Context\n",
        "\n",
        "In semiconductor manufacturing, wafer defect detection is critical for:\n",
        "- **Quality Control**: Early detection prevents defective dies from reaching customers\n",
        "- **Cost Reduction**: Identifying process issues before they impact entire lots\n",
        "- **Process Optimization**: Understanding defect patterns to improve manufacturing\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "1. Understand semiconductor defect classification challenges\n",
        "2. Build and compare multiple ML models for defect detection\n",
        "3. Apply manufacturing-specific metrics (PWS, Estimated Loss)\n",
        "4. Deploy models using standardized CLI interface\n",
        "5. Optimize model thresholds for precision/recall constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our wafer defect pipeline\n",
        "from wafer_defect_pipeline import (\n",
        "    WaferDefectPipeline, \n",
        "    generate_synthetic_wafer_defects,\n",
        "    load_dataset\n",
        ")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Generation and Exploration\n",
        "\n",
        "Let's start by generating synthetic wafer defect data to understand the problem.\n",
        "\n",
        "### \ud83c\udfaf Exercise 1.1: Generate Synthetic Data (\u2605 Beginner)\n",
        "\n",
        "**Task**: Generate 1000 wafer samples with 20% defect rate and explore the dataset.\n",
        "\n",
        "**Requirements**:\n",
        "1. Use `generate_synthetic_wafer_defects()` to create the dataset\n",
        "2. Calculate and print the class distribution (how many defective vs non-defective)\n",
        "3. Calculate the imbalance ratio (e.g., 4:1 means 4 good wafers per 1 defective)\n",
        "\n",
        "**Hints**:\n",
        "- Use `n_samples=1000` and `defect_rate=0.20`\n",
        "- Use `.value_counts()` on the target column to get class distribution\n",
        "- Imbalance ratio = count of class 0 / count of class 1\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Generate synthetic wafer defect data with 1000 samples and 20% defect rate\n",
        "# Your code here:\n",
        "# df = generate_synthetic_wafer_defects(...)\n",
        "\n",
        "# Print dataset shape\n",
        "# print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# TODO: Calculate class distribution\n",
        "# Your code here:\n",
        "# class_counts = ...\n",
        "# print(\"\\n=== Class Distribution ===\")\n",
        "# print(f\"Non-defective wafers (0): ...\")\n",
        "# print(f\"Defective wafers (1): ...\")\n",
        "\n",
        "# TODO: Calculate imbalance ratio\n",
        "# imbalance_ratio = ...\n",
        "# print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "# \u2705 Self-check: \n",
        "# - Do you have exactly 1000 samples?\n",
        "# - Is the defect rate approximately 20%?\n",
        "# - Is the imbalance ratio approximately 4:1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 1.1\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] Dataset has 1000 rows\n",
        "- [ ] Approximately 200 defective wafers (180-220 is normal due to randomness)\n",
        "- [ ] Imbalance ratio is around 4:1\n",
        "- [ ] You understand what defect_rate parameter controls\n",
        "\n",
        "\ud83d\udca1 **Stuck?** Check the solution notebook for the complete implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83c\udfaf Exercise 1.2: Visualize Feature Distributions (\u2605\u2605 Intermediate)\n",
        "\n",
        "**Task**: Create visualizations to understand which features distinguish defective wafers.\n",
        "\n",
        "**Requirements**:\n",
        "1. Create violin plots comparing defective vs non-defective wafers for top 3 features\n",
        "2. Calculate correlation of each feature with the target\n",
        "3. Identify the 3 most discriminative features\n",
        "\n",
        "**Hints**:\n",
        "- Use `.corrwith()` to calculate correlations with target\n",
        "- Take absolute value of correlations (both positive and negative correlations are important)\n",
        "- Sort by correlation in descending order\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Get feature columns (exclude 'defect' and 'wafer_id')\n",
        "# feature_cols = [col for col in df.columns if col not in ['defect', 'wafer_id']]\n",
        "\n",
        "# TODO: Calculate correlations with target\n",
        "# correlations = ...\n",
        "\n",
        "# TODO: Get top 3 features\n",
        "# top_3 = correlations.head(3)\n",
        "# print(\"Top 3 Most Discriminative Features:\")\n",
        "# print(top_3)\n",
        "\n",
        "# TODO: Create violin plots for top 3 features\n",
        "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "# for idx, feature in enumerate(top_3.index):\n",
        "#     ax = axes[idx]\n",
        "#     # Create violin plot comparing defective vs non-defective\n",
        "#     # Your plotting code here...\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Do the top features show clear separation between classes?\n",
        "# - Are correlation values above 0.3?\n",
        "# - Do violin plots show different distributions for defective vs good wafers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 1.2\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] You identified 3 features with highest absolute correlation\n",
        "- [ ] Correlation values make sense (between 0 and 1)\n",
        "- [ ] Violin plots show visual differences between classes\n",
        "- [ ] You understand why higher correlation = better feature\n",
        "\n",
        "\ud83d\udca1 **Manufacturing Insight**: In real semiconductor fabs, these correlations would guide which process parameters to monitor most closely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training and Comparison\n",
        "\n",
        "Now let's train and compare different ML models for wafer defect classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training and Comparison\n",
        "\n",
        "Now let's train multiple ML models and compare their performance.\n",
        "\n",
        "### \ud83c\udfaf Exercise 2.1: Prepare Data and Train Models (\u2605\u2605 Intermediate)\n",
        "\n",
        "**Task**: Split the data and train 5 different classification models.\n",
        "\n",
        "**Requirements**:\n",
        "1. Create stratified 80/20 train/test split (preserve class distribution)\n",
        "2. Train these 5 models:\n",
        "   - Logistic Regression\n",
        "   - Linear SVM\n",
        "   - Decision Tree\n",
        "   - Random Forest\n",
        "   - Gradient Boosting\n",
        "3. Evaluate each with ROC-AUC, PR-AUC, F1, and PWS metrics\n",
        "4. Store results in a DataFrame for comparison\n",
        "\n",
        "**Hints**:\n",
        "- Use `train_test_split(stratify=y)` to maintain class balance\n",
        "- Instantiate each pipeline with default parameters\n",
        "- Use `.fit()` to train and `.evaluate()` to get metrics\n",
        "- Store results in a list of dictionaries\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Prepare features and target\n",
        "# X = df.drop(['defect', 'wafer_id'], axis=1)\n",
        "# y = df['defect']\n",
        "\n",
        "# TODO: Create stratified train/test split (80/20)\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y, test_size=0.2, stratify=..., random_state=42\n",
        "# )\n",
        "\n",
        "# print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "# print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "# print(f\"Training defect rate: {y_train.mean():.1%}\")\n",
        "# print(f\"Test defect rate: {y_test.mean():.1%}\")\n",
        "\n",
        "# TODO: Define model configurations\n",
        "# model_configs = {\n",
        "#     'logistic': 'logistic_regression',\n",
        "#     'linear_svm': 'linear_svm',\n",
        "#     'tree': 'decision_tree',\n",
        "#     'rf': 'random_forest',\n",
        "#     'gb': 'gradient_boosting'\n",
        "# }\n",
        "\n",
        "# TODO: Train and evaluate each model\n",
        "# results = []\n",
        "# for name, model_type in model_configs.items():\n",
        "#     print(f\"\\nTraining {name}...\")\n",
        "#     # Instantiate pipeline\n",
        "#     pipeline = WaferDefectPipeline(model_type=model_type)\n",
        "#     \n",
        "#     # Train\n",
        "#     pipeline.fit(X_train, y_train)\n",
        "#     \n",
        "#     # Evaluate\n",
        "#     metrics = pipeline.evaluate(X_test, y_test)\n",
        "#     \n",
        "#     # Store results\n",
        "#     results.append({\n",
        "#         'model': name,\n",
        "#         'roc_auc': metrics['roc_auc'],\n",
        "#         'pr_auc': metrics['pr_auc'],\n",
        "#         'f1': metrics['f1'],\n",
        "#         'pws': metrics['pws']\n",
        "#     })\n",
        "\n",
        "# TODO: Create results DataFrame\n",
        "# results_df = pd.DataFrame(results)\n",
        "# print(\"\\n=== Model Comparison ===\")\n",
        "# print(results_df.to_string(index=False))\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Do train and test sets have similar defect rates?\n",
        "# - Did all 5 models train successfully?\n",
        "# - Are ROC-AUC scores above 0.5 (better than random)?\n",
        "# - Do ensemble models (rf, gb) perform better than linear models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 2.1\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] Stratified split maintains class distribution (~20% in both train and test)\n",
        "- [ ] All 5 models trained without errors\n",
        "- [ ] Results DataFrame has 5 rows (one per model)\n",
        "- [ ] ROC-AUC scores are between 0.5 and 1.0\n",
        "- [ ] You understand why stratification is critical for imbalanced data\n",
        "\n",
        "\ud83d\udca1 **Common Mistake**: Forgetting `stratify=y` leads to different class distributions in train/test, causing unreliable metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83c\udfaf Exercise 2.2: Visualize Model Performance (\u2605\u2605 Intermediate)\n",
        "\n",
        "**Task**: Create visualizations to compare model performance across all metrics.\n",
        "\n",
        "**Requirements**:\n",
        "1. Create 4 bar charts (one for each metric: ROC-AUC, PR-AUC, F1, PWS)\n",
        "2. Identify the best-performing model\n",
        "3. Explain why certain models perform better for this task\n",
        "\n",
        "**Hints**:\n",
        "- Use `plt.subplots(2, 2)` to create a 2x2 grid\n",
        "- Sort models by metric for easier comparison\n",
        "- Color the best model differently\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create 2x2 grid of bar charts\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# metrics_to_plot = ['roc_auc', 'pr_auc', 'f1', 'pws']\n",
        "# titles = ['ROC-AUC Score', 'PR-AUC Score', 'F1 Score', 'Prediction Within Spec (PWS)']\n",
        "\n",
        "# for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
        "#     ax = axes[idx // 2, idx % 2]\n",
        "#     # TODO: Sort results by current metric\n",
        "#     sorted_results = results_df.sort_values(metric, ascending=False)\n",
        "#     \n",
        "#     # TODO: Create bar chart\n",
        "#     # ax.bar(sorted_results['model'], sorted_results[metric])\n",
        "#     # ax.set_title(title)\n",
        "#     # ax.set_ylabel('Score')\n",
        "#     # ax.set_xlabel('Model')\n",
        "#     # ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# TODO: Identify best model\n",
        "# best_model = results_df.loc[results_df['roc_auc'].idxmax(), 'model']\n",
        "# print(f\"\\n\ud83c\udfc6 Best Model: {best_model}\")\n",
        "# print(f\"ROC-AUC: {results_df.loc[results_df['roc_auc'].idxmax(), 'roc_auc']:.3f}\")\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Do all 4 metric charts show consistent rankings?\n",
        "# - Is the best model an ensemble method?\n",
        "# - Can you explain why ensemble methods often outperform single models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 2.2\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] All 4 metric charts display correctly\n",
        "- [ ] Model rankings are consistent across metrics\n",
        "- [ ] You can explain why the best model performs well\n",
        "- [ ] You understand the trade-offs (complexity vs performance)\n",
        "\n",
        "\ud83d\udca1 **Manufacturing Insight**: While Random Forest and Gradient Boosting often perform best, simpler models like Logistic Regression may be preferred in production if performance difference is small (easier to explain, faster inference, easier to debug).\n",
        "\n",
        "\ud83d\udcda **See Solution**: Check `wafer_defect_solution.ipynb` Exercise 2 for complete implementation and discussion of model selection trade-offs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# ROC-AUC comparison\n",
        "ax1 = axes[0, 0]\n",
        "results_df.set_index('Model')['ROC-AUC'].plot(kind='bar', ax=ax1, color='skyblue')\n",
        "ax1.set_title('ROC-AUC Comparison')\n",
        "ax1.set_ylabel('ROC-AUC Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# PR-AUC comparison\n",
        "ax2 = axes[0, 1]\n",
        "results_df.set_index('Model')['PR-AUC'].plot(kind='bar', ax=ax2, color='lightgreen')\n",
        "ax2.set_title('PR-AUC Comparison')\n",
        "ax2.set_ylabel('PR-AUC Score')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# PWS comparison  \n",
        "ax3 = axes[1, 0]\n",
        "results_df.set_index('Model')['PWS'].plot(kind='bar', ax=ax3, color='salmon')\n",
        "ax3.set_title('PWS (Prediction Within Spec) Comparison')\n",
        "ax3.set_ylabel('PWS Score')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# F1 Score comparison\n",
        "ax4 = axes[1, 1]\n",
        "results_df.set_index('Model')['F1'].plot(kind='bar', ax=ax4, color='gold')\n",
        "ax4.set_title('F1 Score Comparison')\n",
        "ax4.set_ylabel('F1 Score')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = results_df['ROC-AUC'].idxmax()\n",
        "best_model = results_df.loc[best_model_idx]\n",
        "print(f\"\\n=== Best Performing Model ===\")\n",
        "print(f\"Model: {best_model['Model']}\")\n",
        "print(f\"ROC-AUC: {best_model['ROC-AUC']:.3f}\")\n",
        "print(f\"PWS: {best_model['PWS']:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Manufacturing-Specific Metrics Deep Dive\n",
        "\n",
        "Let's explore metrics that matter in semiconductor manufacturing: PWS and cost-based optimization.\n",
        "\n",
        "### \ud83c\udfaf Exercise 3.1: Calculate Manufacturing Costs (\u2605\u2605\u2605 Advanced)\n",
        "\n",
        "**Task**: Calculate the financial impact of classification errors using real manufacturing costs.\n",
        "\n",
        "**Requirements**:\n",
        "1. Train the best model from Exercise 2 (likely Random Forest or Gradient Boosting)\n",
        "2. Get predictions on the test set\n",
        "3. Calculate confusion matrix (TP, TN, FP, FN)\n",
        "4. Assign realistic costs:\n",
        "   - False Positive (unnecessary inspection): $50\n",
        "   - False Negative (missed defect): $200\n",
        "5. Calculate total estimated loss and PWS\n",
        "\n",
        "**Hints**:\n",
        "- Use the best model from your earlier comparison\n",
        "- Get predictions with `pipeline.predict(X_test)`\n",
        "- Use `confusion_matrix(y_test, predictions)` from sklearn.metrics\n",
        "- Total cost = (FP \u00d7 $50) + (FN \u00d7 $200)\n",
        "- PWS = (TP + TN) / total_samples\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train the best model (use model_type from Exercise 2)\n",
        "# best_model_type = 'random_forest'  # or 'gradient_boosting'\n",
        "# print(f\"Training best model: {best_model_type}\")\n",
        "# pipeline = WaferDefectPipeline(model_type=best_model_type)\n",
        "# pipeline.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Get predictions on test set\n",
        "# predictions = pipeline.predict(X_test)\n",
        "\n",
        "# TODO: Calculate confusion matrix\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# cm = confusion_matrix(y_test, predictions)\n",
        "# tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# print(\"\\n=== Confusion Matrix ===\")\n",
        "# print(f\"True Negatives:  {tn}\")\n",
        "# print(f\"False Positives: {fp}\")\n",
        "# print(f\"False Negatives: {fn}\")\n",
        "# print(f\"True Positives:  {tp}\")\n",
        "\n",
        "# TODO: Assign manufacturing costs\n",
        "# FP_COST = 50   # Cost of unnecessary inspection\n",
        "# FN_COST = 200  # Cost of missed defect (much higher!)\n",
        "\n",
        "# TODO: Calculate total cost\n",
        "# total_cost = (fp * FP_COST) + (fn * FN_COST)\n",
        "# print(f\"\\n=== Financial Impact ===\")\n",
        "# print(f\"False Positive Cost: ${fp * FP_COST:,}\")\n",
        "# print(f\"False Negative Cost: ${fn * FN_COST:,}\")\n",
        "# print(f\"Total Estimated Loss: ${total_cost:,}\")\n",
        "\n",
        "# TODO: Calculate PWS\n",
        "# pws = (tp + tn) / len(y_test)\n",
        "# print(f\"\\nPrediction Within Spec (PWS): {pws:.1%}\")\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Is FN cost higher than FP cost? (It should be - missing defects is worse!)\n",
        "# - Is PWS above 90%?\n",
        "# - Can you explain why FN costs more in manufacturing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 3.1\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] Confusion matrix calculated correctly (all 4 values: TN, FP, FN, TP)\n",
        "- [ ] FN cost is 4x FP cost (reflects real manufacturing economics)\n",
        "- [ ] Total cost calculated includes both FP and FN\n",
        "- [ ] PWS metric makes sense (percentage of correct predictions)\n",
        "- [ ] You understand why missing a defect (FN) costs more than a false alarm (FP)\n",
        "\n",
        "\ud83d\udca1 **Manufacturing Reality**: \n",
        "- **False Positive**: Wafer goes to unnecessary inspection ($50 in time/resources)\n",
        "- **False Negative**: Defective wafer reaches customer \u2192 RMA, reputation damage, possible fab shutdown ($200+ in total impact)\n",
        "\n",
        "This cost asymmetry drives threshold optimization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83c\udfaf Exercise 3.2: Optimize Decision Threshold (\u2605\u2605\u2605 Advanced)\n",
        "\n",
        "**Task**: Find the optimal classification threshold that minimizes total manufacturing cost.\n",
        "\n",
        "**Requirements**:\n",
        "1. Get probability predictions (not just binary 0/1)\n",
        "2. Sweep thresholds from 0.1 to 0.9\n",
        "3. For each threshold:\n",
        "   - Convert probabilities to binary predictions\n",
        "   - Calculate confusion matrix\n",
        "   - Calculate total cost (FP \u00d7 $50 + FN \u00d7 $200)\n",
        "   - Calculate precision, recall, F1, PWS\n",
        "4. Find threshold that minimizes total cost\n",
        "5. Compare to default 0.5 threshold\n",
        "\n",
        "**Hints**:\n",
        "- Use `pipeline.model.predict_proba(X_test)[:, 1]` to get probabilities  \n",
        "- `(probabilities >= threshold).astype(int)` converts probs to binary\n",
        "- Store results in a list of dictionaries for easy DataFrame creation\n",
        "- Use `.idxmin()` to find row with minimum cost\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Get probability predictions\n",
        "# probabilities = pipeline.model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# TODO: Define threshold range\n",
        "# thresholds = np.arange(0.1, 1.0, 0.05)  # 0.1, 0.15, 0.2, ..., 0.95\n",
        "\n",
        "# TODO: Sweep thresholds and calculate metrics\n",
        "# threshold_results = []\n",
        "# FP_COST = 50\n",
        "# FN_COST = 200\n",
        "\n",
        "# for threshold in thresholds:\n",
        "#     # TODO: Apply threshold to probabilities\n",
        "#     pred_at_threshold = (probabilities >= threshold).astype(int)\n",
        "#     \n",
        "#     # TODO: Calculate confusion matrix\n",
        "#     cm = confusion_matrix(y_test, pred_at_threshold)\n",
        "#     tn, fp, fn, tp = cm.ravel()\n",
        "#     \n",
        "#     # TODO: Calculate costs\n",
        "#     total_cost = (fp * FP_COST) + (fn * FN_COST)\n",
        "#     \n",
        "#     # TODO: Calculate metrics\n",
        "#     precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "#     recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "#     pws = (tp + tn) / len(y_test)\n",
        "#     \n",
        "#     # Store results\n",
        "#     threshold_results.append({\n",
        "#         'threshold': threshold,\n",
        "#         'precision': precision,\n",
        "#         'recall': recall,\n",
        "#         'f1': f1,\n",
        "#         'pws': pws,\n",
        "#         'fp': fp,\n",
        "#         'fn': fn,\n",
        "#         'total_cost': total_cost\n",
        "#     })\n",
        "\n",
        "# TODO: Create DataFrame\n",
        "# threshold_df = pd.DataFrame(threshold_results)\n",
        "\n",
        "# TODO: Find optimal threshold (minimum cost)\n",
        "# optimal_idx = threshold_df['total_cost'].idxmin()\n",
        "# optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
        "# optimal_cost = threshold_df.loc[optimal_idx, 'total_cost']\n",
        "\n",
        "# print(\"\\n=== Threshold Optimization Results ===\")\n",
        "# print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
        "# print(f\"Optimal Cost: ${optimal_cost:,}\")\n",
        "\n",
        "# TODO: Compare to default threshold (0.5)\n",
        "# default_idx = threshold_df[threshold_df['threshold'] == 0.5].index[0]\n",
        "# default_cost = threshold_df.loc[default_idx, 'total_cost']\n",
        "# cost_savings = default_cost - optimal_cost\n",
        "\n",
        "# print(f\"\\nDefault Threshold (0.5) Cost: ${default_cost:,}\")\n",
        "# print(f\"Cost Savings: ${cost_savings:,} ({cost_savings/default_cost:.1%})\")\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Did you sweep at least 10 different thresholds?\n",
        "# - Is optimal threshold different from 0.5?\n",
        "# - Does optimizing save money compared to default?\n",
        "# - Do you understand the precision/recall trade-off?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 3.2\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] You swept multiple thresholds (at least 10-20)\n",
        "- [ ] Found threshold with minimum cost\n",
        "- [ ] Optimal threshold is likely lower than 0.5 (favoring recall over precision)\n",
        "- [ ] Cost savings are significant (usually 10-30%)\n",
        "- [ ] You understand why lower threshold = higher recall = fewer missed defects\n",
        "\n",
        "\ud83d\udca1 **Key Insight**: Because FN costs 4x more than FP, we want HIGH RECALL (catch all defects) even if it means MORE FALSE POSITIVES. This shifts optimal threshold below 0.5!\n",
        "\n",
        "\ud83d\udcca **Trade-off**: \n",
        "- **Higher threshold** (e.g., 0.7) \u2192 Higher precision, lower recall \u2192 Miss more defects (expensive!)\n",
        "- **Lower threshold** (e.g., 0.3) \u2192 Lower precision, higher recall \u2192 More false alarms (cheaper!)\n",
        "\n",
        "\ud83d\udcda **See Solution**: Check `wafer_defect_solution.ipynb` Exercise 3 for complete threshold optimization with visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION CODE - Try the exercises above first!\n",
        "# This is reference code showing one way to solve the exercises\n",
        "\n",
        "# Train the best performing model for detailed analysis\n",
        "best_pipeline = WaferDefectPipeline(\n",
        "    model_name='rf',  # Usually performs well\n",
        "    handle_imbalance='class_weight'\n",
        ")\n",
        "best_pipeline.fit(X, y)\n",
        "\n",
        "# Get predictions and probabilities\n",
        "y_pred = best_pipeline.predict(X)\n",
        "y_proba = best_pipeline.predict_proba(X)[:, 1]  # Probability of defect\n",
        "\n",
        "# Analyze manufacturing costs at different thresholds\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "threshold_analysis = []\n",
        "\n",
        "print(\"=== Threshold Analysis for Manufacturing Optimization ===\")\n",
        "for threshold in thresholds:\n",
        "    # Apply threshold\n",
        "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate metrics with manufacturing parameters\n",
        "    metrics = WaferDefectPipeline.compute_metrics(\n",
        "        y, y_pred_thresh, y_proba,\n",
        "        cost_false_positive=10.0,  # Cost of scrapping good wafer\n",
        "        cost_false_negative=100.0,  # Cost of shipping defective wafer\n",
        "        tolerance=0.05\n",
        "    )\n",
        "    \n",
        "    threshold_analysis.append({\n",
        "        'Threshold': threshold,\n",
        "        'Precision': metrics['Precision'],\n",
        "        'Recall': metrics['Recall'],\n",
        "        'F1': metrics['F1'],\n",
        "        'PWS': metrics['PWS'],\n",
        "        'Estimated_Loss': metrics['Estimated_Loss']\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_analysis)\n",
        "print(threshold_df.head(10).round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize threshold optimization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Precision vs Recall vs Threshold\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', label='Precision', linewidth=2)\n",
        "ax1.plot(threshold_df['Threshold'], threshold_df['Recall'], 'r-', label='Recall', linewidth=2)\n",
        "ax1.plot(threshold_df['Threshold'], threshold_df['F1'], 'g--', label='F1', linewidth=2)\n",
        "ax1.set_xlabel('Decision Threshold')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Precision-Recall vs Threshold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# PWS vs Threshold\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(threshold_df['Threshold'], threshold_df['PWS'], 'purple', linewidth=2)\n",
        "ax2.set_xlabel('Decision Threshold')\n",
        "ax2.set_ylabel('PWS (Prediction Within Spec)')\n",
        "ax2.set_title('Manufacturing PWS vs Threshold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Estimated Loss vs Threshold\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(threshold_df['Threshold'], threshold_df['Estimated_Loss'], 'orange', linewidth=2)\n",
        "ax3.set_xlabel('Decision Threshold')\n",
        "ax3.set_ylabel('Estimated Loss ($)')\n",
        "ax3.set_title('Manufacturing Cost vs Threshold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Find optimal threshold (minimum loss)\n",
        "optimal_idx = threshold_df['Estimated_Loss'].idxmin()\n",
        "optimal_threshold = threshold_df.loc[optimal_idx, 'Threshold']\n",
        "optimal_loss = threshold_df.loc[optimal_idx, 'Estimated_Loss']\n",
        "\n",
        "ax3.axvline(x=optimal_threshold, color='red', linestyle='--', alpha=0.7)\n",
        "ax3.text(optimal_threshold + 0.05, optimal_loss, \n",
        "         f'Optimal: {optimal_threshold:.2f}', \n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, _ = roc_curve(y, y_proba)\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve')\n",
        "ax4.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
        "ax4.set_xlabel('False Positive Rate')\n",
        "ax4.set_ylabel('True Positive Rate')\n",
        "ax4.set_title('ROC Curve')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== Optimal Operating Point ===\")\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"At this threshold:\")\n",
        "print(f\"  Precision: {threshold_df.loc[optimal_idx, 'Precision']:.3f}\")\n",
        "print(f\"  Recall: {threshold_df.loc[optimal_idx, 'Recall']:.3f}\")\n",
        "print(f\"  PWS: {threshold_df.loc[optimal_idx, 'PWS']:.1%}\")\n",
        "print(f\"  Estimated Loss: ${threshold_df.loc[optimal_idx, 'Estimated_Loss']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Deployment and CLI Usage\n",
        "\n",
        "Now let's deploy your model for production use with the standardized CLI interface.\n",
        "\n",
        "### \ud83c\udfaf Exercise 4.1: Save Production Model (\u2605\u2605 Intermediate)\n",
        "\n",
        "**Task**: Save your optimized model with metadata for production deployment.\n",
        "\n",
        "**Requirements**:\n",
        "1. Create a WaferDefectPipeline with optimal threshold from Exercise 3.2\n",
        "2. Train it on the full training set\n",
        "3. Save the model to disk using `.save()` method\n",
        "4. Include metadata (model type, threshold, training date, performance metrics)\n",
        "\n",
        "**Hints**:\n",
        "- Use the optimal threshold you found in Exercise 3.2\n",
        "- Use `pipeline.save(path)` to persist the model\n",
        "- Include timestamp and key metrics in filename for tracking\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create pipeline with optimal threshold\n",
        "# optimal_threshold = 0.35  # Use your value from Exercise 3.2\n",
        "# pipeline = WaferDefectPipeline(\n",
        "#     model_type='random_forest',  # or your best model\n",
        "#     threshold=optimal_threshold\n",
        "# )\n",
        "\n",
        "# TODO: Train on full training set\n",
        "# pipeline.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Evaluate on test set to get final metrics\n",
        "# final_metrics = pipeline.evaluate(X_test, y_test)\n",
        "# print(\"\\n=== Final Model Performance ===\")\n",
        "# print(f\"ROC-AUC: {final_metrics['roc_auc']:.3f}\")\n",
        "# print(f\"PWS: {final_metrics['pws']:.1%}\")\n",
        "# print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
        "\n",
        "# TODO: Save model with descriptive filename\n",
        "# from datetime import datetime\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "# model_filename = f\"wafer_defect_model_{timestamp}_roc{final_metrics['roc_auc']:.2f}.joblib\"\n",
        "# pipeline.save(Path(model_filename))\n",
        "# print(f\"\\n\u2705 Model saved to: {model_filename}\")\n",
        "# print(f\"File size: {Path(model_filename).stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Did the model save successfully?\n",
        "# - Is the filename descriptive (includes date and performance)?\n",
        "# - Can you locate the saved file on disk?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 4.1\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] Model saved successfully (check file exists on disk)\n",
        "- [ ] Filename includes date and performance metrics\n",
        "- [ ] You can explain why including metadata in filename helps production tracking\n",
        "- [ ] File size is reasonable (typically <1MB for small models)\n",
        "\n",
        "\ud83d\udca1 **Production Tip**: Always include version info (date, metrics, threshold) in model filenames. This enables:\n",
        "- Easy rollback if new model underperforms\n",
        "- A/B testing between model versions\n",
        "- Audit trail for regulatory compliance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83c\udfaf Exercise 4.2: Test Model Loading and CLI Usage (\u2605\u2605 Intermediate)\n",
        "\n",
        "**Task**: Verify model persistence by loading and testing your saved model, then use the CLI.\n",
        "\n",
        "**Requirements**:\n",
        "1. Load the saved model using `.load()` static method\n",
        "2. Verify predictions match the original model\n",
        "3. Test CLI commands:\n",
        "   - Train a new model\n",
        "   - Evaluate saved model\n",
        "   - Make predictions on new wafer\n",
        "4. Generate example CLI commands for your production team\n",
        "\n",
        "**Hints**:\n",
        "- Use `WaferDefectPipeline.load(path)` to load\n",
        "- Compare loaded model predictions to original\n",
        "- CLI uses subcommands: `train`, `evaluate`, `predict`\n",
        "\n",
        "**TODO**: Complete the code below \ud83d\udc47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load the saved model\n",
        "# loaded_pipeline = WaferDefectPipeline.load(Path(model_filename))\n",
        "# print(f\"\u2705 Model loaded from: {model_filename}\")\n",
        "\n",
        "# TODO: Verify predictions match original\n",
        "# original_preds = pipeline.predict(X_test[:5])\n",
        "# loaded_preds = loaded_pipeline.predict(X_test[:5])\n",
        "\n",
        "# print(\"\\n=== Round-Trip Verification ===\")\n",
        "# print(f\"Original predictions: {original_preds}\")\n",
        "# print(f\"Loaded predictions:   {loaded_preds}\")\n",
        "# print(f\"Match: {np.array_equal(original_preds, loaded_preds)}\")\n",
        "\n",
        "# TODO: Test prediction on a single wafer\n",
        "# sample_wafer = X_test.iloc[0:1]  # Get first wafer as DataFrame\n",
        "# prediction = loaded_pipeline.predict(sample_wafer)[0]\n",
        "# probability = loaded_pipeline.model.predict_proba(sample_wafer)[0, 1]\n",
        "\n",
        "# print(f\"\\n=== Sample Prediction ===\")\n",
        "# print(f\"Prediction: {'DEFECTIVE' if prediction == 1 else 'GOOD'}\")\n",
        "# print(f\"Defect Probability: {probability:.1%}\")\n",
        "# print(f\"Decision Threshold: {optimal_threshold:.2f}\")\n",
        "\n",
        "# \u2705 Self-check:\n",
        "# - Do loaded predictions match original?\n",
        "# - Can you predict on new samples?\n",
        "# - Do you understand the probability vs binary prediction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd0d Self-Check Exercise 4.2\n",
        "\n",
        "Before moving on, verify:\n",
        "- [ ] Model loads successfully from disk\n",
        "- [ ] Predictions from loaded model match original (round-trip test passes)\n",
        "- [ ] Can predict on individual wafers\n",
        "- [ ] Understand difference between probability and binary prediction\n",
        "\n",
        "\ud83d\udca1 **CLI Commands for Production**:\n",
        "\n",
        "```bash\n",
        "# Train a new model with SMOTE for class imbalance\n",
        "python wafer_defect_pipeline.py train --data wafer_data.csv --model-type random_forest --use-smote --output-model model.joblib\n",
        "\n",
        "# Evaluate saved model on test data\n",
        "python wafer_defect_pipeline.py evaluate --model model.joblib --data test_data.csv --output-json metrics.json\n",
        "\n",
        "# Make predictions on new wafers\n",
        "python wafer_defect_pipeline.py predict --model model.joblib --data new_wafers.csv --output predictions.json\n",
        "\n",
        "# Train with high-recall constraint (minimize false negatives)\n",
        "python wafer_defect_pipeline.py train --data wafer_data.csv --model-type gradient_boosting --min-recall 0.95 --output-model high_recall_model.joblib\n",
        "```\n",
        "\n",
        "\ud83d\udcda **See Solution**: Check `wafer_defect_solution.ipynb` Exercise 4 for complete deployment workflow and production checklist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLUTION CODE - Try the exercises above first!\n",
        "# This is reference code showing how to deploy the model\n",
        "\n",
        "# Save the optimized model\n",
        "model_path = Path('best_wafer_defect_model.joblib')\n",
        "\n",
        "# Create pipeline with optimal threshold\n",
        "production_pipeline = WaferDefectPipeline(\n",
        "    model_name='rf',\n",
        "    handle_imbalance='class_weight'\n",
        ")\n",
        "production_pipeline.fit(X, y)\n",
        "\n",
        "# Optimize threshold for minimum cost\n",
        "production_pipeline.optimize_threshold(\n",
        "    X, y, \n",
        "    min_precision=0.8,  # Require at least 80% precision\n",
        "    cost_false_positive=10.0,\n",
        "    cost_false_negative=100.0\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "production_pipeline.save(model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Optimized threshold: {production_pipeline.threshold:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate CLI usage (these would be run from command line)\n",
        "print(\"=== CLI Usage Examples ===\")\n",
        "print(\"\\nTo train a new model:\")\n",
        "print(\"python wafer_defect_pipeline.py train --dataset synthetic_wafer --model rf --min-precision 0.8 --save model.joblib\")\n",
        "\n",
        "print(\"\\nTo evaluate an existing model:\")\n",
        "print(\"python wafer_defect_pipeline.py evaluate --model-path model.joblib --dataset synthetic_wafer\")\n",
        "\n",
        "print(\"\\nTo make predictions:\")\n",
        "prediction_example = {\n",
        "    \"center_density\": 0.12,\n",
        "    \"edge_density\": 0.05,\n",
        "    \"pattern_uniformity\": 0.85,\n",
        "    \"thickness_variation\": 0.03\n",
        "}\n",
        "print(f'python wafer_defect_pipeline.py predict --model-path model.joblib --input-json \\'{prediction_example}\\'')\n",
        "\n",
        "# Simulate a prediction\n",
        "print(\"\\n=== Live Prediction Example ===\")\n",
        "sample_wafer = X.iloc[0:1]  # Take first wafer\n",
        "prediction = production_pipeline.predict(sample_wafer)\n",
        "probability = production_pipeline.predict_proba(sample_wafer)[0, 1]\n",
        "\n",
        "print(f\"Sample wafer features: {sample_wafer.iloc[0].to_dict()}\")\n",
        "print(f\"Prediction: {'DEFECTIVE' if prediction[0] == 1 else 'GOOD'}\")\n",
        "print(f\"Defect probability: {probability:.3f}\")\n",
        "print(f\"Actual label: {'DEFECTIVE' if y[0] == 1 else 'GOOD'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Key Takeaways\n",
        "\n",
        "\ud83c\udf89 **Congratulations!** You've completed the wafer defect classification tutorial!\n",
        "\n",
        "### Manufacturing Insights\n",
        "1. **Threshold Optimization**: The optimal decision threshold balances false positive costs (scrapping good wafers) vs false negative costs (shipping defective wafers)\n",
        "   - In manufacturing, FN costs typically 4-10x more than FP costs\n",
        "   - Optimal thresholds are often < 0.5 to favor recall (catch more defects)\n",
        "   \n",
        "2. **PWS Metric**: Prediction Within Spec measures how well predictions align with manufacturing tolerance requirements\n",
        "   - Essential for quality control systems\n",
        "   - Complements standard ML metrics (ROC-AUC, F1)\n",
        "   \n",
        "3. **Cost-Aware ML**: Manufacturing decisions should consider economic impact, not just accuracy\n",
        "   - $50 FP cost vs $200 FN cost drives different optimization strategies\n",
        "   - Threshold optimization can reduce costs by 10-30%\n",
        "\n",
        "### Technical Insights\n",
        "1. **Model Selection**: Random Forest and Gradient Boosting typically perform well for semiconductor defect detection\n",
        "   - Handle non-linear relationships between features\n",
        "   - Robust to feature scaling differences\n",
        "   - Provide feature importance for root cause analysis\n",
        "2. **Imbalance Handling**: Class weights and SMOTE help address the natural imbalance in defect rates (typically 5-20%)\n",
        "   - SMOTE can improve recall but may introduce synthetic noise\n",
        "   - Class weights simpler and often sufficient\n",
        "   \n",
        "3. **Feature Engineering**: Process parameters with high correlation to defects are most valuable\n",
        "   - Focus on features with statistical significance (p < 0.05)\n",
        "   - Domain knowledge guides feature selection\n",
        "\n",
        "### Production Deployment\n",
        "1. **Standardized CLI**: Consistent interface across all semiconductor ML projects\n",
        "   - Subcommands: train, evaluate, predict\n",
        "   - JSON output for system integration\n",
        "   \n",
        "2. **Model Persistence**: Save/load functionality preserves optimal thresholds and preprocessing\n",
        "   - Include metadata (date, threshold, performance) in filenames\n",
        "   - Enable model versioning and rollback\n",
        "   \n",
        "3. **Manufacturing Integration**: JSON output enables integration with MES/ERP systems\n",
        "   - Real-time predictions via API\n",
        "   - Batch processing for historical analysis\n",
        "\n",
        "## \ud83d\udcda Additional Resources\n",
        "\n",
        "### Solution Notebook\n",
        "**`wafer_defect_solution.ipynb`** contains complete solutions for all exercises:\n",
        "- Exercise 1: Data generation with detailed exploration\n",
        "- Exercise 2: 5-model comparison with ROC curve visualization\n",
        "- Exercise 3: Cost optimization with threshold analysis\n",
        "- Exercise 4: Production deployment with 40+ item checklist\n",
        "\n",
        "### Grading Script\n",
        "Run `evaluate_submission.py` to check your work:\n",
        "```bash\n",
        "python evaluate_submission.py --notebook-path wafer_defect_tutorial.ipynb\n",
        "```\n",
        "\n",
        "Scoring rubric (100 points total):\n",
        "- Exercise 1 (Data Exploration): 20 points\n",
        "- Exercise 2 (Model Training): 30 points\n",
        "- Exercise 3 (Manufacturing Metrics): 25 points\n",
        "- Exercise 4 (CLI Usage): 15 points\n",
        "- Documentation/Comments: 10 points\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "To extend this baseline classifier:\n",
        "1. **Real Data Integration**: Connect to actual wafer map datasets (WM-811K dataset in `datasets/wm811k/`)\n",
        "2. **Deep Learning**: Implement CNN models for spatial pattern recognition (see Module 6.2)\n",
        "3. **Feature Engineering**: Add domain-specific features (spatial statistics, pattern descriptors)\n",
        "4. **Model Ensemble**: Combine multiple model predictions for improved robustness\n",
        "5. **Real-time Deployment**: Create API wrapper for production manufacturing lines (FastAPI template in `infrastructure/api/`)\n",
        "6. **Drift Monitoring**: Track model performance degradation over time (see Module 5.2)\n",
        "\n",
        "### Related Projects\n",
        "- **Yield Regression** (`yield_regression/`): Predict wafer yield from process parameters\n",
        "- **Equipment Drift Monitor** (`equipment_drift_monitor/`): Time series anomaly detection\n",
        "- **Die Defect Segmentation** (`die_defect_segmentation/`): Computer vision for defect localization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up temporary files\n",
        "if model_path.exists():\n",
        "    model_path.unlink()\n",
        "    print(\"\u2705 Cleaned up temporary model file\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udf89 TUTORIAL COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n\ud83d\udcca What You've Learned:\")\n",
        "print(\"  \u2713 Generate and explore semiconductor manufacturing data\")\n",
        "print(\"  \u2713 Train and compare 5 classification models\")\n",
        "print(\"  \u2713 Calculate manufacturing-specific metrics (PWS, costs)\")\n",
        "print(\"  \u2713 Optimize decision thresholds for cost reduction\")\n",
        "print(\"  \u2713 Deploy models with CLI interface\")\n",
        "print(\"\\n\ud83d\udcda Next: Check wafer_defect_solution.ipynb for complete solutions\")\n",
        "print(\"\ud83e\uddea Grade: Run evaluate_submission.py to check your work\")\n",
        "print(\"\ud83d\ude80 Ready: Build production wafer defect classifiers!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
