{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a557a0e",
      "metadata": {},
      "source": [
        "# Wafer Defect Classification - Complete Solution\n",
        "\n",
        "**\u26a0\ufe0f SOLUTION NOTEBOOK** - This contains complete implementations of all exercises from the tutorial.\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This solution notebook provides:\n",
        "- \u2705 Complete code implementations for all exercises\n",
        "- \ud83d\udca1 Detailed explanations of design decisions\n",
        "- \ud83d\udcca Expected outputs and interpretations\n",
        "- \u26a0\ufe0f Common pitfalls and debugging tips\n",
        "- \ud83c\udfaf Production-ready best practices\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Attempt exercises first** - Try the tutorial exercises before looking at solutions\n",
        "2. **Compare approaches** - Your solution may differ but still be correct\n",
        "3. **Learn from differences** - Understand why alternative approaches work\n",
        "4. **Adapt to your needs** - Solutions are templates, not rigid requirements\n",
        "\n",
        "## Business Context\n",
        "\n",
        "In semiconductor manufacturing, wafer defect detection is critical for:\n",
        "- **Quality Control**: Early detection prevents defective dies from reaching customers\n",
        "- **Cost Reduction**: Identifying process issues before they impact entire lots ($10K-$50K per lot)\n",
        "- **Process Optimization**: Understanding defect patterns to improve manufacturing yield\n",
        "\n",
        "**Industry Standards**:\n",
        "- Target defect detection rate: >95%\n",
        "- Acceptable false positive rate: <5%\n",
        "- Inspection throughput: >100 wafers/hour\n",
        "- Cost per false negative: $100-$500 (scrapped dies shipped)\n",
        "- Cost per false positive: $10-$50 (good wafer unnecessarily scrapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d51a82",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b81b935a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Import our wafer defect pipeline\n",
        "from wafer_defect_pipeline import (\n",
        "    WaferDefectPipeline, \n",
        "    generate_synthetic_wafer_defects,\n",
        "    load_dataset\n",
        ")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"\u2705 Environment setup complete\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a105fc7c",
      "metadata": {},
      "source": [
        "## Exercise 1: Data Generation and Exploration\n",
        "\n",
        "**Task**: Generate synthetic wafer defect data and perform exploratory analysis.\n",
        "\n",
        "**Requirements**:\n",
        "1. Generate 1000 wafer samples with 20% defect rate\n",
        "2. Calculate class distribution\n",
        "3. Visualize feature distributions for defective vs. non-defective wafers\n",
        "4. Identify top 3 most discriminative features\n",
        "\n",
        "**Solution Approach**:\n",
        "- Use the provided synthetic data generator\n",
        "- Compare feature distributions using violin plots\n",
        "- Calculate correlation with target variable\n",
        "- Statistical tests for feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41baf952",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 1.1: Generate synthetic data\n",
        "print(\"\ud83d\udcca Generating synthetic wafer defect data...\\n\")\n",
        "\n",
        "# Generate 1000 samples with 20% defect rate\n",
        "df = generate_synthetic_wafer_defects(\n",
        "    n_samples=1000,\n",
        "    defect_rate=0.20,\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "display(df.head())\n",
        "\n",
        "# Solution 1.2: Calculate class distribution\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class_counts = df['defect'].value_counts()\n",
        "class_pct = df['defect'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"\\nNon-defective wafers (0): {class_counts[0]:,} ({class_pct[0]:.1f}%)\")\n",
        "print(f\"Defective wafers (1): {class_counts[1]:,} ({class_pct[1]:.1f}%)\")\n",
        "print(f\"\\nImbalance ratio: {class_counts[0] / class_counts[1]:.2f}:1\")\n",
        "\n",
        "# Manufacturing context\n",
        "print(\"\\n\ud83d\udca1 Manufacturing Insight:\")\n",
        "print(f\"   This {class_pct[1]:.1f}% defect rate is realistic for modern fabs\")\n",
        "print(f\"   (Typical range: 5-30% depending on process maturity)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166acc50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 1.3: Visualize feature distributions\n",
        "print(\"\\n\ud83d\udcca Visualizing feature distributions...\")\n",
        "\n",
        "# Get numeric features (exclude wafer_id and target)\n",
        "feature_cols = [col for col in df.columns if col not in ['defect', 'wafer_id']]\n",
        "\n",
        "# Create violin plots for each feature\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(feature_cols):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Violin plot with defect grouping\n",
        "    parts = ax.violinplot(\n",
        "        [df[df['defect']==0][feature], df[df['defect']==1][feature]],\n",
        "        positions=[0, 1],\n",
        "        showmeans=True,\n",
        "        showmedians=True\n",
        "    )\n",
        "    \n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_xticklabels(['Non-Defective', 'Defective'])\n",
        "    ax.set_ylabel(feature)\n",
        "    ax.set_title(f'{feature} Distribution')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_distributions.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2705 Feature distribution plots saved as 'feature_distributions.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51425ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 1.4: Identify most discriminative features\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate correlation with target\n",
        "correlations = df[feature_cols].corrwith(df['defect']).abs().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nFeature correlations with defect status:\")\n",
        "for feature, corr in correlations.items():\n",
        "    print(f\"  {feature:25s}: {corr:.4f}\")\n",
        "\n",
        "# Top 3 features\n",
        "top_3 = correlations.head(3)\n",
        "print(f\"\\n\ud83c\udfaf Top 3 Most Discriminative Features:\")\n",
        "for i, (feature, corr) in enumerate(top_3.items(), 1):\n",
        "    print(f\"  {i}. {feature}: {corr:.4f}\")\n",
        "\n",
        "# Manufacturing interpretation\n",
        "print(\"\\n\ud83d\udca1 Manufacturing Interpretation:\")\n",
        "print(\"   - Higher correlations indicate stronger defect indicators\")\n",
        "print(\"   - These features should be prioritized in process control\")\n",
        "print(\"   - Real-world: align with physical defect mechanisms\")\n",
        "\n",
        "# Statistical significance testing\n",
        "from scipy import stats\n",
        "\n",
        "print(\"\\n\ud83d\udcca Statistical Significance (t-tests):\")\n",
        "for feature in top_3.index:\n",
        "    non_defect = df[df['defect']==0][feature]\n",
        "    defect = df[df['defect']==1][feature]\n",
        "    t_stat, p_value = stats.ttest_ind(non_defect, defect)\n",
        "    print(f\"  {feature:25s}: p-value = {p_value:.2e} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}\")\n",
        "\n",
        "print(\"\\n  *** p < 0.001 (highly significant)\")\n",
        "print(\"  **  p < 0.01  (very significant)\")\n",
        "print(\"  *   p < 0.05  (significant)\")\n",
        "print(\"  ns  p >= 0.05 (not significant)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1275339e",
      "metadata": {},
      "source": [
        "### Exercise 1 Key Takeaways\n",
        "\n",
        "**\u2705 What You Learned**:\n",
        "1. **Class Imbalance**: 20% defect rate creates 4:1 imbalance - requires special handling\n",
        "2. **Feature Discrimination**: Some features are much better defect predictors than others\n",
        "3. **Statistical Validation**: P-values confirm which differences are meaningful vs. random\n",
        "4. **Manufacturing Context**: Feature importance should align with physical defect mechanisms\n",
        "\n",
        "**\u26a0\ufe0f Common Pitfalls**:\n",
        "- Ignoring class imbalance leads to biased models\n",
        "- Using all features equally when some are noise\n",
        "- Not validating statistical significance\n",
        "- Forgetting to set random seed (results not reproducible)\n",
        "\n",
        "**\ud83c\udfaf Production Considerations**:\n",
        "- Real wafer data may have >100 features\n",
        "- Feature selection reduces overfitting and computation\n",
        "- Domain expert input crucial for feature interpretation\n",
        "- Monitor feature distributions for data drift"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54fe6442",
      "metadata": {},
      "source": [
        "## Exercise 2: Model Training and Comparison\n",
        "\n",
        "**Task**: Train multiple classification models and compare their performance.\n",
        "\n",
        "**Requirements**:\n",
        "1. Train 5 different models: Logistic Regression, Linear SVM, Decision Tree, Random Forest, Gradient Boosting\n",
        "2. Use 80/20 train/test split with stratification\n",
        "3. Compare models using ROC AUC, PR AUC, and F1 score\n",
        "4. Create ROC curve comparison plot\n",
        "5. Recommend best model for production\n",
        "\n",
        "**Solution Approach**:\n",
        "- Use WaferDefectPipeline for consistent preprocessing\n",
        "- Evaluate all models on same test set\n",
        "- Consider multiple metrics (no single metric tells full story)\n",
        "- Balance accuracy vs. interpretability vs. speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23db4696",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 2.1: Prepare train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\ud83d\udcca Preparing train/test split...\\n\")\n",
        "\n",
        "# Separate features and target\n",
        "y = df['defect'].to_numpy()\n",
        "X = df.drop(columns=['defect', 'wafer_id'])\n",
        "\n",
        "# Stratified split to maintain class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.20, \n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y  # Critical for imbalanced data!\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"Test set:     {X_test.shape[0]:,} samples\")\n",
        "print(f\"\\nTrain defect rate: {y_train.mean()*100:.1f}%\")\n",
        "print(f\"Test defect rate:  {y_test.mean()*100:.1f}%\")\n",
        "print(\"\\n\u2705 Stratification maintained class balance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc599a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 2.2: Train multiple models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING MULTIPLE MODELS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "models_to_train = ['logistic', 'linear_svm', 'tree', 'rf', 'gb']\n",
        "trained_models = {}\n",
        "results = []\n",
        "\n",
        "for model_name in models_to_train:\n",
        "    print(f\"Training {model_name}...\", end=\" \")\n",
        "    \n",
        "    # Initialize and train pipeline\n",
        "    pipeline = WaferDefectPipeline(model=model_name)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = pipeline.evaluate(X_test, y_test)\n",
        "    \n",
        "    # Store results\n",
        "    trained_models[model_name] = pipeline\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'roc_auc': metrics['roc_auc'],\n",
        "        'pr_auc': metrics['pr_auc'],\n",
        "        'f1': metrics['f1'],\n",
        "        'precision': metrics['precision'],\n",
        "        'recall': metrics['recall'],\n",
        "        'pws': metrics['pws']\n",
        "    })\n",
        "    \n",
        "    print(f\"\u2705 ROC AUC: {metrics['roc_auc']:.4f}\")\n",
        "\n",
        "# Convert to DataFrame for easy comparison\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*60)\n",
        "display(results_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1d881b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 2.3: Visualize model comparison\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "print(\"\\n\ud83d\udcca Creating ROC curve comparison plot...\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot ROC curve for each model\n",
        "for model_name, pipeline in trained_models.items():\n",
        "    # Get predictions\n",
        "    y_proba = pipeline.predict_proba(X_test)[:, 1]  # Positive class probabilities\n",
        "    \n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Plot\n",
        "    plt.plot(fpr, tpr, linewidth=2, \n",
        "             label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "# Plot diagonal (random classifier)\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
        "plt.title('ROC Curve Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_comparison.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab56d44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 2.4: Comprehensive model ranking\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL RANKING AND RECOMMENDATION\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Rank by different metrics\n",
        "print(\"Rankings by metric:\")\n",
        "print(\"\\n1. ROC AUC (overall discrimination):\")\n",
        "top_roc = results_df.nlargest(3, 'roc_auc')[['model', 'roc_auc']]\n",
        "for i, row in top_roc.iterrows():\n",
        "    print(f\"   {row['model']:15s}: {row['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n2. PR AUC (precision-recall balance):\")\n",
        "top_pr = results_df.nlargest(3, 'pr_auc')[['model', 'pr_auc']]\n",
        "for i, row in top_pr.iterrows():\n",
        "    print(f\"   {row['model']:15s}: {row['pr_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n3. F1 Score (precision-recall harmonic mean):\")\n",
        "top_f1 = results_df.nlargest(3, 'f1')[['model', 'f1']]\n",
        "for i, row in top_f1.iterrows():\n",
        "    print(f\"   {row['model']:15s}: {row['f1']:.4f}\")\n",
        "\n",
        "print(\"\\n4. PWS (manufacturing metric):\")\n",
        "top_pws = results_df.nlargest(3, 'pws')[['model', 'pws']]\n",
        "for i, row in top_pws.iterrows():\n",
        "    print(f\"   {row['model']:15s}: {row['pws']:.1f}%\")\n",
        "\n",
        "# Overall recommendation\n",
        "best_overall = results_df.loc[results_df['roc_auc'].idxmax()]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udfaf PRODUCTION RECOMMENDATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nBest Model: {best_overall['model'].upper()}\")\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  - ROC AUC:   {best_overall['roc_auc']:.4f}\")\n",
        "print(f\"  - PR AUC:    {best_overall['pr_auc']:.4f}\")\n",
        "print(f\"  - F1 Score:  {best_overall['f1']:.4f}\")\n",
        "print(f\"  - Precision: {best_overall['precision']:.4f}\")\n",
        "print(f\"  - Recall:    {best_overall['recall']:.4f}\")\n",
        "print(f\"  - PWS:       {best_overall['pws']:.1f}%\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Rationale:\")\n",
        "if best_overall['model'] in ['rf', 'gb']:\n",
        "    print(\"  \u2705 Ensemble models typically provide best accuracy\")\n",
        "    print(\"  \u2705 Handle non-linear relationships and interactions\")\n",
        "    print(\"  \u2705 Built-in feature importance for interpretation\")\n",
        "    print(\"  \u26a0\ufe0f  Longer training time (acceptable for batch processing)\")\n",
        "    print(\"  \u26a0\ufe0f  Larger model size (manageable with modern hardware)\")\n",
        "elif best_overall['model'] == 'logistic':\n",
        "    print(\"  \u2705 Fast training and prediction\")\n",
        "    print(\"  \u2705 Highly interpretable (feature coefficients)\")\n",
        "    print(\"  \u2705 Small model size\")\n",
        "    print(\"  \u26a0\ufe0f  Assumes linear decision boundary\")\n",
        "\n",
        "print(\"\\n\ud83c\udfed Manufacturing Considerations:\")\n",
        "print(\"  - Prioritize recall if false negatives are costly (shipping defects)\")\n",
        "print(\"  - Prioritize precision if false positives are costly (scrapping good wafers)\")\n",
        "print(\"  - Use ROC AUC for balanced scenarios\")\n",
        "print(\"  - Monitor all metrics in production for drift detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdef585b",
      "metadata": {},
      "source": [
        "### Exercise 2 Key Takeaways\n",
        "\n",
        "**\u2705 What You Learned**:\n",
        "1. **Model Selection**: Different algorithms have different strengths\n",
        "2. **Multiple Metrics**: No single metric captures all aspects of performance\n",
        "3. **Train/Test Split**: Stratification crucial for imbalanced data\n",
        "4. **Ensemble Advantage**: Random Forest and Gradient Boosting often outperform simpler models\n",
        "\n",
        "**\u26a0\ufe0f Common Pitfalls**:\n",
        "- Using only accuracy (misleading for imbalanced data)\n",
        "- Not stratifying splits (creates biased test sets)\n",
        "- Overfitting to test set (use cross-validation for hyperparameter tuning)\n",
        "- Ignoring computational constraints (training time, model size)\n",
        "\n",
        "**\ud83c\udfaf Production Considerations**:\n",
        "- Retraining frequency: Weekly/monthly based on data drift\n",
        "- Model versioning: Track which model version is deployed\n",
        "- A/B testing: Compare new models against baseline in production\n",
        "- Fallback strategy: Keep previous version if new model fails"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c283a7",
      "metadata": {},
      "source": [
        "## Exercise 3: Manufacturing-Specific Metrics\n",
        "\n",
        "**Task**: Calculate and interpret manufacturing-specific metrics for wafer defect classification.\n",
        "\n",
        "**Requirements**:\n",
        "1. Calculate PWS (Prediction Within Spec) for best model\n",
        "2. Estimate financial loss from false positives and false negatives\n",
        "3. Optimize decision threshold for different business scenarios\n",
        "4. Create cost-benefit analysis visualization\n",
        "\n",
        "**Solution Approach**:\n",
        "- Assign realistic costs to false positives ($50) and false negatives ($200)\n",
        "- Sweep decision threshold from 0.1 to 0.9\n",
        "- Calculate total cost at each threshold\n",
        "- Identify optimal threshold for different business priorities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90bd3e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 3.1: Calculate PWS and financial metrics for best model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MANUFACTURING-SPECIFIC METRICS ANALYSIS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Use best model from Exercise 2\n",
        "best_model = trained_models[best_overall['model']]\n",
        "\n",
        "# Get predictions at default threshold (0.5)\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]  # Positive class probabilities\n",
        "\n",
        "# Calculate confusion matrix elements\n",
        "from sklearn.metrics import confusion_matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "print(f\"Confusion Matrix (Threshold = 0.5):\")\n",
        "print(f\"  True Negatives (TN):  {tn:4d} (correctly classified as non-defective)\")\n",
        "print(f\"  False Positives (FP): {fp:4d} (good wafers incorrectly flagged as defective)\")\n",
        "print(f\"  False Negatives (FN): {fn:4d} (defective wafers incorrectly passed as good) \u26a0\ufe0f\")\n",
        "print(f\"  True Positives (TP):  {tp:4d} (correctly detected defective wafers)\")\n",
        "\n",
        "# PWS (Prediction Within Spec)\n",
        "pws = (tp + tn) / len(y_test) * 100\n",
        "print(f\"\\nPWS (Prediction Within Spec): {pws:.1f}%\")\n",
        "\n",
        "# Financial impact analysis\n",
        "COST_FALSE_POSITIVE = 50    # Cost of unnecessarily scrapping a good wafer\n",
        "COST_FALSE_NEGATIVE = 200   # Cost of shipping a defective wafer to customer\n",
        "\n",
        "total_fp_cost = fp * COST_FALSE_POSITIVE\n",
        "total_fn_cost = fn * COST_FALSE_NEGATIVE\n",
        "total_cost = total_fp_cost + total_fn_cost\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"FINANCIAL IMPACT ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nCost Parameters:\")\n",
        "print(f\"  - Cost per False Positive: ${COST_FALSE_POSITIVE:,}\")\n",
        "print(f\"  - Cost per False Negative: ${COST_FALSE_NEGATIVE:,}\")\n",
        "print(f\"\\nTotal Costs (test set = {len(y_test)} wafers):\")\n",
        "print(f\"  - False Positive Cost: ${total_fp_cost:,} ({fp} wafers \u00d7 ${COST_FALSE_POSITIVE})\")\n",
        "print(f\"  - False Negative Cost: ${total_fn_cost:,} ({fn} wafers \u00d7 ${COST_FALSE_NEGATIVE}) \u26a0\ufe0f\")\n",
        "print(f\"  - TOTAL COST:          ${total_cost:,}\")\n",
        "print(f\"\\nCost per wafer inspected: ${total_cost/len(y_test):.2f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Manufacturing Insight:\")\n",
        "print(f\"   False negatives are {COST_FALSE_NEGATIVE/COST_FALSE_POSITIVE:.0f}x more costly than false positives\")\n",
        "print(f\"   We should optimize threshold to minimize false negatives\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b78ba25",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 3.2: Threshold optimization for cost minimization\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"THRESHOLD OPTIMIZATION FOR COST MINIMIZATION\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Sweep thresholds from 0.1 to 0.9\n",
        "thresholds = np.linspace(0.1, 0.9, 50)\n",
        "threshold_results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Make predictions at this threshold\n",
        "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate confusion matrix\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
        "    \n",
        "    # Calculate costs\n",
        "    fp_cost = fp * COST_FALSE_POSITIVE\n",
        "    fn_cost = fn * COST_FALSE_NEGATIVE\n",
        "    total = fp_cost + fn_cost\n",
        "    \n",
        "    # Calculate metrics\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    threshold_results.append({\n",
        "        'threshold': threshold,\n",
        "        'fp': fp,\n",
        "        'fn': fn,\n",
        "        'fp_cost': fp_cost,\n",
        "        'fn_cost': fn_cost,\n",
        "        'total_cost': total,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_results)\n",
        "\n",
        "# Find optimal threshold\n",
        "optimal_idx = threshold_df['total_cost'].idxmin()\n",
        "optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
        "optimal_cost = threshold_df.loc[optimal_idx, 'total_cost']\n",
        "\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
        "print(f\"Optimal Total Cost: ${optimal_cost:,.0f}\")\n",
        "print(f\"\\nAt optimal threshold:\")\n",
        "print(f\"  - False Positives: {threshold_df.loc[optimal_idx, 'fp']:.0f}\")\n",
        "print(f\"  - False Negatives: {threshold_df.loc[optimal_idx, 'fn']:.0f}\")\n",
        "print(f\"  - Precision: {threshold_df.loc[optimal_idx, 'precision']:.3f}\")\n",
        "print(f\"  - Recall: {threshold_df.loc[optimal_idx, 'recall']:.3f}\")\n",
        "print(f\"  - F1 Score: {threshold_df.loc[optimal_idx, 'f1']:.3f}\")\n",
        "\n",
        "# Compare to default threshold\n",
        "default_idx = (threshold_df['threshold'] - 0.5).abs().idxmin()\n",
        "default_cost = threshold_df.loc[default_idx, 'total_cost']\n",
        "cost_savings = default_cost - optimal_cost\n",
        "savings_pct = cost_savings / default_cost * 100\n",
        "\n",
        "print(f\"\\nComparison to default threshold (0.5):\")\n",
        "print(f\"  - Default cost: ${default_cost:,.0f}\")\n",
        "print(f\"  - Optimal cost: ${optimal_cost:,.0f}\")\n",
        "print(f\"  - Cost savings: ${cost_savings:,.0f} ({savings_pct:.1f}% reduction)\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Manufacturing Insight:\")\n",
        "print(f\"   Optimizing threshold reduces costs by {savings_pct:.1f}%\")\n",
        "print(f\"   This translates to ${cost_savings/len(y_test):.2f} savings per wafer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e275e660",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 3.3: Visualize cost vs threshold\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Total cost vs threshold\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(threshold_df['threshold'], threshold_df['total_cost'], 'b-', linewidth=2)\n",
        "ax1.axvline(optimal_threshold, color='r', linestyle='--', linewidth=2, label=f'Optimal: {optimal_threshold:.3f}')\n",
        "ax1.axvline(0.5, color='gray', linestyle=':', linewidth=1, label='Default: 0.500')\n",
        "ax1.set_xlabel('Decision Threshold')\n",
        "ax1.set_ylabel('Total Cost ($)')\n",
        "ax1.set_title('Total Cost vs Threshold', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: FP vs FN costs\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(threshold_df['threshold'], threshold_df['fp_cost'], 'g-', linewidth=2, label='FP Cost')\n",
        "ax2.plot(threshold_df['threshold'], threshold_df['fn_cost'], 'r-', linewidth=2, label='FN Cost')\n",
        "ax2.axvline(optimal_threshold, color='k', linestyle='--', linewidth=1)\n",
        "ax2.set_xlabel('Decision Threshold')\n",
        "ax2.set_ylabel('Cost ($)')\n",
        "ax2.set_title('FP vs FN Cost Components', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Error counts\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(threshold_df['threshold'], threshold_df['fp'], 'g-', linewidth=2, label='False Positives')\n",
        "ax3.plot(threshold_df['threshold'], threshold_df['fn'], 'r-', linewidth=2, label='False Negatives')\n",
        "ax3.axvline(optimal_threshold, color='k', linestyle='--', linewidth=1)\n",
        "ax3.set_xlabel('Decision Threshold')\n",
        "ax3.set_ylabel('Error Count')\n",
        "ax3.set_title('Error Counts vs Threshold', fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Precision and Recall\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(threshold_df['threshold'], threshold_df['precision'], 'b-', linewidth=2, label='Precision')\n",
        "ax4.plot(threshold_df['threshold'], threshold_df['recall'], 'orange', linewidth=2, label='Recall')\n",
        "ax4.plot(threshold_df['threshold'], threshold_df['f1'], 'purple', linewidth=2, label='F1 Score')\n",
        "ax4.axvline(optimal_threshold, color='k', linestyle='--', linewidth=1)\n",
        "ax4.set_xlabel('Decision Threshold')\n",
        "ax4.set_ylabel('Metric Value')\n",
        "ax4.set_title('Precision/Recall/F1 vs Threshold', fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('threshold_optimization.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Threshold optimization plots saved as 'threshold_optimization.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f6233f7",
      "metadata": {},
      "source": [
        "### Exercise 3 Key Takeaways\n",
        "\n",
        "**\u2705 What You Learned**:\n",
        "1. **Cost Asymmetry**: False negatives often cost 2-10x more than false positives in manufacturing\n",
        "2. **Threshold Optimization**: Default 0.5 threshold is rarely optimal for business metrics\n",
        "3. **PWS Metric**: Manufacturing-specific metric that combines both error types\n",
        "4. **Trade-offs**: Lower threshold \u2192 higher recall but lower precision (more false alarms)\n",
        "\n",
        "**\u26a0\ufe0f Common Pitfalls**:\n",
        "- Using same cost for FP and FN (unrealistic)\n",
        "- Optimizing for accuracy instead of business cost\n",
        "- Not considering downstream impact (warranty claims, customer satisfaction)\n",
        "- Fixed threshold instead of adaptive threshold based on lot value\n",
        "\n",
        "**\ud83c\udfaf Production Considerations**:\n",
        "- Update cost estimates regularly (market conditions change)\n",
        "- Different thresholds for different product lines (high-value vs commodity)\n",
        "- Real-time threshold adjustment based on fab conditions\n",
        "- Human-in-the-loop for borderline cases (probabilities near threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c06b40",
      "metadata": {},
      "source": [
        "## Exercise 4: Model Deployment and CLI Usage\n",
        "\n",
        "**Task**: Save trained model and demonstrate production deployment using CLI.\n",
        "\n",
        "**Requirements**:\n",
        "1. Save best model with optimal threshold to disk\n",
        "2. Load model and verify it produces same predictions\n",
        "3. Demonstrate CLI usage for train/evaluate/predict commands\n",
        "4. Create production deployment checklist\n",
        "\n",
        "**Solution Approach**:\n",
        "- Use joblib for model persistence\n",
        "- Include threshold in saved metadata\n",
        "- Test round-trip serialization\n",
        "- Document CLI commands for production use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379faac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 4.1: Save model with optimal threshold\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PERSISTENCE AND DEPLOYMENT\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Update best model with optimal threshold\n",
        "best_model.fitted_threshold = optimal_threshold\n",
        "\n",
        "# Save model\n",
        "model_path = Path('wafer_defect_production_model.joblib')\n",
        "best_model.save(model_path)\n",
        "\n",
        "print(f\"\u2705 Model saved to: {model_path.absolute()}\")\n",
        "print(f\"\\nModel metadata:\")\n",
        "print(f\"  - Model type: {best_model.model_name}\")\n",
        "print(f\"  - Decision threshold: {best_model.fitted_threshold:.3f}\")\n",
        "print(f\"  - Training date: {best_model.metadata.trained_at}\")\n",
        "print(f\"  - Number of features: {best_model.metadata.n_features_in}\")\n",
        "print(f\"  - File size: {model_path.stat().st_size / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c270ba6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 4.2: Load model and verify predictions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL LOADING AND VERIFICATION\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Load model from disk\n",
        "loaded_model = WaferDefectPipeline.load(model_path)\n",
        "\n",
        "print(f\"\u2705 Model loaded from: {model_path}\")\n",
        "print(f\"\\nLoaded model metadata:\")\n",
        "print(f\"  - Model type: {loaded_model.model_name}\")\n",
        "print(f\"  - Decision threshold: {loaded_model.fitted_threshold:.3f}\")\n",
        "\n",
        "# Verify predictions match\n",
        "y_pred_original = best_model.predict(X_test[:10])\n",
        "y_pred_loaded = loaded_model.predict(X_test[:10])\n",
        "\n",
        "predictions_match = np.array_equal(y_pred_original, y_pred_loaded)\n",
        "\n",
        "print(f\"\\nPrediction verification (first 10 samples):\")\n",
        "print(f\"  - Original predictions: {y_pred_original}\")\n",
        "print(f\"  - Loaded predictions:   {y_pred_loaded}\")\n",
        "print(f\"  - Predictions match: {'\u2705 YES' if predictions_match else '\u274c NO'}\")\n",
        "\n",
        "if predictions_match:\n",
        "    print(\"\\n\u2705 Model serialization verified successfully\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f WARNING: Model predictions differ after loading!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c904d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution 4.3: Demonstrate CLI usage\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLI USAGE EXAMPLES\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "cli_examples = [\n",
        "    {\n",
        "        'title': 'Training a Model',\n",
        "        'command': 'python wafer_defect_pipeline.py train --dataset synthetic_wafer --model rf --min-precision 0.85 --save production_model.joblib',\n",
        "        'description': 'Train Random Forest with minimum 85% precision constraint'\n",
        "    },\n",
        "    {\n",
        "        'title': 'Evaluating a Model',\n",
        "        'command': 'python wafer_defect_pipeline.py evaluate --model-path production_model.joblib --dataset synthetic_wafer',\n",
        "        'description': 'Evaluate saved model on test data'\n",
        "    },\n",
        "    {\n",
        "        'title': 'Making Predictions',\n",
        "        'command': '''python wafer_defect_pipeline.py predict --model-path production_model.joblib --input-json '{\"center_density\":0.12, \"edge_density\":0.05, \"defect_area_ratio\":0.08, \"defect_spread\":2.5, \"total_pixels\":3000, \"defect_pixels\":240}' ''',\n",
        "        'description': 'Predict defect status for single wafer'\n",
        "    },\n",
        "    {\n",
        "        'title': 'High Recall Training',\n",
        "        'command': 'python wafer_defect_pipeline.py train --dataset synthetic_wafer --model gb --min-recall 0.95 --save high_recall_model.joblib',\n",
        "        'description': 'Train Gradient Boosting optimized for catching 95%+ of defects'\n",
        "    },\n",
        "    {\n",
        "        'title': 'Imbalanced Data Handling',\n",
        "        'command': 'python wafer_defect_pipeline.py train --dataset synthetic_wafer_1000_0.05 --model logistic --use-smote --save balanced_model.joblib',\n",
        "        'description': 'Train with SMOTE oversampling for highly imbalanced data (5% defect rate)'\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, example in enumerate(cli_examples, 1):\n",
        "    print(f\"{i}. {example['title']}\")\n",
        "    print(f\"   {example['description']}\")\n",
        "    print(f\"\\n   $ {example['command']}\\n\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 All CLI commands return JSON output for programmatic consumption\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e03c7e3",
      "metadata": {},
      "source": [
        "### Production Deployment Checklist\n",
        "\n",
        "Use this checklist when deploying wafer defect classifier to production:\n",
        "\n",
        "#### Pre-Deployment (Before Production)\n",
        "- [ ] **Model Training**\n",
        "  - Train on full production dataset (not synthetic)\n",
        "  - Use cross-validation for hyperparameter tuning\n",
        "  - Document training data date range and characteristics\n",
        "  - Save training logs and metrics\n",
        "\n",
        "- [ ] **Model Validation**\n",
        "  - Test on held-out validation set\n",
        "  - Verify performance meets business requirements\n",
        "  - Test on edge cases and rare defect types\n",
        "  - Get sign-off from domain experts\n",
        "\n",
        "- [ ] **Threshold Optimization**\n",
        "  - Calculate real cost estimates (FP and FN)\n",
        "  - Optimize threshold for business objectives\n",
        "  - Document threshold choice rationale\n",
        "  - Get approval from manufacturing management\n",
        "\n",
        "#### Deployment (Going Live)\n",
        "- [ ] **Model Packaging**\n",
        "  - Save model with optimal threshold\n",
        "  - Include metadata (version, date, features, threshold)\n",
        "  - Create model card with performance metrics\n",
        "  - Version control model file (Git LFS or MLflow)\n",
        "\n",
        "- [ ] **Integration Testing**\n",
        "  - Test CLI interface with production data format\n",
        "  - Verify predictions match validation results\n",
        "  - Test error handling (missing features, invalid inputs)\n",
        "  - Load testing for throughput requirements\n",
        "\n",
        "- [ ] **Infrastructure**\n",
        "  - Deploy model to production server\n",
        "  - Set up monitoring and logging\n",
        "  - Configure alerts for prediction errors\n",
        "  - Create backup/rollback procedure\n",
        "\n",
        "#### Post-Deployment (In Production)\n",
        "- [ ] **Monitoring**\n",
        "  - Track prediction distribution (class balance)\n",
        "  - Monitor prediction latency and throughput\n",
        "  - Alert on unusual patterns (data drift)\n",
        "  - Log all predictions for auditing\n",
        "\n",
        "- [ ] **Performance Tracking**\n",
        "  - Compare predictions vs actual outcomes (ground truth)\n",
        "  - Calculate production ROC AUC, precision, recall\n",
        "  - Track false positive and false negative rates\n",
        "  - Measure financial impact (cost savings)\n",
        "\n",
        "- [ ] **Maintenance**\n",
        "  - Schedule regular model retraining (weekly/monthly)\n",
        "  - Review and update cost estimates quarterly\n",
        "  - Re-optimize threshold if business conditions change\n",
        "  - Document lessons learned and model improvements\n",
        "\n",
        "#### Documentation\n",
        "- [ ] **Technical Documentation**\n",
        "  - Model architecture and hyperparameters\n",
        "  - Feature engineering pipeline\n",
        "  - Training procedure and data requirements\n",
        "  - API/CLI usage guide\n",
        "\n",
        "- [ ] **Business Documentation**\n",
        "  - Model purpose and scope\n",
        "  - Performance metrics and targets\n",
        "  - Cost-benefit analysis\n",
        "  - Limitations and failure modes\n",
        "\n",
        "- [ ] **Operational Documentation**\n",
        "  - Deployment procedure\n",
        "  - Monitoring and alerting setup\n",
        "  - Troubleshooting guide\n",
        "  - Rollback procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009e2f97",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What You Accomplished\n",
        "\n",
        "In this solution notebook, you learned how to:\n",
        "\n",
        "1. **Data Analysis** \u2705\n",
        "   - Generate realistic synthetic wafer defect data\n",
        "   - Perform exploratory data analysis\n",
        "   - Identify discriminative features\n",
        "   - Handle class imbalance\n",
        "\n",
        "2. **Model Development** \u2705\n",
        "   - Train and compare multiple classification algorithms\n",
        "   - Evaluate using multiple metrics (ROC AUC, PR AUC, F1)\n",
        "   - Select best model based on comprehensive analysis\n",
        "   - Understand ensemble model advantages\n",
        "\n",
        "3. **Manufacturing Metrics** \u2705\n",
        "   - Calculate PWS (Prediction Within Spec)\n",
        "   - Estimate financial impact of errors\n",
        "   - Optimize decision threshold for cost minimization\n",
        "   - Balance precision and recall for business objectives\n",
        "\n",
        "4. **Production Deployment** \u2705\n",
        "   - Save and load models with metadata\n",
        "   - Verify model serialization\n",
        "   - Use CLI interface for production workflows\n",
        "   - Follow deployment best practices\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "**Technical**:\n",
        "- Ensemble models (RF, GB) typically outperform simpler models\n",
        "- Threshold optimization can reduce costs by 10-30%\n",
        "- Multiple metrics needed for comprehensive evaluation\n",
        "- Stratified splitting crucial for imbalanced data\n",
        "\n",
        "**Manufacturing**:\n",
        "- False negatives usually cost 2-10x more than false positives\n",
        "- PWS metric aligns with manufacturing quality standards\n",
        "- Real-time monitoring essential for production deployment\n",
        "- Domain expert validation critical for model acceptance\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further improve your wafer defect classifier:\n",
        "\n",
        "1. **Real Data Integration** \ud83d\udd04\n",
        "   - Replace synthetic data with actual wafer map images\n",
        "   - Feature engineering from spatial patterns\n",
        "   - Handle missing data and outliers\n",
        "\n",
        "2. **Deep Learning** \ud83e\udde0\n",
        "   - Implement CNN for spatial pattern recognition\n",
        "   - Transfer learning from pretrained models\n",
        "   - Compare to classical ML baseline\n",
        "\n",
        "3. **Advanced Techniques** \ud83d\ude80\n",
        "   - Hyperparameter optimization (Optuna, Grid Search)\n",
        "   - Model ensemble (stacking, voting)\n",
        "   - Online learning for continuous improvement\n",
        "\n",
        "4. **Production MLOps** \ud83c\udfed\n",
        "   - MLflow experiment tracking\n",
        "   - Model versioning and registry\n",
        "   - A/B testing framework\n",
        "   - Drift detection and alerting\n",
        "\n",
        "### Related Modules\n",
        "\n",
        "Continue your learning with these modules:\n",
        "- **Module 6.2**: CNN for wafer map image classification\n",
        "- **Module 9.1**: MLOps with MLflow\n",
        "- **Module 10.2**: Testing and quality assurance\n",
        "- **Module 5.2**: Time series for equipment drift monitoring\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83c\udf89 Congratulations!** You've completed the wafer defect classification solution. You now have the skills to build production-ready classification systems for semiconductor manufacturing."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
